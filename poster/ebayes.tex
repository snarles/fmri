%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Jacobs Landscape Poster
% LaTeX Template
% Version 1.1 (14/06/14)
%
% Created by:
% Computational Physics and Biophysics Group, Jacobs University
% https://teamwork.jacobs-university.de:8443/confluence/display/CoPandBiG/LaTeX+Poster
% 
% Further modified by:
% Nathaniel Johnston (nathaniel@njohnston.ca)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[final]{beamer}
\usepackage[scale=1.24]{beamerposter}
\usetheme{confposter}

%-----------------------------------------------------------
% Define the column widths and overall poster size
% To set effective sepwid, onecolwid and twocolwid values, first choose how many columns you want and how much separation you want between columns
% In this template, the separation width chosen is 0.024 of the paper width and a 4-column layout
% onecolwid should therefore be (1-(# of columns+1)*sepwid)/# of columns e.g. (1-(4+1)*0.024)/4 = 0.22
% Set twocolwid to be (2*onecolwid)+sepwid = 0.464
% Set threecolwid to be (3*onecolwid)+2*sepwid = 0.708

\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\newlength{\threecolwid}
\setlength{\paperwidth}{48in} % A0 width: 46.8in
\setlength{\paperheight}{36in} % A0 height: 33.1in
\setlength{\sepwid}{0.024\paperwidth} % Separation width (white space) between columns
\setlength{\onecolwid}{0.22\paperwidth} % Width of one column
\setlength{\twocolwid}{0.464\paperwidth} % Width of two columns
\setlength{\threecolwid}{0.708\paperwidth} % Width of three columns
\setlength{\topmargin}{-0.5in} % Reduce the top margin size
%-----------------------------------------------------------

\usepackage{graphicx}
\usepackage{booktabs}
\title{Stimulus Identification from fMRI scans}

\author{Charles Zheng and Yuval Benjamini} % Author(s)

\institute{Stanford University, Department of Statistics} % Institution(s)

%----------------------------------------------------------------------------------------

\begin{document}

\addtobeamertemplate{block end}{}{\vspace*{2ex}} % White space under blocks
\addtobeamertemplate{block alerted end}{}{\vspace*{2ex}} % White space under highlighted (alert) blocks

\setlength{\belowcaptionskip}{2ex} % White space under figures
\setlength\belowdisplayshortskip{2ex} % White space under equations

\begin{frame}[t] % The whole poster is enclosed in one beamer frame

\begin{columns}[t] % The whole poster consists of three major columns, the second of which is split into two columns twice - the [t] option aligns each column's content to the top

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % The first column

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\setbeamercolor{block alerted title}{fg=white,bg=BlueViolet} % Change the alert block title colors
\setbeamercolor{block alerted body}{fg=black,bg=white} % Change the alert block body colors

\begin{alertblock}{Overview}
\vspace{0.3in}

Seeking to explain the processes behind human perception, scientists
employ \emph{forward models} to model the causal relationship between
stimulus and neural activity.  But how can we measure the quality of
these models?  Kay et al (2008) introduced the
task of \emph{identification} as a way to demonstrate the fidelity and
generalizability of the model.
\vspace{0.7in}

Using the data of Kay \emph{et al.} as a motivating example, we
consider the statistical problem of optimal identification.  While
identification superficially resembles a classification task (with
many classes), it combines the challenge of multivariate regression
with high-dimensional discrimination.
\end{alertblock}

\begin{block}{Data}
\begin{itemize}
\item Sequence of stimuli (pictures) shown at time $t = 1,\hdots, T = 3500$
\item Record subject's multivariate response $y_t \in \mathbb{R}^p$, here $p \approx 20000$
\end{itemize}

\begin{center}
\begin{tabular}{ccccc}
$t = 1$ & $t = 2$ & $t = 3$ & $t = 4$ & $\cdots$\\ \hline
\includegraphics[scale = 0.5]{img1.png} &
\includegraphics[scale = 0.5]{img2.png} &
\includegraphics[scale = 0.5]{img3.png} &
\includegraphics[scale = 0.5]{img4.png} & $\cdots$\\ \hline
$Y_1$ & $Y_2$ & $Y_3$ & $Y_4$ & $\cdots$\\ \hline
\includegraphics[scale = 0.06]{brain1.png} &
\includegraphics[scale = 0.06]{brain2.png} &
\includegraphics[scale = 0.06]{brain3.png} &
\includegraphics[scale = 0.06]{brain4.png} & $\cdots$\\ \hline
\end{tabular}
\end{center}
\end{block}




\begin{block}{Identification}
\begin{center}
\begin{tabular}{c|c|cccc}
\hline
$y^*$ &     & &  $i^* = ?$  &   \\
\includegraphics[scale = 0.06]{brain7.png} & \hspace{0.5in} 
& \includegraphics[scale = .5]{img5.png}
& \includegraphics[scale = .5]{img6.png}
& \includegraphics[scale = .5]{img7.png}
& \includegraphics[scale = .5]{img8.png}\\
\hline
\end{tabular}
\end{center}
\begin{itemize}
\item Let $S$ be a set of stimuli, possibly outside the training set! $|S|$ can range from 120 to 10000
\item Scientist picks a stimulus $i^*$ from $S$ and measures the subject's reponse $y^*$
\item Can the statistician \emph{identify} $i^* \in S$ from $y^*$?
\end{itemize}
\emph{Remark.} In order to identify images outside the training set,
we need some way to generalize beyond the training set!
\end{block}

\end{column} % End of the first column

\begin{column}{\sepwid}\end{column} % Empty spacer column



\begin{column}{\onecolwid}

%----------------------------------------------------------------------------------------
%	MATERIALS
%----------------------------------------------------------------------------------------



\begin{block}{Previous work}

Previous work [1][2] generally follows likelihood-based approaches.
Consider a parametric model
\[
Y \sim F_\theta(X)
\]
where $Y_{T \times p}$ is a matrix containing the $T$ of recorded responses, and
where $X_{T \times q}$ is the matrix of the \emph{image features} of the corresponding stimuli. 
E.g. columns of $X$ are Gabor filters with
$q \approx 10000$, and $\theta$ is some parameter to be estimated.
Let $x_i : i \in S$ denote features of the test stimuli, and
identify $y^*$ based on the maximimum likelihood (ML) principle
\[
i^* = \text{argmax}_i \ell(y^*, x_i)
\]

We take the following as a representative approach
\begin{itemize}
\item Assume the normal mutivariate linear model
\[
Y \sim N(XB, \Sigma_E^2)
\]
\item Obtain point estimates of $B$ and $\Sigma_E$. E.g. $B$ estimated using elastic net [4], and
\[
\hat{\Sigma}_E = (1-\alpha) \hat{\text{Cov}}(Y - \hat{Y}) + \alpha\text{diag}(\hat{\text{Cov}}(Y - \hat{Y}))
\]
where $\hat{\text{Cov}}$ = sample covariance, $\alpha \in (0, 1)$.
\item Identify the stimulus $i^*$ by
\[
i^* = \text{argmin}_{i} (x_i^T B - y^*)^T \hat{\Sigma}_E^{-1} (x_i^T B - y^*)
\]
\end{itemize}
\end{block}

\begin{block}{Limitations of ML}
\begin{itemize}
\item Point estimates obtained by minimizing \emph{prediction error}, but the loss function for identification is different!
\item In fact, any estimate of $B$ which is degenerate (identical columns up to scaling) is suboptimal
\end{itemize}
\begin{center}
\begin{tabular}{ccc}
(a) & (b) & (c)\\
\includegraphics[scale = 0.5, trim = 1in 1in 0.5in 1in, clip]{loss_se.pdf} & 
\includegraphics[scale = 0.5, trim = 1in 1in 0.5in 1in, clip]{loss_3.pdf} &
\includegraphics[scale = 0.5, trim = 1in 1in 0.5in 1in, clip]{zero2.pdf}\\
{\small 0} & {\small 0} & {\small 0}\\
\end{tabular}
\end{center}
{\small
\emph{(a)} Squared error loss (vertical axis) and \emph{(b)} loss function for identification,
as a function of the difference between the true mean signal and the predicted signal.\\
\emph{(c)} The optimal point estimate for identification (solid) vs the optimal point estimate for regression (dashed)
diverge sharply at $B = 0$ in the one-dimensional case.  The same phenomenon occurs for higher dimensions when $B$ is degenerate.
}
\end{block}

%\begin{center}
%\includegraphics[scale = 1.0]{illus1_A.pdf}
%\end{center}

%\emph{Theoretical results}
%\begin{itemize}
%\item Asymptotic consistency to optimal procedure $T \to \infty$ under correct model specification
%\item Inconsistency given model misspecification (nonlinearity)
%\end{itemize}



%----------------------------------------------------------------------------------------

\end{column} % End of column 2.1

\begin{column}{\sepwid}\end{column} % Empty spacer column


\begin{column}{\onecolwid}

\begin{block}{Empirical Bayes}

Can we find a principled alternative to ML?

\begin{itemize}
\item \emph{Idea}: Unlike ML, the Bayes rule surely optimizes the ``correct'' objective function
\item \emph{Problem}: We don't know the hyperparameters for Bayesian inference
\item \emph{Emprical Bayes}: use the data to estimate the covariances
$\Sigma_B$ and $\Sigma_E$, then compute posterior distribution of $B$
\item In contrast to ML, which results in \emph{linear decision
  boundaries} (below: left), Empirical Bayes (EB) results in \emph{quadratic boundaries} (below: right)
\end{itemize}

\begin{center}
\begin{tabular}{cc}
\includegraphics[scale = 0.5]{illus1_A.pdf} & 
\includegraphics[scale = 0.5]{illus1_B.pdf}\\
ML & EB
\end{tabular}
\end{center}
\end{block}

\setbeamercolor{block alerted title}{fg=black,bg=BlueViolet!20} % Change the alert block title colors
\setbeamercolor{block alerted body}{fg=black,bg=white} % Change the alert block body colors

\begin{alertblock}{Technical details}
\emph{Model}

\begin{itemize}
\item Noise $E_t \sim N(0, \Sigma_E)$ iid 
\item Coefficients $B_i \sim N(0, \sigma^2_i I)$ for $i = 1, \hdots, p$
\end{itemize}

\emph{Estimate hyperparameters}

\begin{itemize}
\item Use \emph{eigenprism} (Janson 2015) to estimate $\theta_i^2 = ||B_i||^2$ for $i =1,\hdots, p$
\item Set $\sigma^2_i = \hat{\theta}_i^2/q$
\item Estimate $\hat{B}$ as posterior mean
\item Estimate $\Sigma_E$ (same as in ML)
\end{itemize}

\emph{Compute posterior}
\begin{itemize}
\item Closed-form expressions for posterior of $B$
\item Computational bottleneck: inverting a $pq \times pq$ covariance matrix
\end{itemize}

\emph{Apply Bayes rule}

\begin{itemize}
\item \emph{Uncertainty} in $B$ is reflected as \emph{added noise}
\item Result: posterior $\text{Cov}(y^*|i^*)$ varies, hence \emph{quadratic boundaries}
\end{itemize}

\end{alertblock}


%----------------------------------------------------------------------------------------

\end{column} % End of column 2.2

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % The third column

%----------------------------------------------------------------------------------------
%	CONCLUSION
%----------------------------------------------------------------------------------------

\begin{block}{Simulation Results}
\begin{itemize}
\item Parameters $p = q = 60$, random $B$ and $\Sigma_E$
\item Empirical bayes outperforms ML when $n < q$... however, still unstable!
\end{itemize}
\begin{center}
\includegraphics[scale = 1.3]{simulation1.pdf}
\end{center}
\small{
 \textcolor{blue}{(E)} Empirical Bayes,  \textcolor{red}{(M)} Maximum likelihood,
(o) Bayes risk \emph{(knowing true $\Sigma_B$, $\Sigma_E$)}}
\end{block}

\begin{block}{Ongoing Work}
\begin{itemize}
\item Why does error \emph{increase} with sample size!? Refine covariance estimation methods..
\item Required cost of $O((pq)^3)$ unacceptable for real data... develop tractable approximations
\end{itemize}
\end{block}

\setbeamercolor{block alerted title}{fg=white,bg=Violet} % Change the alert block title colors
\setbeamercolor{block alerted body}{fg=black,bg=white} % Change the alert block body colors

\begin{block}{Conclusions}
\begin{itemize}
\item ML-based approaches rely on point estimates, and hence optimize the wrong objective function
\item Empirical bayes achieves better performance by approximating the Bayes rule, but the ``empirical'' part remains tricky
\item Better theoretical understanding is needed to explain why EB succeeds (and sometimes fails)
\end{itemize}
\end{block}

%----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------

\begin{block}{References}

\small{
\noindent [1] Kay et al. \emph{Nature} (2008)\\
\noindent [2] Vu et al. \emph{Annals of Applied Statistics} (2011)\\
\noindent [3] Janson et al. (2015) http://arxiv.org/abs/1505.02097\\
\noindent [4] Zou et al. \emph{J. R. Statist. Soc. B} (2005)
}

\end{block}

%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

\begin{block}{Acknowledgements}


\small{ This work was supported by an NSF graduate research fellowship.
  We are also grateful to the travel support provided by the SAND 7 conference.  }

\end{block}




%----------------------------------------------------------------------------------------

\end{column} % End of the third column

\end{columns} % End of all the columns in the poster

\end{frame} % End of the enclosing frame

\end{document}
