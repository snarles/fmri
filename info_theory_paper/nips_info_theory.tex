\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\Cor}{\text{Cor}}

\title{Estimating mutual information in high dimensions via classification error}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Charles Y.~Zheng \\
  Department of Statistics\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{snarles@stanford.edu} \\
  %% examples of more authors
  \And
  Yuval ~Benjamini \\
  Department of Statistics \\
  Hebrew University\\
  Jerusalem, Israel\\
  \texttt{yuval.benjamini@mail.huji.ac.il}
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Estimating the mutual information $I(X; Y)$ based on observations
becomes statistically infeasible in high dimensions without some kind
of assumption or prior.  One approach is to assume a parametric joint
distribution on $(X, Y)$, but in many applications, such a strong
modeling assumption cannot be justified.  Alternatively, one can estimate the mutual information based the performance of a classifier trained on the data.  Existing methods include using the empirical mutual information of the confusion matrix of the classifier, as
well as an estimator based on Fano's inequality.  However, both of these
methods all produce an estimate which is bounded by $\log(k)$,
where $k$ is the number of classes.
This presents a substantial limitation for classification-based approaches, since the number of
repeats per class must be large for the classifier to work well, hence limiting the number of classes $k$ that can be defined. In this paper, we construct a novel classification-based estimator of mutual information which
overcomes these limitations.  Our estimator is based on
high-dimensional asymptotics: we show that in a particular limiting
regime, the mutual information is an invertible function of the
expected $k$-class Bayes error.  While the theory is based
on a large-sample, high-dimensional limit, we demonstrate through
simulations that our proposed lower confidence bound has superior
performance to the alternatives in problems of moderate
dimensionality.
\end{abstract}

\section{Introduction}

Mutual information $I(X; Y)$ is fundamentally a measure of dependence
between random variables $X$ and $Y$, and is defined as
\[
I(X;Y) = \int p(x, y) \log \frac{p(x, y)}{p(x)p(y)}dxdy.
\]
In its original context of information theory, the mutual information
describes the rate at which a noisy communications channel $Y$ can
communicate bits from a source stream $X$, but by now, the quantity
$I(X, Y)$ has found many new uses in science and engineering.  Mutual
information is used to test for conditional independence (CITE), to
quantifying the information between a random stimulus $X$ and the
signaling behavior of an ensembles of neurons, $Y$ (Borst 1999); for
use as an objective function for training neural networks (CITE), for
feature selection in machine learning, and even as an all-purpose
nonlinear measure of ``correlation for the 21st century'' (Speed.)
What is common to all of these new applications, and what differs from
the original setting of Shannon's theory of information, is that the
variables $X$ and $Y$ have unknown distributions which must be
inferred from data.  In the case when $X$ and $Y$ are both
low-dimensional, for instance, when summarizing the properties of a
single neuron in response to a single stimulus feature, $I(X; Y)$ can
be estimated nonparametrically using a reasonable number of
observations.  There exists a huge literature on nonparametric
estimation of entropy and mutual information exists, see (CITE) for a
review.

However, the sample complexity for nonparametric estimation grows
exponentially with the dimension, rendering such methods ineffective
in applications with high-dimensional data.  One such application
includes multivariate pattern analysis (MVPA), an area of neuroscience
research pioneered by Haxby (2001), which studies how entire regions
of the human brain respond to stimuli, using function magnetic
resonance imaging (fMRI) data; in MVPA studies, the input $X$ could be
a natural image parameterized by $p = 10000$ image features, while the
output $Y$ is a $q=20000$-dimensional vector of brain activation
features obtained from the fMRI scan.  In problems of such
dimensionality, one can tractably estimate mutual information by
assuming a multivariate Gaussian model: however, this approach
essentially assumes a linear relationship between the input and
output, and hence fails to quantify nonlinear dependencies.  Rather
than assuming a full parametric generative model, one can empirically
select a good \emph{discriminative} model by using machine learning.
Treves (1997) first proposed using the empirical mutual information of
the classification matrix in order to obtain a lower bound of the
mutual information $I(X; Y)$; this confusion-matrix-based lower bound
has subsequently enjoyed widespread use in the MVPA literature
(Quiroga 2009.)  But even earlier that this, the idea of linking
classification performance to mutual information can be found in the
beginnings of information theory: after all, Shannon's original
motivation was to characterize the minimum achievable error
probability of a noisy communication channel.  More explicitly, Fano's
inequality provides a lower bound on mutual information in relation to
the optimal prediction error, or Bayes error.   Therefore, one can construct an estimator based on Fano's inequality, $\hat{I}_{Fano}$.  In either case, any method which derives an estimate of mutual information
from classification performance may be considered a \emph{discriminative} estimation procedure, in contrast to the \emph{parametric} and
\emph{nonparametric} classes of estimation procedures.  

\subsection{Discriminative estimators of mutual information}

In many applications, the
discriminative approach takes an advantageous middle ground between
the two extremes of nonparametric and parametric approaches for
estimating mutual information. In neuroimaging data, we lack prior
knowledge for specifying parametric models, and the data is too
high-dimensional for nonparametric approaches, but we have a
sufficient idea of the general ``structure'' in the data to achieve
above-chance classification rates.

Five steps are required to implement discriminative estimation of mutual information.
First, one must define a classification task.  The kinds of tasks that can be defined
depend on the sampling scheme used to collect the data.  Second,
one chooses a classifier $\mathcal{F}$.   Third, the classifier is trained on a training subset
of the data to obtain a classification rule $f$.  Fourthly, the performance of the rule $f$ is evaluated on the held-out test set.  Finally, the performance metrics of the classifier are converted into
an estimate of mutual information.  In this paper we are mostly concerned with the final step:
how to convert measures of classification performance into estimates of mutual information.

Let us assume that the variables $X, Y$ have a joint distribution $F$, and
that one can define a conditional distribution of $Y$ given $X$,
$Y|X \sim F_X,$
and let $G$ denote the marginal distribution of $X$.
We consider two different types of sampling procedures:
\begin{itemize}
\item \emph{pair sampling}:  For $i = 1,\hdots, n$, the data $(X^i, Y^i)$ are sampled i.i.d. from the joint distribution of $(X, Y)$.
\item \emph{stratified sampling}:  For $j = 1,\hdots, k$, sample i.i.d. \emph{exemplars} $X^{(1)},\hdots, X^{(k)} \sim G$.  For $i = 1,\hdots, n$, draw $Z^i$ iid from the uniform distribution on $1,\hdots, k$, then draw $Y^i$ from the conditional distribution $F_{X^{(Z_i)}}$.
\end{itemize}

Pair sampling occurs in observational studies, where one observes both $X$ and $Y$ externally.  On the other hand, stratified sampling is more commonly seen in controlled experiments, where an experimenter chooses an input $X$ to feed into a black box, which outputs $Y$.  An example from fMRI studies is an experimental design where the subject is presented a stimulus $X$, and the experimenter measures the subject's response via the brain activation $Y$. \footnote{Note the asymmetry in our definition of stratified sampling: our convention is to take $X$ to be the variable preceding $Y$ in causal order.  Such causal directionality constrains the stratified sampling to have repeated $X$ rather than repeated $Y$ values, but has no consequence for the mutual information $I(X; Y)$, which is a symmetric function.}

Given data from either pair sampling or stratified sampling, one can define various
\emph{classification tasks}.  Here, the point is to use classification as a tool for extracting information
about the relationship between $X$ and $Y$.  As such, it is up to us to define the classification tasks
of interest.  For instance, one can define tasks which either classify $Y$ based on $X$, or classify $X$ based on $Y$; without loss of generality, we henceforth consider the latter.  In the case of continuous $X$, we can define an arbitrary number of classes $k$ by specifying a partition on the space of $X$.
That is, one can define a \emph{class function} $Z: X \to \{1,\hdots, k\}$, and consider the problem of classifying $Z$ given $Y$.  A classification rule is
any (possibly stochastic) mapping $f: \mathcal{Y} \to \{1,\hdots,
k\}$, where $\mathcal{Y}$ is a superset of the support of $Y$.  The \emph{generalization error} of the classification rule is
$
e_{gen}(f) = Pr[f(Y) \neq Z].
$
The Bayes error is the generalization error of the optimal classification rule,
$
e_{Bayes}(f) = \inf_f e_{gen}(f)
$
We call such a classification task a \emph{partition-based} classification task.

The freedom to choose the partition $Z$ may be more of a curse than a blessing when it is unclear how
to choose an appropriate partition on the support of $X$.  If stratified sampling is employed,
one can define an \emph{exemplar-based} classification task which avoids having to specify a partition.  One defines the \emph{class function} $Z$ by
\[
Z: \{X^{(1)}, \hdots, X^{(k)}\} \to \{1,\hdots, k\},
\]
\[
Z(X^{(i)}) = i\text{ for }i = 1, \hdots, k.
\]
Note that the domain of $Z$ is restricted to the set of observed exemplars $X^{(1)},\hdots, X^{(k)}$.
The loss function is not well-defined when $X$ lies outside the set of exemplars,
so it is natural to define the generalization error by
\begin{equation}\label{egendef}
e_{gen}(f) = \frac{1}{k} \sum_{i=1}^k\Pr[f(Y) \neq Z|X = X^{(i)}].
\end{equation}
Indeed, in experiments where stratified sampling is used, this is the most commonly employed notion
of generalization error (CITE).
In an exemplar-based classification, there is no need to specify an arbitrary partition on the input space, but now the $k$ classes will now be \emph{randomly} defined.  One consequence is that the Bayes error $e_{Bayes}$ is a random variable: when the sampling produces $k$ similar exemplars, $e_{Bayes}$ will be higher, and when the sampling produces well-separated exemplars $e_{Bayes}$ may be lower.  
Therefore, in stratified sampling, it is useful to consider the \emph{average Bayes error},
\begin{equation}\label{abedef}
e_{ABE, k} = \E_{X^{(1)},\hdots, X^{(k)}}[e_{Bayes}],
\end{equation}
where the expectation is taken over the joint distribution of $X^{(1)},\hdots, X^{(k)} \stackrel{iid}{\sim} G$.

Unless expert knowledge is available, it is usually necessary to choose the function $f$ in a
data-dependent way in order to obtain a reasonable classification
rule.  We use the terminology \emph{classifier} to refer to any algorithm which takes
data as input, and produces a classification rule $f$ as output.
Mathematically speaking, the classifier
is a functional which maps a set of observations to a classification
rule,
$
\mathcal{F}: \{(x^{1},y^{1}),\hdots, (x^{m}, y^{m})\} \mapsto f(\cdot).
$
The data $(x^1,y^1),\hdots, (x^m, y^m)$ used to obtain the
classification rule is called \emph{training data.}  When the goal is to
obtain \emph{inference} about the generalization error $e_{gen}$ of the classification
rule $f$, it becomes necessary to split the data into two independent sets:
one set to train the classifier, and one to evaluate the performance.
The reason that such a splitting is necessary is because using the
same data to test and train a classifier introduces significant bias
into the empirical classification error (CITE ESL).
One creates a \emph{training
set} consisting of $r_1$ repeats per class,
$
S_{train} = \{(x^{(i)}, y^{(i),j})\}_{i=1, j=1}^{k, r_1},
$
and a \emph{test set} consisting of the remaining $r_2 = r - r_1$ repeats,
$
S_{test} = \{(x^{(i)}, y^{(i),j})\}_{i=1, j=r_1}^{k, r}.
$
The classification rule is obtained via
$
f = \mathcal{F}(S_{train}),
$
and the performance of the classifier is evaluated by predicting the classes of the test set.
The results of this test are summarized by a $k \times k$ \emph{confusion matrix} $M$ with
$
M_{ij} = \sum_{\ell=r_1 + 1}^r I(f(y^{(i), r}) = j).
$
The $i, j$th entry of $M$ counts how many times a output in the $i$th class was classified  to the $j$th class.
The \emph{test error} is the proportion of off-diagonal terms of $M$,
$
e_{test} = \frac{1}{kr} \sum_{i \neq j} M_{ij},
$
and is an unbiased estimator of $e_{gen}$.
However, in small sampling regimes the quantity $e_{test}$ may be too variable to use as an estimator of $e_{gen}$.  We recommend the use of Bayesian smoothing, defining an  $\alpha$-smoothed estimate $\hat{e}_{gen, \alpha}$ by
$
\hat{e}_{gen, \alpha} = (1 - \alpha) e_{test} + \alpha \frac{k-1}{k},
$
which takes a weighted average of the unbiased estimate $e_{test}$, and the natural prior of \emph{chance classification}.

We define a discriminative estimator to be a function which maps the misclassification matrix to a positive number,
$
\hat{I}: \mathbb{N}^{k \times k} \to \mathbb{R}.
$
We are aware of the following examples of discriminative estimators: (1) estimators derived from using Fano's inequality, and (2) the empirical information of the confusion matrix, as introduced by Treves (1999).

Fano's inequality can be easily adapted to yield a discriminative estimator.
The original inequality reads
\[
H(Z|Y) \leq H(e_{Bayes}) + e_{Bayes} \log ||\mathcal{Z}| - 1|
\]
where $H(e)$ is the entropy of a Bernoulli random variable with probability $e$.
Replacing $H(Z|Y)$ with $H(X|Y)$ and replacing $e_{Bayes}$ with $\hat{e}_{gen, \alpha}$,
we get the estimator
\[
\hat{I}_{Fano}(M) = log(K) - \hat{e}_{gen, \alpha} log(K-1) + \hat{e}_{gen, \alpha} log(p) + (1-\hat{e}_{gen, \alpha}) log(1-\hat{e}_{gen, \alpha}).
\]
Meanwhile, the confusion matrix estimator computes
\[
\hat{I}_{CM}(M) = \frac{1}{k^2} \sum_{i=1}^k \sum_{j=1}^k \log \frac{M_{ij}}{r/k},
\]
which is the empirical mutual information of the discrete joint distribution $(Z, f(Y))$.

As noted in the literature (Quiroga et al. 2009), such
discriminative approaches tend to underestimate the mutual information.  Two sources of
``information loss'' are (1) the fact that a continuous input variable
$X$ is discretized into $k$ classes, and (2) that the performance of
any classifier trained from data can at best given an \emph{upper
  bound} to the error of the best classification rule: the Bayes
error.  Dealing with the problem of estimating the Bayes error is beyond the scope of the current paper,
but we will argue that the stratified sampling design overcomes some of the limitations
imposed by the discretization of $X$.

\subsection{The problem of undersampling}

Stratified sampling is commonly used in neuroscience experiments,
and furthermore, it is often the case that the number of classes $k$ is constrained
to be small due to practical and statistical considerations (Gastpar 2009.)
When $\log(k) \ll I(X; Y)$, we say that the data is \emph{undersampled}:
meaning that even if the total number of observations is large (due to having many repeats per class,)
the diversity of the exemplar set is poor.

Gastpar et al. (2009), studied the nonparametric estimator
\[
\hat{I}_0 = \hat{H}(Y) - \frac{1}{k}\sum_{i=1}\hat{H}(Y|X),
\]
where $\hat{H}$ is an estimator for the entropy.
Gastpar et al. showed that $\hat{I}_0$ is biased downwards due to undersampling of the exemplars:
to counteract this bias, they introduce the anthropic correction estimator $\hat{I}_\alpha$.
However, without a principled approach to choose the parameter $\alpha \in (0,1]$,
$\hat{I}_\alpha$ could still vastly underestimate or overestimate the mutual information.

In contrast, the parametric approach is relatively robust to undersampling of the exemplars.
For instance, in the multivariate Gaussian model, the mutual information is a function of the eigenvalues of the canonical correlation matrix $R  =\Sigma_X^{-1/2}\Sigma_{XY}\Sigma_Y^{-1/2}$.  If $X$ is a controlled stimulus, $\Sigma_X$ can be taken to be identity, and the interclass covariance matrix of $Y$ suffices to estimate $\Sigma_Y$ consistently.  The remaining parameter $\Sigma_{XY}$ can be estimated from the inner-product matrix $\boldsymbol{X}^T \boldsymbol{Y}$, which is unbiased for $\Sigma_{XY}$ regardless of whether pair sampling or stratified sampling is employed.

Meanwhile, the discriminative estimators face serious limitations under the undersampled regime--
which is especially unfortunate, because exemplar undersampling is usually necessary for obtaining a good
classification rule.  It is easy to show that $\log(k)$ is a tight upper bound for both $\hat{I}_{CM}$ and 
$\hat{I}_{Fano}$.  As $I(X; Y)$ exceeds $\log(k)$, the estimate $\hat{I}$ can no longer approximate $I(X; Y)$, even up to a constant factor.  On the other hand, the $\log(k)$ barrier is a reasonable limitation, if we consider the following worst-case example.  Let $X$ and $Y$ have joint density
$
p(x, y) = \frac{1}{k}I(\lfloor kx \rfloor = \lfloor ky \rfloor)
$ on the unit square.  Under partition-based classification, if we set $Z(x) = \lfloor kx \rfloor + 1$,
then no errors are made under the Bayes rule. We therefore have a joint distribution which maximizes $\hat{I}_{CM}$ and $\hat{I}_{Fano}$ but has \emph{finite} information $I(X; Y) = \log(k)$.
The consequence of this is that under partition-based classification,
we cannot hope to distinguish distributions with $I(X; Y) > \log(k)$.

However, in situations where we can rule out such pathological cases, an \emph{unbounded} estimate of mutual information could potentially outperform $\hat{I}_{Fano}$ and
$\hat{I}_{CM}$, especially if $I(X; Y) \gg \log(k)$.
The strategy we adopt for overcoming this $\log(k)$ barrier to combine the stratified sampling assumption with an additional assumption of high dimensionality.
Referring to the earlier example, we see that under stratified sampling,
the average Bayes error is approximately $e_{ABE, k} = 1- e$ rather than zero.
Therefore, we could potentially distinguish distributions with $I(X; Y) \gg \log(k)$.

Yet, the bulk of the power in our proposed approach comes from the high-dimensionality assumption.
In section 2 we present an asymptotic
setting intended to capture the notion of high dimensionality; namely,
one where the number of classes is fixed, and where the information
$I(X; Y)$ remains fixed, while the dimensionality of the input $X$ and
output $Y$ both grow to infinity.  
These are appropriate modeling assumptions for many applications
that have high-dimensional data and low signal-to-noise ratio--in other words, the kind of application
for which machine learning is most suited.
We make a number of additional
regularity conditions to rule out scenarios where $(X, Y)$ is really
less ``high-dimensional'' than it appears, since most of the variation
is captured a low-dimensional manifold.  In section 2.1 we present our
key result, which links the asymptotic average Bayes error to the
mutual information; in section 2.2 we apply this result to derive our
proposed estimator, $\hat{I}_{HD}$ (where HD stands for ``high-dimensional.'')  Section 3 presents
simulation results, and section 4 concludes.  All proofs are given in the supplement.

\section{Theory}

We derive a new discriminative estimator 
by exploiting the properties of stratified sampling, plus a universality property that arises in high-dimensions.
This universality phenomenon allows us to establish a relationship between the
mutual information $I(X; Y)$ and the $k$-class average Bayes error,
$e_{ABE, k}$.  In short, we will identify a function $\pi_k$
(which depends on $k$),
\begin{equation}\label{abepi}
e_{ABE, k} \approx \pi_k(\sqrt{2 I(X; Y)})
\end{equation}
and that this approximation becomes accurate under a limit where $I(X; Y)$ is small relative to the dimensionality of $X$,
and under the condition that the components of $X$ are approximately independent.
The function $\pi_k$ is given by
\[
\pi_k(c) = 1 - \int_{\mathbb{R}} \phi(z - c)  \Phi(z)^{k-1} dz.
\]
This formula is not new to the information theory literature: it appears as the error rate of an orthogonal constellation (CITE).  What is surprising is that the same formula can be used to approximate the error rate in much more general class of classification problems\footnote{An intuitive explanation for this fact is that points from any high-dimensional distribution lie in an orthogonal configuration with high probability.}--this 
is precisely the universality result which provides the basis for our proposed estimator.

Figure \ref{fig:pi} displays the plot of $\pi_k$ for several values of $k$.
For all values of $k$, $\pi_k(\mu)$ is monotonically decreasing in $\mu$, and tends to zero as $\mu \to \infty$, which is what we expect since if $I(X; Y)$ is large, then the average Bayes error should be small.
Another intuitive fact is that 
$
\pi_k(0) = 1 - \frac{1}{k},
$ since after all, an uninformative response cannot lead to above-chance classification accuracy.

\begin{figure}
\centering
\includegraphics[scale = 0.5, clip=true, trim=0 0.2in 0 0.5in]{../info_theory_sims/illus_piK.pdf}
\caption{The function $pi_k(\mu)$, for $k = \{2, 9, 99, 999\}$ (left to right) \label{fig:pi}}
\end{figure}

\subsection{Universality result}

We obtain the universality result in two steps.  First, we link the average Bayes error to the moments of
some statistics $Z_i$.  Secondly, we use taylor approximation in order to express $I(X; Y)$ in terms of the moments of $Z_i$.  Connecting these two pieces yields the formula \eqref{abepi}.

Let us start by rewriting the average Bayes error:
\[
e_{ABE, k} = \Pr[p(Y|X_1) \leq \max_{j \neq 1} p(Y|X_j)| X = X_1].
\]
Defining the statistic $Z_i = \log p(Y|X_i) - \log p(Y|X_1)$, where $Y \sim p(y|X_1)$, we obtain $
e_{ABE} = \Pr[\max_{j > 1} Z_i > 0].
$
The key assumption we need is that $Z_2,\hdots, Z_k$ are asymptotically multivariate normal.
If so, the following lemma
allows us to obtain a formula for the misclassification rate.

\textbf{Lemma 1. }
\emph{
Suppose $(Z_1, Z_2, \hdots, Z_k)$ are jointly multivariate normal, with 
$\E[Z_1 - Z_i]= \alpha$, 
$\Var(Z_1) = \beta \geq 0$, 
$\Cov(Z_1, Z_i) = \gamma$, 
$\Var(Z_i)= \delta$, and $\Cov(Z_i, Z_j) = \epsilon$ for all $i, j = 2, \hdots,
k$, such that $\beta + \epsilon - 2\gamma > 0$.  Then, letting
\[
\mu = \frac{\E[Z_1 - Z_i]}{\sqrt{\frac{1}{2}\Var(Z_i - Z_j)}} = \frac{\alpha}{\sqrt{\delta - \epsilon}},
\]
\[
\nu^2 = \frac{\Cov(Z_1 -Z_i, Z_1 - Z_j)}{\frac{1}{2}\Var(Z_i - Z_j)} = \frac{\beta + \epsilon - 2\gamma}{\delta - \epsilon},
\]
we have
\begin{align*}
\Pr[Z_1 < \max_{i=2}^k Z_i] &= \Pr[W < M_{k-1}]
\\&= 1 - \int \frac{1}{\sqrt{2\pi\nu^2}} e^{-\frac{(w-\mu)^2}{2\nu^2}} \Phi(w)^{k-1} dw,
\end{align*}
where $W \sim N(\mu, \nu^2)$ and $M_{k-1}$ is the maximum of $k-1$
independent standard normal variates, which are independent of $W$.
}

To see why the assumption that $Z_2,\hdots, Z_k$ are multivariate normal might be justified, suppose that $X$ and $Y$ have the same dimensionality $d$, and that
joint density factorizes as
\[
p(x^{(j)}, y) = \prod_{i=1}^d p_i(x^{(j)}_i, y_i)
\]
where $x_i^{(j)}, y_i$ are the $i$th scalar components of the vectors $x^{(j)}$ and $y$.
Then,
\[
Z_i = \sum_{m=1}^d \log p_m(y_m | x^{(i)}_m) - \log p_m(y_m | x^{(m)}_1)
\]
where $x_{i, j}$ is the $i$th component of $x_j$.
The $d$ terms $\log p_m(y_m | x_{m, i}) - \log p_m(y_m | x_{m, 1})$ are independent across the indices $m$,
but dependent between the $i = 1,\hdots, k$.
Therefore, the multivariate central limit theorem can be applied to conclude that the vector
$(Z_2,\hdots, Z_k)$ can be scaled to converge to a multivariate normal distribution.
While the componentwise independence condition is not a realistic assumption,
the key property of multivariate normality of $(Z_2,\hdots, Z_k)$ holds under more general conditions, and appears reasonable in practice.  

It remains to link the moments of $Z_i$ to $I(X;Y)$.  This is accomplished by approximating the logarithmic term by the Taylor expansion
\[
\log \frac{p(x, y)}{p(x) p(y)} \approx \frac{p(x, y) - p(x) p(y)}{p(x) p(y)} - \left(\frac{p(x, y) - p(x) p(y)}{p(x) p(y)}\right)^2 + \hdots.
\]
A number of assumptions are needed to ensure that needed approximations are sufficiently accurate;
and additionally,  in order to apply the central limit theorem, we need to consider a \emph{limiting sequence} of problems with increasing dimensionality.  We now state the theorem.

\textbf{Theorem 1.} \emph{Let $p^{[d]}(x, y)$ be a sequence of joint densities
for $d = 1,2,\hdots$.  Further assume that
\begin{itemize}
\item[A1.] $\lim_{d \to \infty} I(X^{[d]}; Y^{[d]}) = \iota < \infty.$
\item[A2.] There exists a sequence of scaling constants $a_{ij}^{[d]}$
and $b_{ij}^{[d]}$ such that the random vector $(a_{ij}\ell_{ij}^{[d]} +
b_{ij}^{[d]})_{i, j = 1,\hdots, k}$ converges in distribution to a
multivariate normal distribution.
\item[A3.] There exists a sequence of scaling constants $a^{[d]}$, $b^{[d]}$ such that
\[
a^{[d]}u(X^{(1)}, Y^{(2)}) + b^{[d]}
\]
converges in distribution to a univariate normal distribution.
\item[A4.] For all $i \neq k$,
\[\lim_{d \to \infty}\Cov[u(X^{(i)}, Y^{(j)}), u(X^{(k)}, Y^{(j)})] = 0.\]
\end{itemize}
Then for $e_{ABE, k}$ as defined above, we have
\[
\lim_{d \to \infty} e_{ABE, k} = \pi_k(\sqrt{2 \iota})
\]
where
\[
\pi_k(c) = 1 - \int_{\mathbb{R}} \phi(z - c)  \Phi(z)^{k-1} dz
\]
where $\phi$ and $\Phi$ are the standard normal density function and
cumulative distribution function, respectively.}

Assumptions A1-A4 are satisfied in a variety of natural models.
One example is a multivariate Gaussian sequence model where
$X \sim N(0, \Sigma_d)$
and 
$
Y = X + E
$ with
$
E \sim N(0, \Sigma_e),
$
where $\Sigma_d$ and $\Sigma_e$ are $d \times d$ covariance matrices, and where $X$ and $E$ are independent.  Then, if $d \Sigma_d$ and $\Sigma_e$ have limiting spectra $H$ and $G$ respectively,
the joint densities $p(x, y)$ for $d = 1,\hdots, $ satisfy assumptions A1 - A4.
Another example is the multivariate logistic model, which we describe in section 3.
We further discuss the rationale behind A1-A4 in the supplement, along with the detailed proof.

\subsection{High-dimensional estimator}

The estimator we propose is
\[
\hat{I}_{HD}(M) = \frac{1}{2}(\pi_{k}^{-1}(\hat{e}_{gen, \alpha}))^2,
\]
obtained by inverting the relation \eqref{abepi}, then substituting the estimate $\hat{e}_{gen, \alpha}$ for the $e_{ABE, k}$.
As such, our estimator can be directly compared to the $\hat{I}_{Fano}$, since both are functions of $\hat{e}_{gen,\alpha}$ (Figure 1.)  

For sufficiently high-dimensional problems, $\hat{I}_{HD}$ can accurately recover $I(X; Y) > \log k$,
supposing also that the classifier $\mathcal{F}$ consistently estimates the Bayes rule.
The number of observations needed depends on the convergence rate of $\mathcal{F}$
and also the complexity of estimating $e_{gen, \alpha}$.  Therefore, without making assumptions on $\mathcal{F}$, the sample complexity is at least exponential in $I(X; Y)$.
This is because when $I(X; Y)$ is large relative to $\log(k)$, the Bayes error $e_{ABE, k}$ is exponentially small.  Hence $O(1/e_{ABE, k})$ observations in the test set are needed to recover $e_{ABE, k}$ to sufficient precision.  While the sample complexity exponential in $I(X; Y)$ is by no means ideal,
by comparison, the nonparametric estimation approaches have a complexity exponential in the dimensionality.  Hence, $\hat{I}_{HD}$ is favored over nonparametric approaches in settings
with high dimensionality and low signal-to-noise ratio.


\section{Simulation}

We compare the discriminative estimators $\hat{I}_{CM}$, $\hat{I}_{Fano}$, $\hat{I}_{HD}$
with a nonparametric estimator $\hat{I}_0$ in the following simulation.
We generate data according to a multiple-response logistic regression model, 
where
$
X \sim N(0, I_p)
$,
and $Y$ is a binary vector with conditional distribution
\[
Y_i|X = x \sim \text{Bernoulli}(x^T B_i)
\]
where $B$ is a $p \times q$ matrix. 
One application of this model might be modeling neural spike count data $Y$ arising in response
to environmental stimuli $X$.
In Figure 2 we show the sampling distributions of the four estimators as $I(X; Y)$ is varied in the interval $[0, 4]$.  We see that $\hat{I}_{CM}$, $\hat{I}_{Fano}$, and $\hat{I}_0$ indeed begin to asymptote as they approach $\log(k) = 2.995$.  In contrast, $\hat{I}_{HD}$ remains a good approximation of $I(X; Y)$ within the range, although it begins to overestimate at the right endpoint.

\begin{figure}
\begin{center}
\textbf{Estimated $\hat{I}$ vs true $I$.} 

\includegraphics[scale = 0.5, clip=true, trim=0.4in 0.5in 0 0.5in]{../info_theory_sims/fig4.pdf}
\end{center}
\caption{$p = q = 10$. $B = sI_{10}$, where $s \in [0, \sqrt{200}]$.  $r = 1000$.}
\end{figure}

\section{Discussion}



A number of adjustments can be made to $\hat{I}_{HD}$ to improve
its performance in special cases.  One can employ more sophisticated methods to
estimate $e_{ABE, k}$: for example, extrapolating from learning curves (Cortes et al. 1999).
Furthermore, depending on the risk function, one may debias or shrink the estimate $\hat{I}_{HD}$
to achieve better performance.  It can be seen from the delta method that $\hat{I}_{HD}$ is
highly biased for small $e_{gen, \alpha}$: in such settings, it may be appropriate to consider
some form of Bayesian smoothing.


\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the final paper.

\section*{References}

References follow the acknowledgments. Use unnumbered first-level
heading for the references. Any choice of citation style is acceptable
as long as you are consistent. It is permissible to reduce the font
size to \verb+small+ (9 point) when listing the references. {\bf
  Remember that you can use a ninth page as long as it contains
  \emph{only} cited references.}
\medskip

\small

[X] Gastpar, M.  Gill, P.  Huth, A.  Theunissen, F. ``Anthropic Correction of Information Estimates and Its Application to Neural Coding.'' \emph{IEEE Trans. Info. Theory}, Vol 56 No 2, 2010.

[X] A. Borst and F. E. Theunissen, ``Information theory and neural coding''
Nature Neurosci., vol. 2, pp. 947?957, Nov. 1999.

[X] L. Paninski, ``Estimation of entropy and mutual information,'' Neural
Comput., vol. 15, no. 6, pp. 1191?1253, 2003.

[X] I. Nelken, G. Chechik, T. D. Mrsic-Flogel, A. J. King, and J. W. H.
Schnupp, ``Encoding stimulus information by spike numbers and mean
response time in primary auditory cortex,'' J. Comput. Neurosci., vol.
19, pp. 199?221, 2005.

[X] Cover and Thomas.  Elements of information theory.

[X]  Muirhead.  Aspects of multivariate statistical theory.

[X] van der Vaart.  Asymptotic statistics.

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
  Exploring Realistic Neural Models with the GEneral NEural SImulation
  System.}  New York: TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
learning and recall at excitatory recurrent synapses and cholinergic
modulation in rat hippocampal region CA3. {\it Journal of
  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
