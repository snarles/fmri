One practical implication of this is that performance on
discrimination tasks can be predicted by measures of the
\emph{information content} of the input. This was the basis of Claude
Shannon's foundational work on information theory, and the definition
of \emph{mutual information.}  However, another, less-studied
application of this connection is that questions about the
\emph{information} content of a signal can be addressed by using
methods based on discrimination tasks.

Take for instance the study of \cite{oram1998theideal}.
Researchers showed a macaque monkey the image of a human face in
varying view angles, while recording the response of 32 cells in the
superior temporal to each view angle.  They then used a Bayesian
decoder to build a disciminative model for the view angle from the
data, and assessed its predictive accuracy for determining the view
angle from the responses of the 32 recorded cells.  In nature, the
brain of the macaque monkey infers the rotational angle of various
objects using lower-level neural inputs; Oram et al. mimic the natural
discriminatory behavior by training a model to achieve the same task
from cell recoding data.  

While the precise mechanisms by which low-level sensory inputs are
aggregated into high-level concepts, such as rotational angle, are
still not well-understood, the feasibility of \emph{artificially}
discriminating the signal based on such data is scientifically
meaningful, even if the artificial classifier employed has little
resemblance to the actual dynamics driving cognition in the brain.  %%mammal brain? human brain?  
This is because the existence of
\emph{any} discriminative model which can infer the signal from the
data is sufficient to establish that \emph{information} about the
signal is contained within the sources of the data--e.g. the neurons
measured.

%% limitations of classification: e.g. can only provide lower bound?

%% Multi-class classification was first studied by SHannon
%% in random codes

%While multi-class classification is a rapidly developing field within
%machine learning, the problem of discriminating inputs according to
%discrete classes had been studied even before the advent of artificial
%intelligence: arguably, it was 

That is to say, the concept of \emph{information} indicates the
potentiality to achieve accurate discrimination.  This was first
formalized by the pioneers of information theory, most notably Claude
Shannon, who laid the foundation for the field in his seminal 1948
paper ``A mathematical theory of communication.''  Shannon defined a
quantity called the \emph{mutual information}, which measures the
information carried by a recieved signal $X$ about a transmitted
signal $Y$.  From Shannon's celebrated noisy channel theorem, we have
a formal relationship between mutual information and discrimination:
one that we will discuss in section \ref{sec:mi}.


Shannon, along
with other pioneers of information theory such as Robert M. Fano and
Norbert Weiner, recognized that the information content of a signal
depended on how many different messages it can plausibly convey.



Furthermore, it was Shannon who first recognized that when the signal
is corrupted by noise, the amount of information \emph{lost} depends
on how reliably the original message can be recovered from the noisy
input--in other words, how well the reciever can \emph{discriminate}
the original message on the basis of the noise-corrupted recieved
message.  %% maybe add more details, redundancy of English, etc.

Indeed, we see that key to the development of information theory is
the study of the \emph{noisy-channel decoding problem}, which is
illustrated in figure \ref{fig:mcc_vs_it}.  % Shannon's key result: decoding accuracy and mutual information

If we compare the discrimination tasks defined in multi-class
classification and information theory, we find much similarity, but
also a few important differences.  Figure \ref{fig:mcc_vs_it} displays
the schematic diagrams of the general multi-class classification
problem and the setup for the noisy channel considered by Shannon.
%% If we put the figure here, probably will need some more explanation

%% we should mention mutual information earlier
From figure \ref{fig:mcc_vs_it}, we can see major similarities between
the multi-class classification problem and the noisy decoding problem
studied in information theory: both involve inferring some latent
variable $Y$ on the basis of observed $X$.  We will go into great
detail about the similarities and differences in section
\ref{sec:rand_code_rand_class}.  However, despite these similarities,
it has historically been the case that the machine learning literature
and the information theory literature diverge in terms of the metric
used to characterize performance in the discrimination task.  In
multi-class learning, the performance is generally characterized by
the \emph{accuracy} of the classification--the probability that the
label is assigned correctly.  However, in information theory, a
quantity called the \emph{mutual information}, which was invented by
Claude Shannon, is used to characterize the quality of the noisy
channel with regards to achievable decoding performance. %% jargony phrase, due to the idea being difficult to express





Furthermore, as we might expect from the close connection between
random codes and randomized classification, we are able to form links
between multi-class classification and information theory.  Therefore,
Chapters 4 and 5 leverage the link between randomized classification
and information theory in order to develop new estimators of mutual
information in high-dimensional datasets consisting of pairs ob
observations $(X, Y)$.  While Chapter 4 works in a setting where a
true, sparse relationship between $Y$ and $X$ is assumed to be known,
Chapter 5 makes a different assumption, which is that $X$ is
high-dimensional, and that its components are not too dependent.

In the following sections, we review the relevant background for
information theory, the applications of information theory to
neuroscience, and multi-class classification, then introduce the
connection between random code models in information theory and
randomized classification.


In multi-class classification, we may assume without loss of generality that the data has been generated in the following manner:
\begin{enumerate}
\item First, a label $Y$ is drawn according to some distribution from the label set $\{y_1,\hdots, y_k\}$.
\item Secondly, the new observation $X$ is drawn according to the unknown conditional distribution $F_{X|Y}$.
\item Finally, an estimated label $\hat{Y} = h(X)$ is obtained
  according to a data-dependent classification rule, $h$.
  Typically, $h$ is determined by fitting a model to training data.
\end{enumerate}
In the particular application, the above description may not match the
\emph{causal relationship} between $X$ and $Y$: however, whether $X$
is drawn conditional on $Y$, or $Y$ is drawn conditional on $X$, or
that $(X, Y)$ are drawn from some joint distribution, makes no
difference from the theoretical standpoint, since only the statistical
(and not causal) properties of the joint distribution $(X, Y)$ are
relevant for determining the peformance of the classification rule $h(X)$.


While Shannon's theory of information was motivated by the problem of
designing communications system, the applicability of mutual
information was quickly recognized by neuroscientists.  Only four
years after Shannon's seminal paper in information theory (1948),
McKay and McCullough (1952) inaugurated the application of mutual
information to neuroscience.  

Since then, mutual information has enjoyed a
celebrated position in both experimental and theoretical neuroscience.
Experimentally, mutual information has been used to detect strong
dependencies between stimulus features and features derived from
neural recordings, which can be used to draw conclusions about the
kinds of stimuli that a neural subsystem is designed to detect, or to
distinguish between signal and noise in the neural output.
Theoretically, the assumption that neural systems maximize mutual
information between salient features of the stimulus and neural output
has allowed scientists to predict neural codes from signal processing
models: for instance, the center-surround structure of human retinal
neurons matches theoretical constructions for the optimal filter based
on correlations found in natural images [cite].

In classification, $Y$ is a
categorical vector: in \emph{binary} classification $Y$ can take one
of two possible values, while in multi-class classification, $Y$ can
take more than two values.  In \emph{regression}, another supervised
learning task, $Y$ takes a numeric value.  In \emph{multiple-response
  regression}, or \emph{multivariate regression}, $Y$ is a numeric
vector.  In \emph{multi-label classification}, $Y$ is a vector of
categorical values.

In neuroscience studies, where $\bX$ is the controlled stimulus, and
$\bY$ is the neural activity, the two mirror pairs \eqref{eq:ce_ident}
and \eqref{eq:ce_ident2} have different interpretations.  Rather than
providing a basis for practical estimation, \eqref{eq:ce_ident2}
provides an \emph{interpretation} of the mutual information.  


\section{Random codes and random classification}

A random code model in information theory is one where given some
distribution $\nu$ (typically a uniform distribution) on the space of
transmittable signals $\mathcal{Y}$, we posit that the decoder $g(M)$
is randomly generated, by letting $g(1),\hdots, g(m)$ be identically
and independently assigned to random draws from $\nu$.  For example,
when $\mathcal{Y}$ is the space of $m$-length binary strings, the
encoder $g$ maps indices $1, \hdots, m$ to random $m$-length binary
strings.  The purpose of the random code model is usually to establish
a lower bound on achievable accuracy given some constraints on the
signal space.

Meanwhile, a randomized classification model is one where the label
set $\{y^{(1)},\hdots, y^{(k)}\}$ is not fixed, but randomly sampled.
One defines a label space $\mathcal{Y}$, a family of conditional
distributions $\{F_{X|y}\}_{y \in \mathcal{Y}}$, and a distribution
$\nu$ on $\mathcal{Y}$.  Then the randomized classification model is a
classification task obtained by drawing labels $\{Y^{(1)},\hdots,
Y^{(k)}\}$ iid from $\nu$, generating training data for those
particular labels by drawing observations $X_i^{(j)} \sim
F_{X|Y^{(j)}}$, and where the problem is to construct a classification
rule for assigning new observations $X$ which are drawn from the
mixture
\[
X \sim \frac{1}{k}\sum_{i=1}^k F_{X|Y^{(k)}}
\]
to one of the labels $\{Y^{(1)},\hdots, Y^{(k)}\}.$ As we will see
throughout the thesis, the randomized classification model can be
naturally applied to a large number of multi-class classification
applications, such as facial recognition, where the labels
(e.g. people) can be justifiably modelled as random draws from some
population.  And, even in the majority of classification problems
where the labels cannot be assumed to come from an iid sample,
randomized classification models can still be applied to provide
intuition for the original problem.

The link between random code models and randomized classification
models should now be apparent. Define $Y_i = g(i)$ in the random code
model, so that $Y_i$ are iid drawn from $\nu$.  Fixing a particular
realization of $Y_1,\hdots, Y_k$, the decoding problem is then
evidently a multi-class classification problem with labels
$\{Y_1,\hdots, Y_k\}$.  The only difference between the two models is
that the conditional distributions $F_{X|Y}$ are assumed to be known
in the random code model, and assumed unknown in the randomized
classification model.

But happens when we assume that $F_{X|Y}$ is known, in the multi-class
classification problem?  In fact, it is common to consider the case of
known $F_{X|Y}$ in the machine learning literature, because the
resulting accruacy gives an \emph{upper bound} on achievable
performance in the multi-class classification problem.  This is
because once $F_{X|Y}$ is known, it is possible to define the
\emph{optimal} classification rule, or \emph{Bayes} classification
rule $h_{Bayes}$.  For example, supposing that the performance
criterion is to minimize the zero-one risk $\Pr[\hat{Y} \neq Y]$, then
the Bayes rule is to assign $X$ to the label with the highest
posterior density,
\[
h_{Bayes}(X) = \text{argmax}_{y \in \{y_1, \hdots, y_k\}} f_{X|y}(x) \pi(y)
\]
where $\pi(y)$ is the prior probability of $Y = y$.  Therefore, it is
the same thing to study the Bayes accuracy in a randomized
classification model and the decoding accuracy of a randomized code in
a noisy channel.

Indeed, the analysis of the Bayes accuracy of a randomized
classification model forms the subject of Chapters 2 and 3.  Due to
the aforementioned equivalence, it can be said that a large body of
work dealing with the Bayes accuracy of randomized classification
problems exists in information theory: however, the majority of such
works deal with the limit as $k \to \infty$, and to our knowledge no
analysis has been done for the case of finite $k$, which is the
relevant scenario for multi-class classification.  There is good
reason for this lacuna in the information theory literature, which is
that since information theorists are concerned with understanding the
properties of \emph{optimal} coding schemes, it follows that
randomized coding schemes are only interesting insofar as that they
give good approximations to optimal coding schemes.  However, in the
classification setting, we may be interested in studying randomized
classification for its own sake, and not merely as a means to obtain
lower bounds or estimates for another problem.  Therefore, Chapter 2
presents novel results on the statistical properties of the Bayes
accuracy in the randomized classification model.  Meanwhile, Chapter 3
studies an interesting application of the randomized classification
problem, which is to analyze the dependence of the classification
accuracy on the size of the label set.  This analysis yields a novel
method for `performance extrapolation' in real-world classification
problems, meaning that our method can be used to estimate the
classification accuracy on a large multi-class classification problem
using data from only a subsample of the classes in the larger problem.


