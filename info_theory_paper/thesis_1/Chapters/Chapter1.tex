% Chapter 1

\chapter{Multi-class classification, random codes, and information} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------


\section{Introduction}

The concepts of \emph{information} and \emph{discrimination} are
linked at a fundamental level.  A statistical hypothesis test is
\emph{informative} because it provides evidence that the data behaves
according to a certain hypothesis rather than another: it allows us to
\emph{discriminate} between two potential possibilities.  More
generally, a true message contains \emph{information} if it allows the
reciever to understand that the world is in a certain state and not
another: it conveys the fact that out of two possible sets of world
states, that the reciever is in one state rather than another.  The
link between information and discrimination can also be formalized
using the tools of measure theory.  Supposing $\Omega$ is a
probability space defined with respect to a $\sigma$-algebra
$\mathcal{F}$, we can represent our state of knowledge with a
filtration (or sub-$\sigma$-algebra) $\mathcal{F}' \subseteq
\mathcal{F}$.  Complete knowledge (zero uncertainty) is represented by
the full $\sigma$-algebra: that is, $\mathcal{F}' = \mathcal{F}$.
Partial knowledge is represented by a coarser filtration, $\mathcal{F}
\subset \mathcal{F}'$.  The filtration, of course, indicates that our
knowledge is sufficient to \emph{discriminate} the outcome space
$\Omega$ into a number of finitely or infinitely many categories.  The
more information we have, (or, the closer we come to complete
knowledge of the outcome), the more finely we can discriminate the
realized outcomes given by $\omega \in \Omega$.
%% probably need to elaborate on sigma-algebra

A system is \emph{intelligent} if it uses this discriminatory
information to choose the optimal response to the situation.  A
primitive organism may classify percieved objects in its environment
as either beneficial (potential food and resources) or harmful (toxins
and predators), and intelligently respond by means of pursuing the
former and avoiding the latter.  Complex organisms, like humans, not
only discriminate in order to make immediate decisions, but also
categorize objects in the world in order to perform intermediate
calculations and carry out contextual reasoning.  
%% example?

%% Introduce examples of classification and multi-class classification

Similarly, \emph{artificially intelligent} algorithms and agents will
also discriminate input data into categories, either to (i) make
immediate decisions, or (ii) to facillitate intermediate calculations
and optimizations, like intelligent organisms, or (iii) to communicate
the labels of the categories to human users or other algorithms.
Accordingly, a large swath of the field of artificial intelligence
consists of various types of discrimination tasks, from natural
language processing, to object recognition, to game playing, to
multi-class classification.  Among these types of tasks, multi-class
classification is the simplest and the most generally applicable
type of discrimination problem.  Supposing the input data $x$ is
associated with one of $k$ pre-defined classes or labels
$\{y_1,\hdots, y_k\}$, the problem of multi-class classification is to
determine a rule for assigning new inputs $x$ to the correct label.
Examples of multi-class classification applications include:
\begin{itemize}
\item Optical character recognition: labeling bitmaps of handwritten characters with the corresponding character.
\item Biomedical diagnoses: assigning possible diagnoses to patients based on biological measurements and demographic characteristics.
\item Image annotation: assigning descriptive words to images on the internet.
\end{itemize}

%% Multi-class classification was first studied by SHannon (?) (was it Shannon?  look up the first ref to gaussian random codes)
%% in random codes
While multi-class classification is a rapidly developing field within
machine learning, the problem of discriminating inputs according to
discrete classes had been studied even before the advent of artificial
intelligence: arguably, it was Claude Shannon who developed some the
earliest theory pertaining to multi-class classification in his
seminal 1948 paper ``A mathematical theory of communication,'' which
laid the foundation for the field of information theory.  Shannon,
along with other pioneers of information theory such as Robert M. Fano
and Norbert Weiner, recognized that the information content of a
signal depended on how many different messages it can plausibly
convey.  Furthermore, it was Shannon who first recognized that when
the signal is corrupted by noise, the amount of information
\emph{lost} depends on how reliably the original message can be
recovered from the noisy input--in other words, how well the reciever
can \emph{discriminate} the original message on the basis of the
noise-corrupted recieved message.  %% maybe add more details, redundancy of English, etc.

\tikzstyle{block} = [rectangle, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cloud} = [ellipse, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
    
\begin{figure}
\centering
\begin{tabular}{ccc}

Multi-class classification & & Information Theory\\

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (init1) {label $Y$};
    \node [cloud, below of=init1] (init2) {distribution $F_{X|Y}$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {classification rule $h(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{Y}$};
    % Draw edges
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

& & 

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (initA) {message $M$};
    \node [cloud, below of=initA] (initB) {encoder $g(M)$};
    \node [block, below of=initB] (init1) {encoded message $Y$};
    \node [cloud, below of=init1] (init2) {noisy channel $F_{X|Y}$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {decoder $d(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{M}$};
    % Draw edges
    \path [line] (initA) -- (initB);
    \path [line] (initB) -- (init1);
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

\end{tabular}
\caption{Comparing the discrimnation tasks in multi-class classification and information theory.}
\label{fig:mcc_vs_it}
\end{figure}

If we compare the discrimination tasks defined in multi-class
classification and information theory, we find much similarity, but
also a few important differences.  Figure \ref{fig:mcc_vs_it} displays
the schematic diagrams of the general multi-class classification
problem and the setup for the noisy channel considered by Shannon.

In multi-class classification, we may assume without loss of generality that the data has been generated in the following manner:
\begin{enumerate}
\item First, a label $Y$ is drawn according to some distribution from the label set $\{y_1,\hdots, y_k\}$.
\item Secondly, the new observation $X$ is drawn according to the unknown conditional distribution $F_{X|Y}$.
\item Finally, an estimated label $\hat{Y} = h(X)$ is obtained
  according to a data-dependent classification rule, $h$.
  Typically, $h$ is determined by fitting a model to training data.
\end{enumerate}
In the particular application, the above description may not match the
\emph{causal relationship} between $X$ and $Y$: however, whether $X$
is drawn conditional on $Y$, or $Y$ is drawn conditional on $X$, or
that $(X, Y)$ are drawn from some joint distribution, makes no
difference from the theoretical standpoint, since only the statistical
(and not causal) properties of the joint distribution $(X, Y)$ are
relevant for determining the peformance of the classification rule $h(X)$.

Meanwhile, in the noisy channel model, we assume that the sender wants
to transmit message $M$, out of a finite set of possible messages
$\mathcal{M} = \{1,\hdots, m\}$.  The message must be encoded into a
signal $Y \in \mathcal{Y}$, which is sent through a stochastic channel
$F_{X|Y}$.  Given that a signal $y$ is sent through the channel, the
reciever observes a signal $X$ drawn from the distribution $F_{X|y}$.
The communications problem is to design an encoding function $g(M)$,
which is an injective map from messages $\{1,\hdots,m\}$ to signals in
$\mathcal{M}$, and a corresponding decoding function $d(X)$ which
infers the message $\{1,\hdots, m\}$ from the recieved signal $X$.

Therefore, we see that in both the multi-class classification problem
and the noisy channel model present examples of discrimination
problems where one must recover some latent variable $Y$ from
observations $X$, where $X$ is related to $Y$ through the family of
conditional distributions $F_{X|Y}$.  One difference is that while in
multi-class classification, $F_{X|Y}$ is unknown and has to be
inferred from data, in the noisy channel model, the stochastic
properties of the channel $F_{X|Y}$ are usually assumed to be known.
For example, for a binary channel where $Y$ consists of an $m$-length
binary string, a commonly studied channel $F_{X|Y}$ generates $X$ by
randomly flipping bits in $Y$ independently with some probability
$\epsilon$.  A second difference is that in the noisy channel model,
there is a choice in how to specify the encoding function $g(M)$,
which affects subsequent performance.  Finally, in the broader
research context, machine learning research has traditionally focused
on multi-class problems with relatively few classes, while information
theory tends to consider problems in asymptotic regimes where the
number of possible messages $m$ is taken to infinity. These
differences were sufficient to explain why little overlap exists in
the respective literatures between multi-class classification and the
noisy channel model.  

However, an interesting development in the machine learning community
has been the application of multi-class classification to problems
with increasingly large and complex label sets.  Consider the
following timeline of representative papers in the multi-class
classification literature:
\begin{itemize}
\item Fisher's Iris data set, \cite{fisher1936use}, $K = 3$ classes
\item Letter recognition, \cite{frey1991letter}, $K = 26$ classes
\item Michalski's soybean dataset, \cite{mickalstd1980learning}, $K = 15$ classes
\item The NIST handwritten digits data set, \cite{grother1995nist}, $K = 10$ classes
\item Phoneme recognition on the TIMIT datset, \cite{clarkson1999use}, $K = 39$ classes
\item Object categorization using Corel images, \cite{duygulu2002object} $K = 371$ classes
\item Object categorization for ImageNet dataset, \cite{deng2010does}, $K = 10,184$ classes
\item The 2nd Kaggle large-scale hierarchical text classification challenge (LSHTC), \cite{partalas2015lshtc}, $K = 325,056$
\end{itemize}

Yet, in theoretical work it is useful to study an \emph{upper bound}
on achievable performance in the multi-class classification problem,
by considering the performance of the \emph{optimal} classification
rule $h(X)$.  This is the Bayes classifer $h_{Bayes}$, which can be
obtained from knowledge of the conditional distributions $F_{X|Y}$ and
marginal distribution of $Y$.  For instance, supposing that the
performance criterion is to minimize the zero-one risk $\Pr[\hat{Y}
  \neq Y]$, then the Bayes rule is to assign $X$ to the label with the
highest posterior density,
\[
g_{Bayes}(X) = \text{argmax}_{y \in \{y_1, \hdots, y_k\}} f_{X|y}(x) \pi(y)
\]
where $\pi(y)$ is the prior probability of $Y = y$.  Since the Bayes
rule yields the best possible performance and requires knowledge of
$F_{X|Y}$ for its construction, the assumption that one has access to
the Bayes classifier is effectively equivalent to assuming that one
has knowledge of $F_{X|Y}$.




%% Make the connection between random codes and randomized classification,
%% point out why randomized classification is a good model for some contemporary problems

%% Link back to Shannon and information theory.  We develop further links between randomized classification and information theory.


\section{Supervised learning}

The generalization error of the learner as a statistic.

\subsection{General characaterization of supervised learning}

\section{Mutual information}

\subsection{Definition and history}

\subsection{Usage in neuroscience}

\section{Generalizations of information}

\subsection{Information axioms}

\subsection{Information coefficients based on supervised learning}

