% Chapter 1

\chapter{Multi-class classification, random codes, and information} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------


\section{Introduction}

The concepts of \emph{information} and \emph{discrimination} are
linked at a fundamental level.  A statistical hypothesis test is
\emph{informative} because it provides evidence that the data behaves
according to a certain hypothesis rather than another: it allows us to
\emph{discriminate} between two potential possibilities.  More
generally, a true message contains \emph{information} if it allows the
reciever to understand that the world is in a certain state and not
another: it conveys the fact that out of two possible sets of world
states, that the reciever is in one state rather than another.  The
link between information and discrimination can also be formalized
using the tools of measure theory.  Supposing $\Omega$ is a
probability space defined with respect to a $\sigma$-algebra
$\mathcal{F}$, we can represent our state of knowledge with a
filtration (or sub-$\sigma$-algebra) $\mathcal{F}' \subseteq
\mathcal{F}$.  Complete knowledge (zero uncertainty) is represented by
the full $\sigma$-algebra: that is, $\mathcal{F}' = \mathcal{F}$.
Partial knowledge is represented by a coarser filtration, $\mathcal{F}
\subset \mathcal{F}'$.  The filtration, of course, indicates that our
knowledge is sufficient to \emph{discriminate} the outcome space
$\Omega$ into a number of finitely or infinitely many categories.  The
more information we have, (or, the closer we come to complete
knowledge of the outcome), the more finely we can discriminate the
realized outcomes given by $\omega \in \Omega$.
%% probably need to elaborate on sigma-algebra

A system is \emph{intelligent} if it uses this discriminatory
information to choose the optimal response to the situation.  A
primitive organism may classify percieved objects in its environment
as either beneficial (potential food and resources) or harmful (toxins
and predators), and intelligently respond by means of pursuing the
former and avoiding the latter.  Complex organisms, like humans, not
only discriminate in order to make immediate decisions, but also
categorize objects in the world in order to perform intermediate
calculations and carry out contextual reasoning.  
%% example?

%% Introduce examples of classification and multi-class classification

Similarly, \emph{artificially intelligent} algorithms and agents will
also discriminate input data into categories, either to (i) make
immediate decisions, or (ii) to facillitate intermediate calculations
and optimizations, like intelligent organisms, or (iii) to communicate
the labels of the categories to human users or other algorithms.
Accordingly, a large swath of the field of artificial intelligence
consists of various types of discrimination tasks, from natural
language processing, to object recognition, to game playing, to
multi-class classification.  Among these types of tasks, multi-class
classification is the simplest and the most generally applicable
type of discrimination problem.  Supposing the input data $x$ is
associated with one of $k$ pre-defined classes or labels
$\{y_1,\hdots, y_k\}$, the problem of multi-class classification is to
determine a rule for assigning new inputs $x$ to the correct label.
Examples of multi-class classification applications include:
\begin{itemize}
\item Optical character recognition: labeling bitmaps of handwritten characters with the corresponding character.
\item Biomedical diagnoses: assigning possible diagnoses to patients based on biological measurements and demographic characteristics.
\item Image annotation: assigning descriptive words to images on the internet.
\end{itemize}

%% Multi-class classification was first studied by SHannon (?) (was it Shannon?  look up the first ref to gaussian random codes)
%% in random codes
While multi-class classification is a rapidly developing field within
machine learning, the problem of discriminating inputs according to
discrete classes had been studied even before the advent of artificial
intelligence: arguably, it was Claude Shannon who developed some the
earliest theory pertaining to multi-class classification in his
seminal 1948 paper ``A mathematical theory of communication,'' which
laid the foundation for the field of information theory.  Shannon,
along with other pioneers of information theory such as Robert M. Fano
and Norbert Weiner, recognized that the information content of a
signal depended on how many different messages it can plausibly
convey.  Furthermore, it was Shannon who first recognized that when
the signal is corrupted by noise, the amount of information
\emph{lost} depends on how reliably the original message can be
recovered from the noisy input--in other words, how well the reciever
can \emph{discriminate} the original message on the basis of the
noise-corrupted recieved message.  %% maybe add more details, redundancy of English, etc.

\tikzstyle{block} = [rectangle, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cloud} = [ellipse, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
    
\begin{figure}
\centering
\begin{tabular}{ccc}

Multi-class classification & & Information Theory\\

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (init1) {label $Y$};
    \node [cloud, below of=init1] (init2) {distribution $F_{X|Y}$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {classification rule $h(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{Y}$};
    % Draw edges
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

& & 

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (initA) {message $M$};
    \node [cloud, below of=initA] (initB) {encoder $g(M)$};
    \node [block, below of=initB] (init1) {encoded message $Y$};
    \node [cloud, below of=init1] (init2) {noisy channel $F_{X|Y}$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {decoder $d(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{M}$};
    % Draw edges
    \path [line] (initA) -- (initB);
    \path [line] (initB) -- (init1);
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

\end{tabular}
\caption{Comparing the discrimnation tasks in multi-class classification and information theory.}
\label{fig:mcc_vs_it}
\end{figure}

If we compare the discrimination tasks defined in multi-class
classification and information theory, we find much similarity, but
also a few important differences.  Figure \ref{fig:mcc_vs_it} displays
the schematic diagrams of the general multi-class classification
problem and the setup for the noisy channel considered by Shannon.

In multi-class classification, we may assume without loss of generality that the data has been generated in the following manner:
\begin{enumerate}
\item First, a label $Y$ is drawn according to some distribution from the label set $\{y_1,\hdots, y_k\}$.
\item Secondly, the new observation $X$ is drawn according to the unknown conditional distribution $F_{X|Y}$.
\item Finally, an estimated label $\hat{Y} = h(X)$ is obtained
  according to a data-dependent classification rule, $h$.
  Typically, $h$ is determined by fitting a model to training data.
\end{enumerate}
In the particular application, the above description may not match the
\emph{causal relationship} between $X$ and $Y$: however, whether $X$
is drawn conditional on $Y$, or $Y$ is drawn conditional on $X$, or
that $(X, Y)$ are drawn from some joint distribution, makes no
difference from the theoretical standpoint, since only the statistical
(and not causal) properties of the joint distribution $(X, Y)$ are
relevant for determining the peformance of the classification rule $h(X)$.

In the noisy channel model, we also see the appearance of an unknown
encoded message $Y$, a conditional distribution $F_{X|Y}$ which is now
referred to as a \emph{noisy channel}, and the observed $X$ drawn from
the conditional distribution.  However, the differences between the
noisy channel model and the multi-class classification problem are:
\begin{itemize}
\item While in multi-class classification, $F_{X|Y}$ is unknown and
  has to be inferred from data, in the noisy channel model, the
  stochastic properties of the channel $F_{X|Y}$ are usually assumed
  to be known.  For example, for a binary channel where $Y$ consists
  of an $m$-length binary string, a commonly studied channel $F_{X|Y}$
  generates $X$ by randomly flipping bits in $Y$ independently with
  some probability $\epsilon$.
\item In the noisy channel model, the goal is to recover a message
  $M$, rather than the encoded message $Y$.  However, in the problem
  setup, the sender is allowed to choose a deterministic encoder
  function which produces $Y$ as a deterministic function of $M$.  And
  since we assume that the reciever knows the encoder function, the
  problem of recovering $M$ from $X$ reduces to the problem of
  recovering $Y$ from $X$.  Yet, an important detail is that the
  distribution of $Y$ depends on both the distribution of $M$ and the
  chosen encoder, and the reciever should use this information, along
  with the known channel properties, in choosing a decoder $d(X)$.
\end{itemize}
In general, we can say that both the multi-class classification
problem and the noisy channel model present examples of discrimination
problems where one must recover some latent variable $Y$ from
observations $X$; however, the two problems diverge on the matter of
whether or not the stochastic relationship $F_{X|Y}$ is known, and
whether or not there are any degrees of freedom in controlling the
distribution of $Y$ (by means of choosing the encoder $g(M)$ in the
noisy channel model.)

Yet, in theoretical work it is useful to study an \emph{upper bound}
on achievable performance in the multi-class classification problem,
by considering the performance of the \emph{optimal} classification
rule $h(X)$.  This is the Bayes classifer $h_{Bayes}$, which can be
obtained from knowledge of the conditional distributions $F_{X|Y}$ and
marginal distribution of $Y$.  For instance, supposing that the
performance criterion is to minimize the zero-one risk $\Pr[\hat{Y}
  \neq Y]$, then the Bayes rule is to 




%% Make the connection between random codes and randomized classification,
%% point out why randomized classification is a good model for some contemporary problems

%% Link back to Shannon and information theory.  We develop further links between randomized classification and information theory.


\section{Supervised learning}

The generalization error of the learner as a statistic.

\subsection{General characaterization of supervised learning}

\section{Mutual information}

\subsection{Definition and history}

\subsection{Usage in neuroscience}

\section{Generalizations of information}

\subsection{Information axioms}

\subsection{Information coefficients based on supervised learning}

