% Chapter 1

\chapter{Multi-class classification, random codes, and information} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------


\section{Introduction}

The study of naturally intelligent systems, and the study and design
of artificially intelligent systems, form two major intertwining
domains of modern research.  Here, we take \emph{intelligence} to
refer to the property of a system by which it can modulate its
reaction to external inputs in a purposeful manner.  Therefore, by
this definition, an amoeba which seeks food and avoids toxins is
intelligent, as is a gene-regulation network, or a mammalian brain.
Similarly, artificially intelligent algorithms which implement
automated decision-making rules in response to external inputs also
satisfy this definition of intelligence.

A primitive organism may classify percieved objects in its environment
as either beneficial (potential food and resources) or harmful (toxins
and predators), and intelligently respond by means of pursuing the
former and avoiding the latter.  %% note that this example is redundant
Complex organisms, like humans, not
only discriminate in order to make immediate decisions, but also
categorize objects in the world in order to perform intermediate
calculations and carry out contextual reasoning.
%% example?


%% Introduce examples of classification and multi-class classification

Similarly, \emph{artificially intelligent} algorithms and agents will
also discriminate input data into categories.  Very often, these types
of classification and recognition tasks mimic and automate
discrimination tasks that humans already perform, such as:
\begin{itemize}
\item Optical character recognition: recognizing characters from handwritten glyphs.
\item Facial recognition: identifying individuals from images of their faces.
\item Object recognition: identifying objects in photographs and labelling them with the appropriate keyword.
\end{itemize}
However, classification can also be performed on novel types of tasks
where no human substitute exists, such as diagnosis of cancer
phenotype based on thousands of gene expression levels.

While a major use of artificially intelligent classifiers is to
automate routine tasks or to categorize complex data, it is also
scientifically interesting to compare the performance of naturally
intelligent agents (human or animal subjects) versus artificially
intelligent agents on discrimination tasks.  One reason is that human
benchmarks on complex classification tasks, such as object
recognition, are often non-trivial to beat, and provide a useful
reference to quantify progress in machine learning.  Yet, another
reason is that the attempt to engineer an artificially intelligent
solution to a natural recognition problem provides insight into the
nature of intelligence in human and animal brains.  We still cannot
fully explain the human capability to quickly locate and identify
objects in a natural scene, but research into the mechanisms of
vision, and attempts to replicate human capabilities for object
recognition, have already provided crucial insight into the
hierarchical nature of mammalian vision.  %% needs citation

However, a potentially puzzling aspect of this approach, is that the
structure or dynamics of the artificially intelligent classifier need
not imitate the dynamics of the biological system for the modelling
approach to be useful.  Computational neuroscientists have developed
biological realistic models of neural signalling activity which could
be employed to simulate the neural decision-making process from input
to output, at a very high level of versimilitude.  However, it is
often more useful in practice to model the decision-making process
with much simpler statistical models, such as linear regression.  Even
relatively complex machine learning models, such as deep neural
networks, represent a vast simplification from biological dynamics.
Even when an artificial learning algorithm has a much different
structure than the biological mechanism being studied, its performance
on a discrimination task can yield insight into the system of
interest.  The whole reason for this is that the quantity of interest
does not depend on the dynamics or the specific mechanisms of the
system.  Rather, the quantity of interest is the \emph{informational
  content} of the signal, which is defined independently of a choice
of a specific classification algorithm or natural mechanism.  [NOTE:
  This claim is too strong, because sometimes we do think that the
  artificial classifier is a good coarse model for the system, and the
  fact that information is what we really care about is only really
  apparent at the implicit level.]



The concept of intelligence is then further intertwined with two other
highly related concepts: the ability to \emph{discriminate}, and the
ability to process \emph{information.}



The concepts of \emph{information} and \emph{discrimination} are
linked at a fundamental level.  A statistical hypothesis test is
\emph{informative} because it provides evidence that the data behaves
according to a certain hypothesis rather than another: it allows us to
\emph{discriminate} between two potential possibilities.  More
generally, a true message contains \emph{information} if it allows the
reciever to understand that the world is in a certain state and not
another: it conveys the fact that out of two possible sets of world
states, that the reciever is in one state rather than another.  The
formalism of measure-theoretic probability theory provides yet another
example of the conceptual link between information and
discrimination\footnote{Supposing $\Omega$ is a probability space
  defined with respect to a $\sigma$-algebra $\mathcal{F}$, we can
  represent our state of knowledge with a filtration (or
  sub-$\sigma$-algebra) $\mathcal{F}' \subseteq \mathcal{F}$.
  Complete knowledge (zero uncertainty) is represented by the full
  $\sigma$-algebra: that is, $\mathcal{F}' = \mathcal{F}$.  Partial
  knowledge is represented by a coarser filtration, $\mathcal{F}
  \subset \mathcal{F}'$.  The filtration, of course, indicates that
  our knowledge is sufficient to \emph{discriminate} the outcome space
  $\Omega$ into a number of finitely or infinitely many categories.
  The more information we have, (or, the closer we come to complete
  knowledge of the outcome), the more finely we can discriminate the
  realized outcomes given by $\omega \in \Omega$.}.
%% transition here to the applications of such a link
One practical implication of this is that performance on
discrimination tasks can be predicted by measures of the
\emph{information content} of the input. This was the basis of Claude
Shannon's foundational work on information theory, and the definition
of \emph{mutual information.}  However, another, less-studied
application of this connection is that questions about the
\emph{information} content of a signal can be addressed by using
methods based on discrimination tasks.  



%% we played a trick where natural intelligence = brain, but remember that an 'intelligent system' could also be a gene regulation system or something

%% the purpose of this paragraph is to make the link between discrimination and information
Either natural or artificially intelligence discrimination systems
must rely on input data that is \emph{informative} of the optimal
response.  In natural environments, mammals rely on a combination of
visual, auditory, and tactile cues to recognize potential threats in
the environment.  Mammalian brains integrate all of this sensory
information in order to make more rapid and reliable decisions.
Generally, increased diversity and quality of the available sources of
information will lead to more accurate discrimination.
%% possibly mention the study about information integration (visual + audio) to identify objects?

This link between the information content of the sensory input and the
achievable discrimination accuracy was first formalized by Claude
Shannon via the concept of \emph{mutual information.}  The mutual
information $I(X; Y)$ quantifies the information content that an input
$X$ holds abut a target of interest, $Y$.  For instance, in the case
of facial identification, the discrimination target $Y$ is a label
corresponding to the identity of the person, and $X$ is an image of
the individual's face.  An image corrupted by noise holds less
information, and correspondingly leads to lower classification
accuracies.
%% good way to introduce mutual information
%% need to decide on the order of introducing these various concepts
%% - natural and artificial discrimination systems
%  - mutual information
%  - studying parts of systems
%  - using artificial systems to model natural ones
%  - discrimination as a measure of information


A key problem in the study of intelligent systems is to understand how
the various components of the system specialize for different tasks,
and how those components cooperate to perform complex tasks.
Specializing to the brain, a key problem is to discover how to divide
the brain into discrete ``modules'' responsible for differing types of
cognitive tasks.  Given observed behavior--for instance, the human
ability to recognize faces--we can conclude that the brain, as a
whole, possesses the ability to discriminate faces based on visual
input.  However, the next step is to establish which \emph{parts} of
the brain are neccessary--and which parts are unneccessary--for the
particular discrimination task of facial recognition.  Ignoring
ethical and practical considerations, an ideal approach might be to
selectively disable parts of the brain while testing to see whether
the recognition ability is preserved.  Indeed, some of the early
results on functional specialization, such as the discovery of Broca's
area for speech production, resulted from studies on patients with
brain lesions.  However, given the important ethical and practical
limitations on the possibility shutting down parts of the brain, a far
more flexible approach is to leverage the power of \emph{artificially
  intelligent} discriminators as a model for intelligent subsystems. 
%% def unclear.  what we mean is that performance of artificially int subsystems can be informative of how the natural intelligence works


%% put the facial recognition exampl here?

%COMMENTING OUT: m either to (i) make
%immediate decisions, or (ii) to facillitate intermediate calculations
%and optimizations, like intelligent organisms, or (iii) to communicate
%the labels of the categories to human users or other algorithms.
%Accordingly, a large swath of the field of artificial intelligence
%consists of various types of discrimination tasks, from natural
%language processing, to object recognition, to game playing, to
%multi-class classification.  Among these types of tasks, multi-class
%classification is the simplest and the most generally applicable
%type of discrimination problem.  Supposing the input data $x$ is
%associated with one of $k$ pre-defined classes or labels
%$\{y_1,\hdots, y_k\}$, the problem of multi-class classification is to
%determine a rule for assigning new inputs $x$ to the correct label.

%Other examples of multi-class classification applications include:
%\begin{itemize}
%\item Biomedical diagnoses: assigning possible diagnoses to patients based on biological measurements and demographic characteristics.
%\item Image annotation: assigning descriptive words to images on the internet.
%\end{itemize}

%% Write about neuroscience application here
%An interesting development in science is the use of \emph{artificial
%  intelligence}--that is, machine learning and classification
%algorithms, to mimic \emph{natural intelligence.}  This is the case in
%neuroscience.  
%Take for instance the study of \cite{oram1998theideal}.
%Researchers showed a macaque monkey the image of a human face in
%varying view angles, while recording the response of 32 cells in the
%superior temporal to each view angle.  They then used a Bayesian
%decoder to build a disciminative model for the view angle from the
%data, and assessed its predictive accuracy for determining the view
%angle from the responses of the 32 recorded cells.  In nature, the
%brain of the macaque monkey infers the rotational angle of various
%objects using lower-level neural inputs; Oram et al. mimic the natural
%discriminatory behavior by training a model to achieve the same task
%from cell recoding data.  

While the precise mechanisms by which low-level sensory inputs are
aggregated into high-level concepts, such as rotational angle, are
still not well-understood, the feasibility of \emph{artificially}
discriminating the signal based on such data is scientifically
meaningful, even if the artificial classifier employed has little
resemblance to the actual dynamics driving cognition in the brain.  %%mammal brain? human brain?  
This is because the existence of
\emph{any} discriminative model which can infer the signal from the
data is sufficient to establish that \emph{information} about the
signal is contained within the sources of the data--e.g. the neurons
measured.

%% limitations of classification: e.g. can only provide lower bound?

%% Multi-class classification was first studied by SHannon
%% in random codes

%While multi-class classification is a rapidly developing field within
%machine learning, the problem of discriminating inputs according to
%discrete classes had been studied even before the advent of artificial
%intelligence: arguably, it was 

That is to say, the concept of \emph{information} indicates the
potentiality to achieve accurate discrimination.  This was first
formalized by the pioneers of information theory, most notably Claude
Shannon, who laid the foundation for the field in his seminal 1948
paper ``A mathematical theory of communication.''  Shannon defined a
quantity called the \emph{mutual information}, which measures the
information carried by a recieved signal $X$ about a transmitted
signal $Y$.  From Shannon's celebrated noisy channel theorem, we have
a formal relationship between mutual information and discrimination:
one that we will discuss in section \ref{sec:mi}.


Shannon, along
with other pioneers of information theory such as Robert M. Fano and
Norbert Weiner, recognized that the information content of a signal
depended on how many different messages it can plausibly convey.



Furthermore, it was Shannon who first recognized that when the signal
is corrupted by noise, the amount of information \emph{lost} depends
on how reliably the original message can be recovered from the noisy
input--in other words, how well the reciever can \emph{discriminate}
the original message on the basis of the noise-corrupted recieved
message.  %% maybe add more details, redundancy of English, etc.

Indeed, we see that key to the development of information theory is
the study of the \emph{noisy-channel decoding problem}, which is
illustrated in figure \ref{fig:mcc_vs_it}.  % Shannon's key result: decoding accuracy and mutual information

\tikzstyle{block} = [rectangle, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cloud} = [ellipse, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
    
\begin{figure}
\centering
\begin{tabular}{ccc}

Multi-class classification & & Information Theory\\

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (init1) {label $Y$};
    \node [cloud, below of=init1] (init2) {distribution $F_{X|Y}$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {classification rule $h(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{Y}$};
    % Draw edges
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

& & 

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (initA) {message $M$};
    \node [cloud, below of=initA] (initB) {encoder $g(M)$};
    \node [block, below of=initB] (init1) {encoded message $Y$};
    \node [cloud, below of=init1] (init2) {noisy channel $F_{X|Y}$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {decoder $d(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{M}$};
    % Draw edges
    \path [line] (initA) -- (initB);
    \path [line] (initB) -- (init1);
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

\end{tabular}
\caption{Comparing the discrimnation tasks in multi-class classification and information theory.}
\label{fig:mcc_vs_it}
\end{figure}

If we compare the discrimination tasks defined in multi-class
classification and information theory, we find much similarity, but
also a few important differences.  Figure \ref{fig:mcc_vs_it} displays
the schematic diagrams of the general multi-class classification
problem and the setup for the noisy channel considered by Shannon.
%% If we put the figure here, probably will need some more explanation

%% we should mention mutual information earlier
From figure \ref{fig:mcc_vs_it}, we can see major similarities between
the multi-class classification problem and the noisy decoding problem
studied in information theory: both involve inferring some latent
variable $Y$ on the basis of observed $X$.  We will go into great
detail about the similarities and differences in section
\ref{sec:rand_code_rand_class}.  However, despite these similarities,
it has historically been the case that the machine learning literature
and the information theory literature diverge in terms of the metric
used to characterize performance in the discrimination task.  In
multi-class learning, the performance is generally characterized by
the \emph{accuracy} of the classification--the probability that the
label is assigned correctly.  However, in information theory, a
quantity called the \emph{mutual information}, which was invented by
Claude Shannon, is used to characterize the quality of the noisy
channel with regards to achievable decoding performance. %% jargony phrase, due to the idea being difficult to express

\section{Supervised learning}

The generalization error of the learner as a statistic.

%\subsection{General characaterization of supervised learning}

\section{Mutual information}\ref{sec:intro_mi}

\subsection{Definition and history}

\subsection{Usage in neuroscience}


\section{Random codes and random classification}\label{sec:rand_code_rand_class}

In multi-class classification, we may assume without loss of generality that the data has been generated in the following manner:
\begin{enumerate}
\item First, a label $Y$ is drawn according to some distribution from the label set $\{y_1,\hdots, y_k\}$.
\item Secondly, the new observation $X$ is drawn according to the unknown conditional distribution $F_{X|Y}$.
\item Finally, an estimated label $\hat{Y} = h(X)$ is obtained
  according to a data-dependent classification rule, $h$.
  Typically, $h$ is determined by fitting a model to training data.
\end{enumerate}
In the particular application, the above description may not match the
\emph{causal relationship} between $X$ and $Y$: however, whether $X$
is drawn conditional on $Y$, or $Y$ is drawn conditional on $X$, or
that $(X, Y)$ are drawn from some joint distribution, makes no
difference from the theoretical standpoint, since only the statistical
(and not causal) properties of the joint distribution $(X, Y)$ are
relevant for determining the peformance of the classification rule $h(X)$.

Meanwhile, in the noisy channel model, we assume that the sender wants
to transmit message $M$, out of a finite set of possible messages
$\mathcal{M} = \{1,\hdots, m\}$.  The message must be encoded into a
signal $Y \in \mathcal{Y}$, which is sent through a stochastic channel
$F_{X|Y}$.  Given that a signal $y$ is sent through the channel, the
reciever observes a signal $X$ drawn from the distribution $F_{X|y}$.
The communications problem is to design an encoding function $g(M)$,
which is an injective map from messages $\{1,\hdots,m\}$ to signals in
$\mathcal{M}$, and a corresponding decoding function $d(X)$ which
infers the message $\{1,\hdots, m\}$ from the recieved signal $X$.

Therefore, we see that in both the multi-class classification problem
and the noisy channel model present examples of discrimination
problems where one must recover some latent variable $Y$ from
observations $X$, where $X$ is related to $Y$ through the family of
conditional distributions $F_{X|Y}$.  One difference is that while in
multi-class classification, $F_{X|Y}$ is unknown and has to be
inferred from data, in the noisy channel model, the stochastic
properties of the channel $F_{X|Y}$ are usually assumed to be known.
For example, for a binary channel where $Y$ consists of an $m$-length
binary string, a commonly studied channel $F_{X|Y}$ generates $X$ by
randomly flipping bits in $Y$ independently with some probability
$\epsilon$.  A second difference is that in the noisy channel model,
there is a choice in how to specify the encoding function $g(M)$,
which affects subsequent performance.  Finally, in the broader
research context, machine learning research has traditionally focused
on multi-class problems with relatively few classes, while information
theory tends to consider problems in asymptotic regimes where the
number of possible messages $m$ is taken to infinity. These
differences were sufficient to explain why little overlap exists in
the respective literatures between multi-class classification and the
noisy channel model.  

However, an interesting development in the machine learning community
has been the application of multi-class classification to problems
with increasingly large and complex label sets.  Consider the
following timeline of representative papers in the multi-class
classification literature:
\begin{itemize}
\item Fisher's Iris data set, \cite{fisher1936use}, $K = 3$ classes
\item Letter recognition, \cite{frey1991letter}, $K = 26$ classes
\item Michalski's soybean dataset, \cite{mickalstd1980learning}, $K = 15$ classes
\item The NIST handwritten digits data set, \cite{grother1995nist}, $K = 10$ classes
\item Phoneme recognition on the TIMIT datset, \cite{clarkson1999use}, $K = 39$ classes
\item Object categorization using Corel images, \cite{duygulu2002object} $K = 371$ classes
\item Object categorization for ImageNet dataset, \cite{deng2010does}, $K = 10,184$ classes
\item The 2nd Kaggle large-scale hierarchical text classification challenge (LSHTC), \cite{partalas2015lshtc}, $K = 325,056$
\end{itemize}
As we can see, in recent times we begin to see classification problems
with extremely large label sets.  In such large-scale classification
problems, or `extreme' classification problems, results for $K \to
\infty$ numbers of classes, like those found in information theory,
begin to look more applicable.

This work focuses on a particular intersection between multi-class
classification and information theory, which is the study of
\emph{random classification tasks.}  In numerous domains of applied
mathematics, it has been found that systems with large numbers of
components can be modelled using randomized versions of those same
systems, which are more tractable to mathematical analysis: for
example, studying the properties of networks by studying random graphs
in graph theory, or studying the performance of combinatorial
optimization algorithms for random problem instances.  Similarly, it
makes sense to posit randomized models of multi-class discrimination
problems.  Since information theorists were the first to study
discrimination problems with large number of classes, we find in the
information theory literature a long tradition of the study of
\emph{random code} models, dating back all the way to Shannon's
seminal 1948 paper.  This thesis is dedicated to the the study of the
analogue of random code models in the multi-class classification
setting: models of \emph{randomized classification.}

Furthermore, as we might expect from the close connection between
random codes and randomized classification, we are able to form links
between multi-class classification and information theory.  Therefore,
Chapters 4 and 5 leverage the link between randomized classification
and information theory in order to develop new estimators of mutual
information in high-dimensional datasets consisting of pairs ob
observations $(X, Y)$.  While Chapter 4 works in a setting where a
true, sparse relationship between $Y$ and $X$ is assumed to be known,
Chapter 5 makes a different assumption, which is that $X$ is
high-dimensional, and that its components are not too dependent.

In the following sections, we review the relevant background for
information theory, the applications of information theory to
neuroscience, and multi-class classification, then introduce the
connection between random code models in information theory and
randomized classification.


%% Make the connection between random codes and randomized classification,
%% point out why randomized classification is a good model for some contemporary problems


A random code model in information theory is one where given some
distribution $\nu$ (typically a uniform distribution) on the space of
transmittable signals $\mathcal{Y}$, we posit that the decoder $g(M)$
is randomly generated, by letting $g(1),\hdots, g(m)$ be identically
and independently assigned to random draws from $\nu$.  For example,
when $\mathcal{Y}$ is the space of $m$-length binary strings, the
encoder $g$ maps indices $1, \hdots, m$ to random $m$-length binary
strings.  The purpose of the random code model is usually to establish
a lower bound on achievable accuracy given some constraints on the
signal space.

Meanwhile, a randomized classification model is one where the label
set $\{y^{(1)},\hdots, y^{(k)}\}$ is not fixed, but randomly sampled.
One defines a label space $\mathcal{Y}$, a family of conditional
distributions $\{F_{X|y}\}_{y \in \mathcal{Y}}$, and a distribution
$\nu$ on $\mathcal{Y}$.  Then the randomized classification model is a
classification task obtained by drawing labels $\{Y^{(1)},\hdots,
Y^{(k)}\}$ iid from $\nu$, generating training data for those
particular labels by drawing observations $X_i^{(j)} \sim
F_{X|Y^{(j)}}$, and where the problem is to construct a classification
rule for assigning new observations $X$ which are drawn from the
mixture
\[
X \sim \frac{1}{k}\sum_{i=1}^k F_{X|Y^{(k)}}
\]
to one of the labels $\{Y^{(1)},\hdots, Y^{(k)}\}.$ As we will see
throughout the thesis, the randomized classification model can be
naturally applied to a large number of multi-class classification
applications, such as facial recognition, where the labels
(e.g. people) can be justifiably modelled as random draws from some
population.  And, even in the majority of classification problems
where the labels cannot be assumed to come from an iid sample,
randomized classification models can still be applied to provide
intuition for the original problem.

The link between random code models and randomized classification
models should now be apparent. Define $Y_i = g(i)$ in the random code
model, so that $Y_i$ are iid drawn from $\nu$.  Fixing a particular
realization of $Y_1,\hdots, Y_k$, the decoding problem is then
evidently a multi-class classification problem with labels
$\{Y_1,\hdots, Y_k\}$.  The only difference between the two models is
that the conditional distributions $F_{X|Y}$ are assumed to be known
in the random code model, and assumed unknown in the randomized
classification model.

But happens when we assume that $F_{X|Y}$ is known, in the multi-class
classification problem?  In fact, it is common to consider the case of
known $F_{X|Y}$ in the machine learning literature, because the
resulting accruacy gives an \emph{upper bound} on achievable
performance in the multi-class classification problem.  This is
because once $F_{X|Y}$ is known, it is possible to define the
\emph{optimal} classification rule, or \emph{Bayes} classification
rule $h_{Bayes}$.  For example, supposing that the performance
criterion is to minimize the zero-one risk $\Pr[\hat{Y} \neq Y]$, then
the Bayes rule is to assign $X$ to the label with the highest
posterior density,
\[
h_{Bayes}(X) = \text{argmax}_{y \in \{y_1, \hdots, y_k\}} f_{X|y}(x) \pi(y)
\]
where $\pi(y)$ is the prior probability of $Y = y$.  Therefore, it is
the same thing to study the Bayes accuracy in a randomized
classification model and the decoding accuracy of a randomized code in
a noisy channel.

Indeed, the analysis of the Bayes accuracy of a randomized
classification model forms the subject of Chapters 2 and 3.  Due to
the aforementioned equivalence, it can be said that a large body of
work dealing with the Bayes accuracy of randomized classification
problems exists in information theory: however, the majority of such
works deal with the limit as $k \to \infty$, and to our knowledge no
analysis has been done for the case of finite $k$, which is the
relevant scenario for multi-class classification.  There is good
reason for this lacuna in the information theory literature, which is
that since information theorists are concerned with understanding the
properties of \emph{optimal} coding schemes, it follows that
randomized coding schemes are only interesting insofar as that they
give good approximations to optimal coding schemes.  However, in the
classification setting, we may be interested in studying randomized
classification for its own sake, and not merely as a means to obtain
lower bounds or estimates for another problem.  Therefore, Chapter 2
presents novel results on the statistical properties of the Bayes
accuracy in the randomized classification model.  Meanwhile, Chapter 3
studies an interesting application of the randomized classification
problem, which is to analyze the dependence of the classification
accuracy on the size of the label set.  This analysis yields a novel
method for `performance extrapolation' in real-world classification
problems, meaning that our method can be used to estimate the
classification accuracy on a large multi-class classification problem
using data from only a subsample of the classes in the larger problem.






%% Link back to Shannon and information theory.  We develop further links between randomized classification and information theory.


%\section{Generalizations of information}

%\subsection{Information axioms}

%\subsection{Information coefficients based on supervised learning}

