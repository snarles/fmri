% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------


\section{Recognition tasks}

The study of human intelligence, and the study of artificial
intelligence, form two major intertwining areas of modern research.
The attempt to algorithmically mimic or exceed human perceptual and
cognitive capabilities not only advances the application of artificial
intelligence for industrial applications, but also sheds light on the
nature of biological intelligence, and the nature of intelligence in
general.  One of the key capabilities of an intelligent system,
natural or artificial, is the ability to recognize objects, agents,
and signs in the environment based on input data.  Human brains have a
remarkable ability to recognize objects, faces, spoken syllables and
words, and written symbols or words, and this recognition ability is
essential for everyday life.  While researchers in artificial
intelligence have attempted to meet human benchmarks for these
classical recognition tasks for the last X decades, only very recent
advances in machine learning, such as deep neural networks, have
allowed algorithmic recognition algorithms to approach or exceed human
performance [CITE].

Within the statistics and machine learning literature, the usual
formalism for studying a recognition task is to pose it as a
\emph{multi-class classification} problem.  One delineates a finite
set of distinct entities which are to be recognized and distinguished,
which is the \emph{label set} $\mathcal{Y}$.  The input data is
assumed to take the form of a finite-dimensional real \emph{feature
  vector} $X \in \mathbb{R}^p$.  Each input instance is associated
with exactly one true label $Y \in \mathcal{Y}$.  The solution to the
classification problem takes the form of an algorithmically
implemented \emph{classification rule} $h$ that maps vectors $X$ to
predicted labels $\hat{Y} \in \mathcal{Y}$.  The classification rule
can be constructed in a data-dependent way: that is, one collects a
number of labelled \emph{training observations} $(X_1, Y_1)$ which is
used to inform the construction of the classification rule $h$.  The
success of the classification rule $h$ is measured by the \emph{expected loss} or \emph{risk},
which in the case of zero-one loss takes the form
\[
\text{Risk}_{0-1}(h) = \Pr[h(X) \neq Y],
\]
where the probability is defined with reference to the unknown
population joint distribution of $(X, Y)$.  A common approach for
constructing a classifier $h$ is \emph{empirical risk minimization} [CITE],
which constructs $h$ by minimizing the objective function
\[
\text{EmpiricalRisk}_{h \in \mathcal{H}}(h) = \frac{1}{n} \sum_{i=1}^n I(h(X) \neq Y)
\]
where $\mathcal{H}$ is some pre-specified function class.

However, a limitation of the usual multi-class classification
framework for studying recognition problems is the assumption that the
label set $\mathcal{Y}$ is finite and known in advance.  When
considering human recognition capabilities, it is clear that this is
not the case.  Our ability to recognize faces is not limited to some
pre-defined, fixed set of faces; same with our ability to recognize
objects in the environment.  Humans learn to recognize novel faces and
objects on a daily basis.  And, if artificial intelligence is to fully
match the human capability for recognition, it must also possess the
ability to add new categories of entities to its label set over time;
however, at present, there currently exists a void in the machine
learning literature on the subject of the online learning of new
classes in the data [CITE].

The central theme of this thesis is the study of \emph{randomized
  classification}, which can be motivated as an extension of the
classical multi-class classification framework to accommodate the
possibility of growing or infinite label sets $\mathcal{Y}$. The basic
approach taken is to assume an infinite or even continuous label space
$\mathcal{Y}$, and then to study the problem of classification on
finite label sets $S$ which are randomly sampled from $\mathcal{Y}.$
This, therefore defines a \emph{randomized classification} problem
where the label set is finite but may vary from instance to instance.
One can then proceed to answer questions about the variability of the
performance due to randomness in the labels, or how performance
changes depending on the size of the random label set.

An additional set of applications of the randomized classification
framework lies in its connection to information theory.  Randomized
classification is the natural analogue of the \emph{random code}
models first studied by Claude Shannon.  Furthermore, it becomes
possible to prove extensions of Fano's inequality to the case of
continuous $X$ and $Y$ by means of randomized classification.
Therefore, randomized classification can be used as a means of
inferring mutual information.

The rest of the thesis is organized as follows.  The remaining
sections in this chapter deal with background material on supervised
learning and information theory, as well as the application of both to
neuroscience, which forms a major motivation for the current work.
Chapter 2 introduces the concept of randomized classification, and
also establishes some variability bounds which will be used later in
the development of inference procedures.  Chapter 3 studies the
dependence of classification accuracy on the label set size in
randomized classification, and a practical method for predicting the
accuracy-versus-label set size curve from real data.  Chapter 4 and 5
deal with the applications of randomized classification to the
estimation of mutual information in continuous data: chapter 4 derives
a lower confidence bound for mutual information under very weak
assumptions, while chapter 5 works within an asymptotic
high-dimensional framework which leads to a more powerful but less
robust estimator estimate of mutual information.

\section{Information and Discrimination}

In studying the problem of recognition, we make use of two closely
related frameworks: firstly, the multi-class classification framework
from the statistics and machine learning literature, and secondly, the
concepts of information theory.  From a broader perspective, this is
hardly unusual, since concepts such as entropy, divergence, and mutual
information are commonly applied in theoretical statistics and machine
learning.  Furthermore, information theory, theoretical statistics,
and machine learning are based on the same foundation:
measure-theoretic probability theory; one could even say that all
three disciplines are subfields of applied probability.  However,
while the three sub-fields may appear very similar from a mathematical
perspective, some differences arise if we examine the kinds of
intuitions and assumptions that are characteristic of the literature
in each area.

A common problem to all three subfields is the inference of some
unobserved quantity on the basis of observed quantities.  In classical
statistics, the problem is to infer an unknown parameter; in
supervised learning, the problem is to predict an unobserved label or
response $Y$; in information theory, the problem is to decode a noisy
message.  Next, the metric for quantifying achievable performance
differs between the three disciplines.  In classical statistics, one
is concerned with the variance of the estimated parameter, or
equivalently, the Fisher information.  In machine learning, one seeks
to minimize (in expectation) a \emph{loss} function which measures the
discrepancy between the prediction and the truth.  In information
theory, one can measure the quality of the noisy channel (and
therefore, the resulting achievable accuracy) through the \emph{mutual
  information} $I(X; Y)$ between the sender's encoded message $X$ and
the reciever's recieved message $Y$.  If we specialize within machine
learning to the study of classification, then we are concerned with
accurate \emph{discrimination} of the input $X$ according to labels
$Y$.

%% have to reorder the next para, 
The concepts of \emph{information} and \emph{discrimination} are quite
distinct from an intuitive standpoint; however, they are linked at a
fundamental level.  This link can be seen throughout statistics and
machine learning, and in the way we think about statistical problems.
A statistical hypothesis test is \emph{informative} because it
provides evidence that the data behaves according to a certain
hypothesis rather than another: it allows us to \emph{discriminate}
between two potential possibilities.  More generally, a true message
contains \emph{information} if it allows the reciever to understand
that the world is in a certain state and not another: it conveys the
fact that out of two possible sets of world states, that the reciever
is in one state rather than another.  The formalism of
measure-theoretic probability theory provides yet another example of
the conceptual link between information and
discrimination\footnote{Supposing $\Omega$ is a probability space
  defined with respect to a $\sigma$-algebra $\mathcal{F}$, we can
  represent our state of knowledge with a filtration (or
  sub-$\sigma$-algebra) $\mathcal{F}' \subseteq \mathcal{F}$.
  Complete knowledge (zero uncertainty) is represented by the full
  $\sigma$-algebra: that is, $\mathcal{F}' = \mathcal{F}$.  Partial
  knowledge is represented by a coarser filtration, $\mathcal{F}
  \subset \mathcal{F}'$.  The filtration, of course, indicates that
  our knowledge is sufficient to \emph{discriminate} the outcome space
  $\Omega$ into a number of finitely or infinitely many categories.
  The more information we have, (or, the closer we come to complete
  knowledge of the outcome), the more finely we can discriminate the
  realized outcomes given by $\omega \in \Omega$.}.

%% the purpose of this paragraph is to make the link between discrimination and information
Either natural or artificially intelligence recognition systems
must rely on input data that is \emph{informative} of the optimal
response if they are to achieve reasonable discriminative accuracy.
In natural environments, mammals rely on a combination of visual,
auditory, and tactile cues to recognize potential threats in the
environment.  Mammalian brains integrate all of this sensory
information in order to make more rapid and reliable decisions.
Generally, increased diversity and quality of the available sources of
information will lead to more accurate recognition.

This link between the information content of the sensory input and the
achievable discrimination accuracy was first formalized by Claude
Shannon via the concept of \emph{mutual information.}  The mutual
information $I(X; Y)$ quantifies the information content that an input
$X$ holds abut a target of interest, $Y$.  For instance, in the case
of facial identification, the discrimination target $Y$ is a label
corresponding to the identity of the person, and $X$ is an image of
the individual's face.  An image corrupted by noise holds less
information, and correspondingly leads to lower classification
accuracies.

The discrmination problem that Shannon studied--the
\emph{noisy-channel decoding problem}, is extremely similar to the
multi-class classification problem, but also features some important
differences.  A side-by side comparison between the schematics of
multi-class classification and the noisy channel problem is displayed
in Figure \ref{fig:mcc_vs_it}.

\tikzstyle{block} = [rectangle, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cloud} = [ellipse, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
    
\begin{figure}
\centering
\begin{tabular}{ccc}

Multi-class classification & & Information Theory\\

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (init1) {label $Y$};
    \node [cloud, below of=init1] (init2) {distribution $F_Y$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {classification rule $h(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{Y}$};
    % Draw edges
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

& & 

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (initA) {message $M$};
    \node [cloud, below of=initA] (initB) {encoder $g(M)$};
    \node [block, below of=initB] (init1) {encoded message $Y$};
    \node [cloud, below of=init1] (init2) {noisy channel $F_Y$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {decoder $d(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{M}$};
    % Draw edges
    \path [line] (initA) -- (initB);
    \path [line] (initB) -- (init1);
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

\end{tabular}
\caption{Comparing the discrimination tasks in multi-class classification and information theory.}
\label{fig:mcc_vs_it}
\end{figure}

We will now briefly review the relevant background for supervised learning and
information theory, to give the context for each side of figure
\ref{fig:mcc_vs_it}.  Afterwards, we will compare and contrast the
supervised learning and information theory, and note what kind of
cross-talk exists between the two related fields, and what new
developments could still arise by way of a dialogue between supervised
learning and information theory.  One such new development is the
\emph{randomized classification} model, since it is a very close
analogue of the \emph{random code} model studied in information
theory.

\subsection{Supervised learning}

Multi-class classification is a special case of a \emph{prediction
  task}, which refers to a problem in which one is asked to predict a
variable $Y$ conditional on an observation $X$.  In classification,
$Y$ is a categorical vector: in \emph{binary} classification $Y$ can
take one of two possible values, while in multi-class classification,
$Y$ can take more than two values.  In \emph{regression}, another
supervised learning task, $Y$ takes a numeric value.  In
\emph{multiple-response regression}, or \emph{multivariate
  regression}, $Y$ is a numeric vector.  In \emph{multi-label
  classification}, $Y$ is a vector of categorical values.

However, both
classification and regression fit into the same general framework,
which consists of:

\begin{itemize}
\item A predictor space $\mathcal{X}$ defining the possible values the predictor $X$ can take;
\item A response space $\mathcal{Y}$ defining the possible values the response $Y$ can take;
\item An \emph{unknown} population joint distribution $G$ for the pair $(X, Y)$;
\item A \emph{loss} function defining the penalty for incorrect
  predictions, $L: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$.  If
  $Y$ is the response, and $\hat{Y} = h(X)$ is the prediction, then
  the loss for making the prediction $\hat{Y}$ when the truth is $Y$
  is given by $L(Y; \hat{Y})$.
\end{itemize}

Therefore, some of the aforementioned special cases of prediction tasks include:

\begin{itemize}
\item In classification, the response space is finite and discrete.  The most common loss function is zero-one loss,
\[
L(y; \hat{y}) = I(y \neq \hat{y}).
\]
\item In regression, the response space is $\mathbb{R}$.  The most common loss function is squared loss:
\[
L(y; \hat{y}) = (y - \hat{y})^2.
\]
\item In multiple-response regression, the response space is $\mathbb{R}^p$.  A natural loss function is squared Euclidean distance,
\[
L(y; \hat{y}) = ||y - \hat{y}||^2.
\]
\end{itemize}

A \emph{prediction rule} is a function $h: \mathcal{X} \to
\mathcal{Y}$ for predicting $Y$ as a function of $X$.  The \emph{risk}
of the prediction rule is the expected loss under the joint
distribution $G$,
\[
\text{Risk}(h) = \mathbb{E}[L(Y, h(X))]
\]

Prediction rules can be found through a variety of means.  In some
domains, experts manually construct the prediction rules using their
domain knowledge.  However, the field of \emph{supervised learning}
aims to algorithmically construct, or `learn' a good prediction rule
from data.  In supervised learning, we assume that we have access to a
\emph{training set} of observations $\{(X_i,Y_i)\}_{i=1}^{n_1}$, plus
a \emph{test set} of observations $\{(X_i,Y_i\}_{i=n_1 + 1}^{n_1 +
  n_2}$; usually, we assume that the pairs in both the training and
test set have been sampled i.i.d. from the distribution $G$.  We will
also write $\bX$ for the matrix of training observations, with each
$X_i$ stacked in rows, and $\bY$ for the vector of training
responses. The training set is used to construct the prediction rule
$h$.  The test set is then used to estimate the risk of the
constructed rule (which is also called the \emph{generalization
  error}.)

A \emph{learning algorithm} $\Lambda$ is a procedure for constructing the
prediction rule $h$ given training data $\{(X_i,Y_i)\}_{i=1}^{n_1}$ as
an input.  Formally, we write
\[
h = \Lambda(\{(X_i,Y_i)\}_{i=1}^{n_1}),
\]
indicating that $h$ is the output of the function $\Lambda$ evaluated
on the input $\{(X_i,Y_i)\}_{i=1}^{n_1}$.  However, how this works in
practice can very considerably; we illustrate just a few of the most
common types of learning algorithms:

\begin{itemize}
\item \emph{Parametric generative models.}  Define a parametric family
  $F_\theta$ of joint distributions $(X, Y)$.  For instance, in linear
  regression, a commonly studied family is the multivariate normal linear model,
  where
\[
(X, Y) \sim N((1,0,\hdots,0, \beta_0), \begin{pmatrix}\Sigma_X & \Sigma_X \beta \\
\beta^T \Sigma_X & \beta^T \Sigma_X \beta + \Sigma_\epsilon\end{pmatrix},
\]
or equivalently,
\[
X \sim N((1,0,\hdots,0), \Sigma_X)
\]
\[
Y|X \sim N(X \beta, \Sigma_\epsilon).
\]
Then, fit the parametric model to estimate the parameter
$\hat{\theta}$.  One may use a variety of methods to estimate
$\theta$: maximimum likelihood, penalized maximum likelihood, or
Bayesian estimation.  Note also that in many cases, such as
regression, not all parameters of the model need to be estimated for
prediction purposes.  For instance, in the linear regression model
given above, only $\beta$ needs to be estimated, and not $\Sigma_X$ or
$\Sigma_Y$.  One then constructs the prediction rule $h$ depending on
the estimated parameter $\hat{\theta}$, in a way so that the risk is
controlled.  For instance, in linear regression, one takes $h =
\hat{\beta}^T X.$
\item \emph{Empirical risk minimization.} As mentioned before, define
  a function class $\mathcal{H}$.  We wish to search for an element $h
  \in \mathcal{H}$ which minimizes the empirical risk on the training
  set,
\[
h = \text{argmin}_{h \in \mathcal{H}} \frac{1}{n_1} \sum_{i=1}^{n_1} \tilde{L}(Y_i, h(X_i))
\]
Here, $\tilde{L}$ could be taken to be equal to the original loss
function $L$, or could be taken to be a different function, such as a
smoothed approximation of $L$.  The advantage of using a smoothed
approximation $\tilde{L}$ is that the empirical risk can be made
differentiable (whereas the original loss $L$ might be
nondifferentiable) and hence the optimization made much more tractable
from a numerical standpoint.  This is often the case in binary
classification, where $L$ is zero-one loss, but $\tilde{L}$ is the logistic loss
\[
\tilde{L}(y; p) = y \log p + (1-y) \log (1-p).
\]
%% might have to explain why the prediction space is probabilities rather than responses, as well
\end{itemize}
Further complicating the picture is the fact that often the learning
algorithm requires specification of various \emph{hyperparameters}.
For instance, lasso regression is a penalized generative model which
finds $\beta$ minimizing the objective function
\[
\beta = \text{argmin}_\beta \frac{1}{2}||\bY - \bX \beta||^2 + \lambda ||\beta||_1.
\]
%% No explanation of L1 norm notation
Here, the L1-penalty constant $\lambda$ needs to be specified by the
user.  In practice, one can either use prior knowledge or
theoretically-justified rules to select $\lambda$; or, more commonly,
one uses various procedures to automatically tune $\lambda$ based on
the training data.  The most common procedure for automatically
selecting $\lambda$ is cross-validation, with either the ``min'' or
``one standard deviation'' rule.  We do not go into details here, and
refer the interested reader to \cite{Hastie2009a}, section 7.10.

\subsection{Information Theory}\label{sec:intro_mi}

Information theory is motivated by the question of how to design a
message-transmission system, which includes two users--a sender and a
reciever, a \emph{channel} that the sender can use in order to
communicate to the reciever, and a protocol that specifies:
\begin{itemize}
\item[a.] how the sender can \emph{encode} the message in order to
  transmit it over the channel.  Morse code is one example of an
  encoding scheme: a means of translating plaintext into signals than
  can be transmitted over a wire (dots and dashes); and
\item[b.] how the reciever can \emph{decode} the signals recieved from
  the channel output in order to (probabilistically) recover the
  original message.
\end{itemize}

Beginning with Shannon (1948), one constrains the properties of the
channel, and studies properties of encoding/decoding protocols to be
used with the channel.  Two types of channels are studied:
\emph{noiseless} channels, which transmit symbols from a fixed
alphabet (e.g. ``dots'' and ``dashes'') from the sender to reciever,
and \emph{noisy} channels, which transmit symbols from a discrete
symbol space $\mathcal{Y}$ to a possibly different symbol space
$\mathcal{X}$ in a stochastic fashion.  That is, for each input symbol
$y \in \mathcal{Y}$, the transmitted symbol output $X$ is drawn from a
distribution $F_y$ that depends on $y$\footnote{Note that here we
  have flipped the usual convention in information theory, in which
  the letter $X$ commonly denotes the input and $Y$ denotes the
  output.  However, we flip the notation in order to match the
  convention in multi-class classification.}.  It is the study of
noisy channels that is of primary interest to us.

We allow the sender to transmit a sequence of $L$ input symbols over
the channel, $\vec{Y} = (Y_1,Y_2,\hdots, Y_L)$. The reciever will observe the
output $\vec{X} = (X_1,X_2,\hdots, X_L)$, where each $X_i$ is drawn from
$F_{Y_i}$ independently of the previous $X_1,\hdots, X_{i-1}$.

Now, let us assume that the sender wants to transmit message $M$, out
of a finite set of possible messages $\mathcal{M} = \{1,\hdots, m\}$.
The message must be encoded into a signal $\vec{Y} \in \mathcal{Y}^L$,
which is sent through a stochastic channel $F$.  Thus, the encoding
scheme is given by a \emph{codebook} or \emph{encoding function} $g:
\{1,\hdots, m\} \to \mathcal{Y}^L$ which specifies how each message
$i$ is mapped to an input sequence, $g(i) \in \mathcal{Y}^L$.
Conversely, the decoding scheme is given by a decoding function
$d(\vec{X})$ which infers the message $\{1,\hdots, m\}$ from the
recieved signal $\vec{X}$.  Theoretically
speaking\footnote{Practically speaking, the maximum likelihood (ML)
  decoder may be intractable to implement, and computational
  considerations mean that development of practical decoders remains a
  challenging problem.}, a reasonable decoding scheme is the
\emph{maximum likelihood decoder},
\[
d(\vec{x}) = \max_{i \in \{1,\hdots, m\}} \Pr[\vec{X} = \vec{x}| \vec{Y} = g(i)] = \max_{i \in \{1,\hdots, m\}} \prod_{j=1}^L F_{(g(i))_j}(X_j).
\]

The design of encoding/decoding schemes with minimal error (or other
desirable properties) over a fixed channel is a highly nontrivial
problem, which remains a core problem in the information theory
literature.  However, Shannon's original proof of the noisy channel
capacity theorem demonstrates a surprising fact, which is that for
large message spaces $\mathcal{M}$, close-to-optimal information
transmission can be achieved by using a \emph{randomized} codebook.
In order to discuss the noisy channel capacity theorem and the
construction of the randomized codebook, we first need to define
the concept of \emph{mutual information}.

\subsubsection{Mutual information}

If $\bX$ and $\bY$ have joint density $p(\bx, \by)$ with respect to
the product measure $\mu_x \times \mu_y$, then the mutual information
is defined as
\[
\text{I}(\bX;\bY) = \int p(\bx, \by) \log \frac{p(\bx, \by)}{p(\bx)p(\by)}d\mu_x(\bx) d\mu_y(\by).
\]
where $p(\bx)$ and $p(\by)$ are the marginal densities with respect to
$\mu_x$ and $\mu_y$\footnote{Note that the mutual information is
  invariant with respect to change-of-measure.}.  When the reference
measure $\mu_x \times \mu_y$ is unambiguous, note that $\text{I}$ is
simply a functional of the joint density $p(\bx, \by)$.  Therefore, we
can also use the \emph{functional} notation
\[
\text{I}[p(\bx, \by)] = \int p(\bx, \by) \log \frac{p(\bx, \by)}{p(\bx)p(\by)}d\mu_x(\bx) d\mu_y(\by).
\]


The mutual information is a measure of dependence between random
vectors $\bX$ and $\bY$, and satisfies a number of important
properties.
\begin{enumerate}
\item The channel input $\bX$ and output $\bY$ can be random vectors of arbitrary dimension, and the mutual information remains a scalar functional of the joint distribution $P$ of $(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\text{I}(\bX; \bY) = 0$; otherwise, $\text{I}(\bX; \bY) > 0$.
\item The data-processing inequality: for any vector-valued function $\vec{f}$ of the output space,
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
\item Symmetry: $\text{I}(\bX; \bY) = \text{I}(\bY; \bX)$.
\item Independent additivity: if $(\bX_1,\bY_1)$ is independent of $(\bX_2, \bY_2)$, then
\[
\text{I}((\bX_1,\bY_1); (\bX_2, \bY_2)) = \text{I}(\bX_1; \bY_1) + \text{I}(\bX_2; \bY_2).
\]
\end{enumerate}
Three additional consequences result from the data-processing inequality:
\begin{itemize}
\item \emph{Stochastic data-processing inequality}  If $\vec{f}$ is a stochastic function independent of both $\bX$ and $\bY$, then
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
This can be shown as follows: any stochastic function $\vec{f}(\bY)$
can be expressed as a deterministic function $\vec{g}(\bY, W)$, where
$W$ is a random variable independent of $\bX$ and $\bY$.
By independent additivity,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)).
\]
Then, by the data-processing inequality,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)) \geq \text{I}(\bX; \vec{g}(\bY, W)) = \text{I}(\bX; \vec{f}(\bY)).
\]
\item \emph{Invariance under bijections.} If $\vec{f}$ has an inverse $\vec{f}^{-1}$, then 
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY) = \text{I}(\bX; \vec{f}^{-1}(\vec{f}(\bY))) \leq \text{I}(\bX; \vec{f}(\bY)),
\]
therefore, $\text{I}(\bX; \vec{f}(\bY)) = \text{I}(\bX; \bY)$.
\item \emph{Monotonicity with respect to inclusion of outputs.}  Suppose we have an output ensemble $(\bY_1,\bY_2)$.  Then the individual component $\bY_1$ can be obtained as a projection of the ensemble.  By the data-processing inequality, we therefore have
\[
\text{I}(\bX; \bY_1) \leq \text{I}(\bX; (\bY_1, \bY_2)).
\]
Intuitively, if we observe both $\bY_1$ and $\bY_2$, this can
only \emph{increase} the information we have about $\bX$ compared to
the case where we only observe $\bY_1$ by itself.
\end{itemize}
And it is the property of \emph{invariance under bijections},
inclusive of non-linear bijections, which qualifies mutual information
as a \emph{non-linear measure of dependence.}  Linear correlations are
invariant under scaling and translation, but not invariant
to \emph{nonlinear} bijections.

%As for
%the completeness of the five listed properties: as we know, Shannon's
%mutual information (up to arbitrary scaling factor) is the only
%functional proposed in the literature which satisfies all five
%properties.
Besides the formal definition, there are a number of well-known alternative
characterizations of mutual information in terms of other
information-theoretic quantities: the \emph{entropy} $\text{H}$:
\[
\text{H}_\mu(\bX) = -\int p(\bX) \log p(\bX) d\mu(\bX),
\]
and the \emph{conditional entropy}:
\[
\text{H}_\mu(\bX|\bY) = -\int p(\bY) d\mu_y(\bY) \int p(\bX|\bY) \log p(\bX|\bY) d\mu_x(\bX).
\]
Some care needs to be taken with entropy and conditional entropy since
they are not invariant with respect to change-of-measure: hence the
use of the subscript in the notation $\text{H}_\mu$.  In particular,
there is a difference between \emph{discrete entropy} (when $\mu$ is
the counting measure) and \emph{differential entropy} (when $\mu$ is
$p$-dimensional Lesbegue measure.)  Intutively, entropy measures an
observer's uncertainty of the random variable $\bX$, supposing the
observer has no prior information other than the distribution of
$\bX$. Conditional entropy measures the \emph{expected uncertainty} of
$\bX$ supposing the observer observes $\bY$.

The following identities characterize mutual information in terms of entropy:
\[
\text{I}(\bX; \bY) = \text{H}_{\mu_x \times \mu_y}((\bX, \bY)) - \text{H}_{\mu_x}(\bX) - \text{H}_{\mu_y}(\bY).
\]
\begin{equation}\label{eq:ce_ident}
\text{I}(\bX; \bY) = \text{H}_\mu(\bY) - \text{H}_\mu(\bY|\bX).
\end{equation}
The second identity \eqref{eq:ce_ident} is noteworthy
as being practically important for estimation of mutual information.
Since the entropies in question only depend on the marginal and
conditional distributions of $\bY$, the problem of estimating
$\text{I}(\bX; \bY)$ can be reduced from a $\dim(\bX)
+ \dim(\bY)$-dimensional nonparametric estimation problem to a
$\dim(\bY)$-dimensional problem: hence this identity is a basis of
several methods of estimation used in neuroscience, such as Gastpar
(2014).

However, by symmetry, we also have the flipped identity
\begin{equation}\label{eq:ce_ident2}
\text{I}(\bX; \bY) = \text{H}_\mu(\bX) - \text{H}_\mu(\bX|\bY).
\end{equation}
In neuroscience studies, where $\bX$ is the controlled stimulus, and
$\bY$ is the neural activity, the two mirror pairs \eqref{eq:ce_ident}
and \eqref{eq:ce_ident2} have different interpretations.  Rather than
providing a basis for practical estimation, \eqref{eq:ce_ident2}
provides an \emph{interpretation} of the mutual information.  Loosely
speaking, $\text{H}_\mu(\bX)$ is the uncertainty of $\bX$ before
having observed $\bY$, and $\text{H}_\mu(\bX|\bY)$ is the uncertainty
of $\bX$ after having observed $\bY$, hence $\text{H}_\mu(\bX)
- \text{H}_\mu(\bX|\bY)$ is how much the observation of $\bY$
has \emph{reduced} the uncertainty of $\bX$.  Stated in words,
\[
\text{I}(\bX; \bY) = \text{average reduction of uncertainty about $\bX$ upon observing $\bY$}.
\]

\subsubsection{Channel capacity and randomized codebooks}

As a general measure of dependence, mutual information has enjoyed
numerous and diverse applications outside of information theory.
However, its original role in Shannon's paper was to define the
quantity known as \emph{channel capacity} of a noisy channel.

Let us first note that the channel capacity of a noiseless channel
with $S$ symbols is simply $\log S$.  The justification is that if we
allow $L$ symbols to be sent, then $S^L$ possible messages can be
encoded.  Therefore, the channel capacity of a noiseless channel can
be understood as the logarithm of the number of possible messages to
be transmitted divided by the length of the sequence, with is $\log
S$.


However, how can the channel capacity $C$ be defined for a noisy channel?
Suppose we have a noisy channel with transmission probabilities given by $\{F_y\}_{y \in \mathcal{Y}}$.
Shannon came with with the following definition:
\[
C = \max_{p\in \mathcal{P}} I[p(x,y)].
\]
where $\mathcal{P}$ is the set of joint distributions that can be
realized in the channel.  We will define $\mathcal{P}$ in a moment,
but what is the key property of $C = \max_p I[p(x,y)]$ that makes it
the correct analogue of the noiseless channel capacity?  It turns out
that the optimal ratio of logarithm of the number of messages $M$ that
can be transmitted with \emph{vanishing} error, divided by the length
$L$ of the encoding scheme, approaches $C$ as $M \to \infty$--which is
the crux of the noisy channel capacity theorem: that is,
\[
\lim_{M \to \infty} \frac{\log M}{L} = C.
\]

Now let us define the set $\mathcal{P}$.  Let $p_y$ be a probability
distribution over input symbols $\mathcal{Y}$.  If we transmit input $Y$
randomly according to $Y \sim p_y$, the induced joint distribution $p(Y, X)$
is given by
\[
p(y, x) = p_y(y) F_y(x).
\]
The set $\mathcal{P}$ is simply the collection of all such distributions: that is,
\[
\mathcal{P} = \{p(y, x) \text{ such that } p(x|y) = F_y(x)\text{ for all }(x, y) \in \mathcal{X} \times \mathcal{Y}\}.
\]

To show that $C = \max_p I[p(y, x)]$ is the noisy channel capacity,
then, (i) we need to show that there exists a sequence of codes with
length $L = \frac{\log M}{C}$ which achieves vanishing decoding error
as $M \to \infty$, and (ii) we need to show that any code with a
shorter length has non-vanishing decoding error.  We omit the proof of
(ii), which can be found in any textbook on information theory, such
as \cite{Cover2006}.  However, it is of interest to illustrate (i),
because randomized codebooks provide such a construction.


For a given channel $\{F_y\}$, let $p^* \in \mathcal{P}$ be the
distribution which maximizes $I[p(y, x)]$.  Let $p^*_y$ be the
marginal distribution of $Y$, and let $L = \lceil \frac{\log M}{C}
\rceil$.  Now we can define the random code.  Let $g(i) =
(Y_1^{(i)},\hdots, Y_L^{(i)})$ where $Y_j^{(i)}$ are iid draws from
$p^*_y$ for $i = 1,\hdots, M$ and $j = 1,\hdots, L$.  We will show
that the average decoding error (taken over the distribution of random
codebooks) goes to zero as $M \to \infty$.






\subsection{Comparisons}

Therefore, we see that in both the multi-class classification problem
and the noisy channel model present examples of discrimination
problems where one must recover some latent variable $Y$ from
observations $X$, where $X$ is related to $Y$ through the family of
conditional distributions $F_{X|Y}$.  One difference is that while in
multi-class classification, $F_{X|Y}$ is unknown and has to be
inferred from data, in the noisy channel model, the stochastic
properties of the channel $F_{X|Y}$ are usually assumed to be known.
For example, for a binary channel where $Y$ consists of an $m$-length
binary string, a commonly studied channel $F_{X|Y}$ generates $X$ by
randomly flipping bits in $Y$ independently with some probability
$\epsilon$.  A second difference is that in the noisy channel model,
there is a choice in how to specify the encoding function $g(M)$,
which affects subsequent performance.  Finally, in the broader
research context, machine learning research has traditionally focused
on multi-class problems with relatively few classes, while information
theory tends to consider problems in asymptotic regimes where the
number of possible messages $m$ is taken to infinity. These
differences were sufficient to explain why little overlap exists in
the respective literatures between multi-class classification and the
noisy channel model.  

%% unusual to put mention this here

However, an interesting development in the machine learning community
has been the application of multi-class classification to problems
with increasingly large and complex label sets.  Consider the
following timeline of representative papers in the multi-class
classification literature:
\begin{itemize}
\item Fisher's Iris data set, \cite{fisher1936use}, $K = 3$ classes
\item Letter recognition, \cite{frey1991letter}, $K = 26$ classes
\item Michalski's soybean dataset, \cite{mickalstd1980learning}, $K = 15$ classes
\item The NIST handwritten digits data set, \cite{grother1995nist}, $K = 10$ classes
\item Phoneme recognition on the TIMIT datset, \cite{clarkson1999use}, $K = 39$ classes
\item Object categorization using Corel images, \cite{duygulu2002object} $K = 371$ classes
\item Object categorization for ImageNet dataset, \cite{deng2010does}, $K = 10,184$ classes
\item The 2nd Kaggle large-scale hierarchical text classification challenge (LSHTC), \cite{partalas2015lshtc}, $K = 325,056$
\end{itemize}
As we can see, in recent times we begin to see classification problems
with extremely large label sets.  In such large-scale classification
problems, or `extreme' classification problems, results for $K \to
\infty$ numbers of classes, like those found in information theory,
begin to look more applicable.

This work focuses on a particular intersection between multi-class
classification and information theory, which is the study of
\emph{random classification tasks.}  In numerous domains of applied
mathematics, it has been found that systems with large numbers of
components can be modelled using randomized versions of those same
systems, which are more tractable to mathematical analysis: for
example, studying the properties of networks by studying random graphs
in graph theory, or studying the performance of combinatorial
optimization algorithms for random problem instances.  Similarly, it
makes sense to posit randomized models of multi-class discrimination
problems.  Since information theorists were the first to study
discrimination problems with large number of classes, we find in the
information theory literature a long tradition of the study of
\emph{random code} models, dating back all the way to Shannon's
seminal 1948 paper.  This thesis is dedicated to the the study of the
analogue of random code models in the multi-class classification
setting: models of \emph{randomized classification.}


%% Make the connection between random codes and randomized classification,
%% point out why randomized classification is a good model for some contemporary problems


%% Link back to Shannon and information theory.  We develop further links between randomized classification and information theory.

\section{Applications of supervised learning and information theory in neuroscience}

The study of naturally intelligent systems, and the study and design
of artificially intelligent systems, form two major intertwining
domains of modern research.  Here, we take \emph{intelligence} to
refer to the property of a system by which it can modulate its
reaction to external inputs in a purposeful manner.  Therefore, by
this definition, an amoeba which seeks food and avoids toxins is
intelligent, as is a gene-regulation network, or a mammalian brain.
Similarly, artificially intelligent algorithms which implement
automated decision-making rules in response to external inputs also
satisfy this definition of intelligence.

A primitive organism may classify percieved objects in its environment
as either beneficial (potential food and resources) or harmful (toxins
and predators), and intelligently respond by means of pursuing the
former and avoiding the latter.  %% note that this example is redundant
Complex organisms, like humans, not
only discriminate in order to make immediate decisions, but also
categorize objects in the world in order to perform intermediate
calculations and carry out contextual reasoning.
%% example?


%% Introduce examples of classification and multi-class classification

Similarly, \emph{artificially intelligent} algorithms and agents will
also discriminate input data into categories.  Very often, these types
of classification and recognition tasks mimic and automate
discrimination tasks that humans already perform, such as:
\begin{itemize}
\item Optical character recognition: recognizing characters from handwritten glyphs.
\item Facial recognition: identifying individuals from images of their faces.
\item Object recognition: identifying objects in photographs and labelling them with the appropriate keyword.
\end{itemize}
However, classification can also be performed on novel types of tasks
where no human substitute exists, such as diagnosis of cancer
phenotype based on thousands of gene expression levels.

While a major use of artificially intelligent classifiers is to
automate routine tasks or to categorize complex data, it is also
scientifically interesting to compare the performance of naturally
intelligent agents (human or animal subjects) versus artificially
intelligent classifiers on discrimination tasks.  One reason is that human
benchmarks on complex classification tasks, such as object
recognition, are often non-trivial to beat, and provide a useful
reference to quantify progress in machine learning.  Yet, another
reason is that the attempt to engineer an artificially intelligent
solution to a natural recognition problem provides insight into the
nature of intelligence in human and animal brains.  We still cannot
fully explain the human capability to quickly locate and identify
objects in a natural scene, but research into the mechanisms of
vision, and attempts to replicate human capabilities for object
recognition, have already provided crucial insight into the
hierarchical nature of mammalian vision.  %% needs citation

The approach of using classifiers as models of cognition is especially
valuable for the problem of understanding specialization in the brain.
Historically, the earliest discoveries of specialized modules in the
brain were due to lesion studies, where patients who had parts of
their brain destroyed due to injury or clinical surgeries also lost
cognitive or motor functionality as a result.  The loss of
functionality established a causal pathway between the lesioned area
and the affected behavior--as, for example, in the case of the
discovery of Broca's area, which was established in this way to be
critical for speech production [CITE].  However, ethical and practical
limitations restrict the use of lesioning as an experimental
technique, and furthermore, lesion studies cannot be applied to
exhaustively isolate the regions of the brain which are specialized
for a given task.  
%% Next part is a bit brief
%% We want to say that training classifiers on parts of brain is like a ``synthetic lesion'' study,
%% but this would be misleading because unlike lesion studies, these cannot establish causality.
Therefore, an alternative method of studying specialization is to
acquire an image of the brain activity, and then to assess the
discriminative performance of classifiers on either the whole brain,
the brain minus a number of ``lesioned'' areas, or isolated regions of
the brain by themselves.  The classifier may achieve a certain
accuracy using the whole brain image, and reduced accuracies using
``lesioned'' images--and, given the removal of the task-critical parts
of the brain in the image, may be reduced to chance accuracy levels.
Therefore, similar to lesion studies, these classification experiments
can reveal specialization in the brain by establishing which parts of
the brain image contain \emph{information} related to the task.
However, an important distinction is that these studies can only
establish association, rather than causality, because actual lesions
to the brain affect the activity patterns of other parts of the brain,
and therefore the effect of lesions cannot actually be completely
simulated by merely masking parts of the brain image.
%% examples where the artificial classifier does mimic the natural classifier

%% ''puzzling'' is being used in a patronizing manner
However, a potentially puzzling aspect of this approach, is that the
structure or dynamics of the artificially intelligent classifier need
not imitate the dynamics of the biological system for the modelling
approach to be useful.  Computational neuroscientists have developed
biological realistic models of neural signalling activity which could
be employed to simulate the neural decision-making process from input
to output, at a very high level of versimilitude.  However, it is
often more useful in practice to model the decision-making process
with much simpler statistical models, such as linear regression.  Even
relatively complex machine learning models, such as deep neural
networks, represent a vast simplification from biological dynamics.
Yet their use can often be justified by the assumption that the
performance achieved on this task by the given classifier is
reflective of the performance that would be achieved by a more
biologically realistic model.  Indeed, this assumption can often be
justified because, as is generally observed in real-world machine
learning applications, several very different types of classifiers
often end up achieving similar performance (when tuned correctly).

How can we explain the correlation in the performance of very
different families of classifiers across problems in a wide range of
domains? In problems where it is hard to a given family of classifiers
to perform well, it is also hard to other families of classifiers.  In
problems where one classifier achieves very high accuracy, many other
families of classifier can achieve high accuracy as well.  

One explanation for this phenomenon is that the input contains a
certain amount of \emph{information} about the output, and that the
amount of information sets fundamental limits on what kind of
classification accuracies can be achieved.  Indeed, the relationship
between information and bounds on discrimination accuracy were first
established by Claude Shannon, who gave the formula for the
\emph{mutual information} between two random variables $X$ and $Y$,
and whose proved in his noisy-channel theorem an asymptotic
relationship between the mutual information and achievable
discrimination accuracy.  Fano [CITE] followed this result with the
equally classical Fano's inequality, which establishes an upper bound
on accuracy as a function of mutual information.

Neuroscientists have used the connection between information and
discrimination in several ways.  One is to sidestep the application of
classifiers altogether, and to directly estimate the mutual
information between brain activity and the task of interest [CITE].  A
second way to to proceed with training classifiers on the data, but
then summarizing the performance of the classifier by the mutual
information implied by the confusion matrix.  Hence, implied mutual
information becomes a error metric for the classification--but one
that is more readily comparable across different classification tasks.

[OK, now what do we do?  We play around with the connection between MI
  and discrimination some more, and find some things.]

We list these properties of mutual information in preparation for
section \ref{sec:axiom_info}, where we prepare a ``minimal'' set of
properties for an information coefficient, and consider how much of
the functionality of the mutual information would be preserved by an
alternative information coefficient satisfying only those minimal
properties.

But what, exactly, is the functionality of mutual information in
neuroscience?  How it is used in practice?  A nice summary of the
applications of mutual information is provided in the introduction of
Gastpar (2014).  Taking their list as a starting point, we briefly
overview the main use-cases of mutual information, and illustrate each
with a representative example.

\subsubsection{Selecting decoding models.}
Neurons carry information via \emph{spike trains}, which are temporal
point processes.  In response to stimulus $\bX$, the neuron produces a
spike train $Y(t)$ where $Y(t) = 1$ indicates a spike at time $t$ and
$Y(t) = 0$ indicates no spiking, for $t \in [0, T]$.

An open question in neuroscience that of how information is encoded in
the spike train.  Put loosely, what is `signal' in the spike train
$Y(t)$ and what is `noise'?  Presumably there exists
some \emph{decoder}--some function $\vec{g}$ of the time series, which
compresses $Y(t)$ to a small dimension while preserving most of the
information about $\bX$.

Nelken et. al. (2005) investigated the neural code in the A1 auditory
cortex of a cat, in response to recorded birdsongs.  The stimulus $\bX
= (X_1, X_2)$ takes the form of 15 different auditory recordings
presented in 24 spatial directions: $X_1 \in \{1,\hdots, 15\}$ indexes
the recording and $X_2 \in \{1,\hdots, 24\}$ indexes the direction of
presentation.  The response $Y(t)$ takes the form of a spike train.

%First, Nelken et al. found a discretization bin width $\delta t$ which
%appeared to preserve most of the information in $Y(t)$.  Let $\bY$
%represent the discretized signal, where
%\[
%\bY_i = I\{\max_{t \in [(i-1)\delta t, i \delta t]} Y(t)\}
%\]
%for $i = 1,\hdots, \lceil \frac{T}{\delta t}\rceil$.  Making the
%working assumption that $\text{I}(\bX;
%Y(t)) \approx \text{I}(\bX; \bY)$, Nelken estimate
%$\text{I}(\bX; \bY)$ as the information in the spike train.

Nelkin et. al. compare the following \emph{decoders} $\vec{g}$ in terms
of the information $\text{I}(\bX; \vec{g}(Y(t)))$.
\begin{itemize}
\item The total spike count $\vec{g}_1(Y(t)) = \sum_t Y(t)$.
\item The mean response time $\vec{g}_2(Y(t)) = \frac{1}{\sum_t Y(t)} \sum_t t Y(t)$.
\item The combination of the two codes: $\vec{g}_{1+2}(Y(t)) = (\vec{g}_1(Y(t)), \vec{g}_2(Y(t)))$.
\end{itemize}
The information of each decoder is compared to the full information of
the signal, $\text{I}(\bX; Y(t))$, which is estimated via binning.

Nelkin et al. find that while the decoder $\vec{g}_1$ reduces the
mutual information by 20 to 90 percent, the information loss from
$\vec{g}_2$ is much less, and barely any information at all is lost
when both decoders are used jointly in $\vec{g}_{1+2}$.  The
scientific conclusion that can be drawn is that since
$\text{I}(\bX; \vec{g}_{1+2}(Y(t)))$ is not much smaller than
$\text{I}(\bX; Y(t))$, the ``signal'' in the spike train is mostly
captured by the spike counts and response times: beyond that, the
detailed temporal pattern of spiking is likely to be ``noise.''  Of
course, an important caveat to their conclusions is
only \emph{individual} neurons are considered: the analysis did not
rule out the possibility that the temporal spiking pattern could yield
information within an \emph{ensemble} of neurons.


%\item Example: Comparison of decoders in Nelken.  
%Property (i) is important to enable model comparison.  Property (iii)
%is needed because relationships may be nonlinear.
\begin{itemize}
\item Example: Redundancy in population code of retina.  
Property (i)-(iii) and (v) are needed to obtain a meaningful measure
of redundancy.
\item In general, symmetry not important, but additivity is desirable 
for measures of redundancy.  Property (ii) can usually be enforced
since any measure needs to have a unique ``minimum'' value for the
case of independence.
\end{itemize}




%Furthermore, given the
%limit to achievable classification in the data, if the functional
%relationship between input and output is not too complicated, then it
%is likely that many of the most commonly used families of machine
%learning models can approach the limit of achievable classification
%[again, within a rather generous margin, like 10 percent.]

%% introduce information theory and MI here?

%% why do we care about information?? 
\section{Random codes and random classification}

A random code model in information theory is one where given some
distribution $\nu$ (typically a uniform distribution) on the space of
transmittable signals $\mathcal{Y}$, we posit that the decoder $g(M)$
is randomly generated, by letting $g(1),\hdots, g(m)$ be identically
and independently assigned to random draws from $\nu$.  For example,
when $\mathcal{Y}$ is the space of $m$-length binary strings, the
encoder $g$ maps indices $1, \hdots, m$ to random $m$-length binary
strings.  The purpose of the random code model is usually to establish
a lower bound on achievable accuracy given some constraints on the
signal space.

Meanwhile, a randomized classification model is one where the label
set $\{y^{(1)},\hdots, y^{(k)}\}$ is not fixed, but randomly sampled.
One defines a label space $\mathcal{Y}$, a family of conditional
distributions $\{F_{X|y}\}_{y \in \mathcal{Y}}$, and a distribution
$\nu$ on $\mathcal{Y}$.  Then the randomized classification model is a
classification task obtained by drawing labels $\{Y^{(1)},\hdots,
Y^{(k)}\}$ iid from $\nu$, generating training data for those
particular labels by drawing observations $X_i^{(j)} \sim
F_{X|Y^{(j)}}$, and where the problem is to construct a classification
rule for assigning new observations $X$ which are drawn from the
mixture
\[
X \sim \frac{1}{k}\sum_{i=1}^k F_{X|Y^{(k)}}
\]
to one of the labels $\{Y^{(1)},\hdots, Y^{(k)}\}.$ As we will see
throughout the thesis, the randomized classification model can be
naturally applied to a large number of multi-class classification
applications, such as facial recognition, where the labels
(e.g. people) can be justifiably modelled as random draws from some
population.  And, even in the majority of classification problems
where the labels cannot be assumed to come from an iid sample,
randomized classification models can still be applied to provide
intuition for the original problem.

The link between random code models and randomized classification
models should now be apparent. Define $Y_i = g(i)$ in the random code
model, so that $Y_i$ are iid drawn from $\nu$.  Fixing a particular
realization of $Y_1,\hdots, Y_k$, the decoding problem is then
evidently a multi-class classification problem with labels
$\{Y_1,\hdots, Y_k\}$.  The only difference between the two models is
that the conditional distributions $F_{X|Y}$ are assumed to be known
in the random code model, and assumed unknown in the randomized
classification model.

But happens when we assume that $F_{X|Y}$ is known, in the multi-class
classification problem?  In fact, it is common to consider the case of
known $F_{X|Y}$ in the machine learning literature, because the
resulting accruacy gives an \emph{upper bound} on achievable
performance in the multi-class classification problem.  This is
because once $F_{X|Y}$ is known, it is possible to define the
\emph{optimal} classification rule, or \emph{Bayes} classification
rule $h_{Bayes}$.  For example, supposing that the performance
criterion is to minimize the zero-one risk $\Pr[\hat{Y} \neq Y]$, then
the Bayes rule is to assign $X$ to the label with the highest
posterior density,
\[
h_{Bayes}(X) = \text{argmax}_{y \in \{y_1, \hdots, y_k\}} f_{X|y}(x) \pi(y)
\]
where $\pi(y)$ is the prior probability of $Y = y$.  Therefore, it is
the same thing to study the Bayes accuracy in a randomized
classification model and the decoding accuracy of a randomized code in
a noisy channel.

Indeed, the analysis of the Bayes accuracy of a randomized
classification model forms the subject of Chapters 2 and 3.  Due to
the aforementioned equivalence, it can be said that a large body of
work dealing with the Bayes accuracy of randomized classification
problems exists in information theory: however, the majority of such
works deal with the limit as $k \to \infty$, and to our knowledge no
analysis has been done for the case of finite $k$, which is the
relevant scenario for multi-class classification.  There is good
reason for this lacuna in the information theory literature, which is
that since information theorists are concerned with understanding the
properties of \emph{optimal} coding schemes, it follows that
randomized coding schemes are only interesting insofar as that they
give good approximations to optimal coding schemes.  However, in the
classification setting, we may be interested in studying randomized
classification for its own sake, and not merely as a means to obtain
lower bounds or estimates for another problem.  Therefore, Chapter 2
presents novel results on the statistical properties of the Bayes
accuracy in the randomized classification model.  Meanwhile, Chapter 3
studies an interesting application of the randomized classification
problem, which is to analyze the dependence of the classification
accuracy on the size of the label set.  This analysis yields a novel
method for `performance extrapolation' in real-world classification
problems, meaning that our method can be used to estimate the
classification accuracy on a large multi-class classification problem
using data from only a subsample of the classes in the larger problem.
