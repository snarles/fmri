% Chapter 1

\chapter{Multi-class classification, random codes, and information} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------


\section{Introduction}

The concepts of \emph{information} and \emph{discrimination} are
linked at a fundamental level.  A statistical hypothesis test is
\emph{informative} because it provides evidence that the data behaves
according to a certain hypothesis rather than another: it allows us to
\emph{discriminate} between two potential possibilities.  More
generally, a true message contains \emph{information} if it allows the
reciever to understand that the world is in a certain state and not
another: it conveys the fact that out of two possible sets of world
states, that the reciever is in one state rather than another.  The
link between information and discrimination can also be formalized
using the tools of measure theory.  Supposing $\Omega$ is a
probability space defined with respect to a $\sigma$-algebra
$\mathcal{F}$, we can represent our state of knowledge with a
filtration (or sub-$\sigma$-algebra) $\mathcal{F}' \subseteq
\mathcal{F}$.  Complete knowledge (zero uncertainty) is represented by
the full $\sigma$-algebra: that is, $\mathcal{F}' = \mathcal{F}$.
Partial knowledge is represented by a coarser filtration, $\mathcal{F}
\subset \mathcal{F}'$.  The filtration, of course, indicates that our
knowledge is sufficient to \emph{discriminate} the outcome space
$\Omega$ into a number of finitely or infinitely many categories.  The
more information we have, (or, the closer we come to complete
knowledge of the outcome), the more finely we can discriminate the
realized outcomes given by $\omega \in \Omega$.
%% probably need to elaborate on sigma-algebra

A system is \emph{intelligent} if it uses this discriminatory
information to choose the optimal response to the situation.  A
primitive organism may classify percieved objects in its environment
as either beneficial (potential food and resources) or harmful (toxins
and predators), and intelligently respond by means of pursuing the
former and avoiding the latter.  Complex organisms, like humans, not
only discriminate in order to make immediate decisions, but also
categorize objects in the world in order to perform intermediate
calculations and carry out contextual reasoning.  
%% example?

%% Introduce examples of classification and multi-class classification

Similarly, \emph{artificially intelligent} algorithms and agents will
also discriminate input data into categories, either to (i) make
immediate decisions, or (ii) to facillitate intermediate calculations
and optimizations, like intelligent organisms, or (iii) to communicate
the labels of the categories to human users or other algorithms.
Accordingly, a large swath of the field of artificial intelligence
consists of various types of discrimination tasks, from natural
language processing, to object recognition, to game playing, to
multi-class classification.  Among these types of tasks, multi-class
classification is the simplest and the most generally applicable
type of discrimination problem.  Supposing the input data $x$ is
associated with one of $k$ pre-defined classes or labels
$\{y_1,\hdots, y_k\}$, the problem of multi-class classification is to
determine a rule for assigning new inputs $x$ to the correct label.
Examples of multi-class classification applications include:
\begin{itemize}
\item Optical character recognition: labeling bitmaps of handwritten characters with the corresponding character.
\item Biomedical diagnoses: assigning possible diagnoses to patients based on biological measurements and demographic characteristics.
\item Image annotation: assigning descriptive words to images on the internet.
\end{itemize}

%% Write about neuroscience application here
An interesting development in science is the use of \emph{artificial
  intelligence}--that is, machine learning and classification
algorithms, to mimic \emph{natural intelligence.}  This is the case in
neuroscience.  Take for instance the study of \cite{oram1998theideal}.
Researchers showed a macaque monkey the image of a human face in
varying view angles, while recording the response of 32 cells in the
superior temporal to each view angle.  They then used a Bayesian
decoder to build a disciminative model for the view angle from the
data, and assessed its predictive accuracy for determining the view
angle from the responses of the 32 recorded cells.  In nature, the
brain of the macaque monkey infers the rotational angle of various
objects using lower-level neural inputs; Oram et al. mimic the natural
discriminatory behavior by training a model to achieve the same task
from cell recoding data.

%% Multi-class classification was first studied by SHannon
%% in random codes
While multi-class classification is a rapidly developing field within
machine learning, the problem of discriminating inputs according to
discrete classes had been studied even before the advent of artificial
intelligence: arguably, it was Claude Shannon who developed some the
earliest theory pertaining to multi-class classification in his
seminal 1948 paper ``A mathematical theory of communication,'' which
laid the foundation for the field of information theory.  Shannon,
along with other pioneers of information theory such as Robert M. Fano
and Norbert Weiner, recognized that the information content of a
signal depended on how many different messages it can plausibly
convey.  Furthermore, it was Shannon who first recognized that when
the signal is corrupted by noise, the amount of information
\emph{lost} depends on how reliably the original message can be
recovered from the noisy input--in other words, how well the reciever
can \emph{discriminate} the original message on the basis of the
noise-corrupted recieved message.  %% maybe add more details, redundancy of English, etc.

\tikzstyle{block} = [rectangle, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cloud} = [ellipse, draw, fill=white, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
    
\begin{figure}
\centering
\begin{tabular}{ccc}

Multi-class classification & & Information Theory\\

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (init1) {label $Y$};
    \node [cloud, below of=init1] (init2) {distribution $F_{X|Y}$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {classification rule $h(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{Y}$};
    % Draw edges
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

& & 

\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (initA) {message $M$};
    \node [cloud, below of=initA] (initB) {encoder $g(M)$};
    \node [block, below of=initB] (init1) {encoded message $Y$};
    \node [cloud, below of=init1] (init2) {noisy channel $F_{X|Y}$};
    \node [block, below of=init2] (init3) {observation $X$};
    \node [cloud, below of=init3] (init4) {decoder $d(X)$};
    \node [block, below of=init4] (init5) {estimate $\hat{M}$};
    % Draw edges
    \path [line] (initA) -- (initB);
    \path [line] (initB) -- (init1);
    \path [line] (init1) -- (init2);
    \path [line] (init2) -- (init3);
    \path [line] (init3) -- (init4);
    \path [line] (init4) -- (init5);
\end{tikzpicture} 

\end{tabular}
\caption{Comparing the discrimnation tasks in multi-class classification and information theory.}
\label{fig:mcc_vs_it}
\end{figure}

If we compare the discrimination tasks defined in multi-class
classification and information theory, we find much similarity, but
also a few important differences.  Figure \ref{fig:mcc_vs_it} displays
the schematic diagrams of the general multi-class classification
problem and the setup for the noisy channel considered by Shannon.
%% If we put the figure here, probably will need some more explanation

%% we should mention mutual information earlier
From figure \ref{fig:mcc_vs_it}, we can see major similarities between
the multi-class classification problem and the noisy decoding problem
studied in information theory: both involve inferring some latent
variable $Y$ on the basis of observed $X$.  We will go into great
detail about the similarities and differences in section
\ref{sec:rand_code_rand_class}.  However, despite these similarities,
it has historically been the case that the machine learning literature
and the information theory literature diverge in terms of the metric
used to characterize performance in the discrimination task.  In
multi-class learning, the performance is generally characterized by
the \emph{accuracy} of the classification--the probability that the
label is assigned correctly.  However, in information theory, a
quantity called the \emph{mutual information}, which was invented by
Claude Shannon, is used to characterize the quality of the noisy
channel with regards to achievable decoding performance. %% jargony phrase, due to the idea being difficult to express

\section{Supervised learning}

The generalization error of the learner as a statistic.

\subsection{General characaterization of supervised learning}

\section{Mutual information}

\subsection{Definition and history}

\subsection{Usage in neuroscience}


\section{Random codes and random classification}\label{sec:rand_code_rand_class}

In multi-class classification, we may assume without loss of generality that the data has been generated in the following manner:
\begin{enumerate}
\item First, a label $Y$ is drawn according to some distribution from the label set $\{y_1,\hdots, y_k\}$.
\item Secondly, the new observation $X$ is drawn according to the unknown conditional distribution $F_{X|Y}$.
\item Finally, an estimated label $\hat{Y} = h(X)$ is obtained
  according to a data-dependent classification rule, $h$.
  Typically, $h$ is determined by fitting a model to training data.
\end{enumerate}
In the particular application, the above description may not match the
\emph{causal relationship} between $X$ and $Y$: however, whether $X$
is drawn conditional on $Y$, or $Y$ is drawn conditional on $X$, or
that $(X, Y)$ are drawn from some joint distribution, makes no
difference from the theoretical standpoint, since only the statistical
(and not causal) properties of the joint distribution $(X, Y)$ are
relevant for determining the peformance of the classification rule $h(X)$.

Meanwhile, in the noisy channel model, we assume that the sender wants
to transmit message $M$, out of a finite set of possible messages
$\mathcal{M} = \{1,\hdots, m\}$.  The message must be encoded into a
signal $Y \in \mathcal{Y}$, which is sent through a stochastic channel
$F_{X|Y}$.  Given that a signal $y$ is sent through the channel, the
reciever observes a signal $X$ drawn from the distribution $F_{X|y}$.
The communications problem is to design an encoding function $g(M)$,
which is an injective map from messages $\{1,\hdots,m\}$ to signals in
$\mathcal{M}$, and a corresponding decoding function $d(X)$ which
infers the message $\{1,\hdots, m\}$ from the recieved signal $X$.

Therefore, we see that in both the multi-class classification problem
and the noisy channel model present examples of discrimination
problems where one must recover some latent variable $Y$ from
observations $X$, where $X$ is related to $Y$ through the family of
conditional distributions $F_{X|Y}$.  One difference is that while in
multi-class classification, $F_{X|Y}$ is unknown and has to be
inferred from data, in the noisy channel model, the stochastic
properties of the channel $F_{X|Y}$ are usually assumed to be known.
For example, for a binary channel where $Y$ consists of an $m$-length
binary string, a commonly studied channel $F_{X|Y}$ generates $X$ by
randomly flipping bits in $Y$ independently with some probability
$\epsilon$.  A second difference is that in the noisy channel model,
there is a choice in how to specify the encoding function $g(M)$,
which affects subsequent performance.  Finally, in the broader
research context, machine learning research has traditionally focused
on multi-class problems with relatively few classes, while information
theory tends to consider problems in asymptotic regimes where the
number of possible messages $m$ is taken to infinity. These
differences were sufficient to explain why little overlap exists in
the respective literatures between multi-class classification and the
noisy channel model.  

However, an interesting development in the machine learning community
has been the application of multi-class classification to problems
with increasingly large and complex label sets.  Consider the
following timeline of representative papers in the multi-class
classification literature:
\begin{itemize}
\item Fisher's Iris data set, \cite{fisher1936use}, $K = 3$ classes
\item Letter recognition, \cite{frey1991letter}, $K = 26$ classes
\item Michalski's soybean dataset, \cite{mickalstd1980learning}, $K = 15$ classes
\item The NIST handwritten digits data set, \cite{grother1995nist}, $K = 10$ classes
\item Phoneme recognition on the TIMIT datset, \cite{clarkson1999use}, $K = 39$ classes
\item Object categorization using Corel images, \cite{duygulu2002object} $K = 371$ classes
\item Object categorization for ImageNet dataset, \cite{deng2010does}, $K = 10,184$ classes
\item The 2nd Kaggle large-scale hierarchical text classification challenge (LSHTC), \cite{partalas2015lshtc}, $K = 325,056$
\end{itemize}
As we can see, in recent times we begin to see classification problems
with extremely large label sets.  In such large-scale classification
problems, or `extreme' classification problems, results for $K \to
\infty$ numbers of classes, like those found in information theory,
begin to look more applicable.

This work focuses on a particular intersection between multi-class
classification and information theory, which is the study of
\emph{random classification tasks.}  In numerous domains of applied
mathematics, it has been found that systems with large numbers of
components can be modelled using randomized versions of those same
systems, which are more tractable to mathematical analysis: for
example, studying the properties of networks by studying random graphs
in graph theory, or studying the performance of combinatorial
optimization algorithms for random problem instances.  Similarly, it
makes sense to posit randomized models of multi-class discrimination
problems.  Since information theorists were the first to study
discrimination problems with large number of classes, we find in the
information theory literature a long tradition of the study of
\emph{random code} models, dating back all the way to Shannon's
seminal 1948 paper.  This thesis is dedicated to the the study of the
analogue of random code models in the multi-class classification
setting: models of \emph{randomized classification.}

Furthermore, as we might expect from the close connection between
random codes and randomized classification, we are able to form links
between multi-class classification and information theory.  Therefore,
Chapters 4 and 5 leverage the link between randomized classification
and information theory in order to develop new estimators of mutual
information in high-dimensional datasets consisting of pairs ob
observations $(X, Y)$.  While Chapter 4 works in a setting where a
true, sparse relationship between $Y$ and $X$ is assumed to be known,
Chapter 5 makes a different assumption, which is that $X$ is
high-dimensional, and that its components are not too dependent.

In the following sections, we review the relevant background for
information theory, the applications of information theory to
neuroscience, and multi-class classification, then introduce the
connection between random code models in information theory and
randomized classification.


%% Make the connection between random codes and randomized classification,
%% point out why randomized classification is a good model for some contemporary problems


A random code model in information theory is one where given some
distribution $\nu$ (typically a uniform distribution) on the space of
transmittable signals $\mathcal{Y}$, we posit that the decoder $g(M)$
is randomly generated, by letting $g(1),\hdots, g(m)$ be identically
and independently assigned to random draws from $\nu$.  For example,
when $\mathcal{Y}$ is the space of $m$-length binary strings, the
encoder $g$ maps indices $1, \hdots, m$ to random $m$-length binary
strings.  The purpose of the random code model is usually to establish
a lower bound on achievable accuracy given some constraints on the
signal space.

Meanwhile, a randomized classification model is one where the label
set $\{y^{(1)},\hdots, y^{(k)}\}$ is not fixed, but randomly sampled.
One defines a label space $\mathcal{Y}$, a family of conditional
distributions $\{F_{X|y}\}_{y \in \mathcal{Y}}$, and a distribution
$\nu$ on $\mathcal{Y}$.  Then the randomized classification model is a
classification task obtained by drawing labels $\{Y^{(1)},\hdots,
Y^{(k)}\}$ iid from $\nu$, generating training data for those
particular labels by drawing observations $X_i^{(j)} \sim
F_{X|Y^{(j)}}$, and where the problem is to construct a classification
rule for assigning new observations $X$ which are drawn from the
mixture
\[
X \sim \frac{1}{k}\sum_{i=1}^k F_{X|Y^{(k)}}
\]
to one of the labels $\{Y^{(1)},\hdots, Y^{(k)}\}.$ As we will see
throughout the thesis, the randomized classification model can be
naturally applied to a large number of multi-class classification
applications, such as facial recognition, where the labels
(e.g. people) can be justifiably modelled as random draws from some
population.  And, even in the majority of classification problems
where the labels cannot be assumed to come from an iid sample,
randomized classification models can still be applied to provide
intuition for the original problem.

The link between random code models and randomized classification
models should now be apparent. Define $Y_i = g(i)$ in the random code
model, so that $Y_i$ are iid drawn from $\nu$.  Fixing a particular
realization of $Y_1,\hdots, Y_k$, the decoding problem is then
evidently a multi-class classification problem with labels
$\{Y_1,\hdots, Y_k\}$.  The only difference between the two models is
that the conditional distributions $F_{X|Y}$ are assumed to be known
in the random code model, and assumed unknown in the randomized
classification model.

But happens when we assume that $F_{X|Y}$ is known, in the multi-class
classification problem?  In fact, it is common to consider the case of
known $F_{X|Y}$ in the machine learning literature, because the
resulting accruacy gives an \emph{upper bound} on achievable
performance in the multi-class classification problem.  This is
because once $F_{X|Y}$ is known, it is possible to define the
\emph{optimal} classification rule, or \emph{Bayes} classification
rule $h_{Bayes}$.  For example, supposing that the performance
criterion is to minimize the zero-one risk $\Pr[\hat{Y} \neq Y]$, then
the Bayes rule is to assign $X$ to the label with the highest
posterior density,
\[
h_{Bayes}(X) = \text{argmax}_{y \in \{y_1, \hdots, y_k\}} f_{X|y}(x) \pi(y)
\]
where $\pi(y)$ is the prior probability of $Y = y$.  Therefore, it is
the same thing to study the Bayes accuracy in a randomized
classification model and the decoding accuracy of a randomized code in
a noisy channel.

Indeed, the analysis of the Bayes accuracy of a randomized
classification model forms the subject of Chapters 2 and 3.  Due to
the aforementioned equivalence, it can be said that a large body of
work dealing with the Bayes accuracy of randomized classification
problems exists in information theory: however, the majority of such
works deal with the limit as $k \to \infty$, and to our knowledge no
analysis has been done for the case of finite $k$, which is the
relevant scenario for multi-class classification.  There is good
reason for this lacuna in the information theory literature, which is
that since information theorists are concerned with understanding the
properties of \emph{optimal} coding schemes, it follows that
randomized coding schemes are only interesting insofar as that they
give good approximations to optimal coding schemes.  However, in the
classification setting, we may be interested in studying randomized
classification for its own sake, and not merely as a means to obtain
lower bounds or estimates for another problem.  Therefore, Chapter 2
presents novel results on the statistical properties of the Bayes
accuracy in the randomized classification model.  Meanwhile, Chapter 3
studies an interesting application of the randomized classification
problem, which is to analyze the dependence of the classification
accuracy on the size of the label set.  This analysis yields a novel
method for `performance extrapolation' in real-world classification
problems, meaning that our method can be used to estimate the
classification accuracy on a large multi-class classification problem
using data from only a subsample of the classes in the larger problem.






%% Link back to Shannon and information theory.  We develop further links between randomized classification and information theory.


\section{Generalizations of information}

\subsection{Information axioms}

\subsection{Information coefficients based on supervised learning}

