\title{Prediction, information, and inference in neuroimaging}
\author{Charles Zheng and Yuval Benjamini}
\date{\today}

\documentclass[12pt]{article} 

% packages with special commands
\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}

\definecolor{grey}{rgb}{0.5,0.5,0.5}

\begin{document}
\maketitle

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\Cor}{\text{Cor}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bV}{\boldsymbol{V}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]



\begin{abstract}
Neuroscientists have a variety of tools for quantifying multivariate
dependence: mutual information, linear correlation-based statistics,
Fisher information, and more recently, measures of performance on
supervised learning tasks such as classification.  We argue that both
mutual information and classification accuracy capture intuitive
properties of an ``information coefficient'' for a channel, and we proceed
to develop a general axiomatic characterization of information coefficients
for channels consisting of a pair of input and output random
variables.  Arguably, the key properties of an information coefficient are
that (i) it is a scalar measure of multivariate dependence, and (ii)
that it satisfies a \emph{stochastic data-processing inequality}: any
coefficient with such properties can be used for model selection.
%The key axioms of an information coefficient are that (i) it is
%a scalar functional of the joint distribution of the input-output
%pair, (ii) it is zero for independent variables, and positive for
%dependent variables, (iii) satisfies a
%generalized \emph{data-processing} inequality, where transformations
%of the output can only preserve or reduce the information.  
We show how prediction tasks can be used to define a general class of
information coefficients which includes mutual information, as well as a
novel information coefficient, \emph{average Bayes accuracy}, which can be
considered an ``idealization'' of classification accuracy.
Furthermore, we consider the possibility of developing a general
theory of statistical inference for this class of information coefficients.
Concretely, we derive a lower confidence bound for average Bayes
accuracy as well as a novel lower confidence bound for mutual
information.
\end{abstract}


\section{Introduction}

Historically, neuroscience has largely taken a reductionist approach
to understanding the nervous system, proceeding by defining elements
and subelements of the nervous system (e.g. neurons), and
investigating relationship between two different elements, or the
response of an element to external stimulation: say, the response of a
neuron's average firing rate to skin temperature.  At one level of
abstraction, neuroscientists might seek to characterize the functional
relationship between elements, but at a higher level of abstraction,
it may be sufficient to report scalar measures of dependence.  Since
neural dynamics are generally both stochastic and nonlinear, it was a
natural choice for early neuroscientists to adopt
Shannon's \emph{mutual information} as a quantitative measure of
dependence.  But as new technologies enabled the recording of neural
data at larger scales and resolution, the traditional reductionist
goals of neuroscience were supplemented by increasingly ambitious
attempts within neuroscience to understand the dynamics of neural
ensembles, and by efforts originating within psychology and medicine
to link the structure and function of the entire human brain to
behavior or disease.  The larger scope of the data and the questions
being asked of the data created an increasing demand for multivariate
statistical methods for analyzing neural data of increasingly high
dimension.  Due to the complexity, variety, and practical difficulties
of multivariate statistical analysis of the brain, alternative
measures of multivariate dependence such as linear-based correlational
statistics, or Fisher information, started to gain traction.  For the
most part, alternative measures of dependence sacrifice flexibility
for a gain in practical convenience: linear-based statistics such as
canonical correlation or correlation coefficients fail to capture
nonlinear dependencies, and Fisher information requires strong
parametric assumptions.  Therefore, it was of considerable interest
when Haxby (2001) introduced the usage of \emph{supervised learning}
(classification tasks) for the purpose of quantifying stimulus
information in task fMRI scans.  Since then, an entire subfield of
neuroimaging, multivariate pattern analysis (MVPA) has been
established dedicated to quantifying multivariate information in the
brain, and both mutual information and classification accuracy are
used by practitioners within the field.  Judging from the language
used by the practioners themselves, it is intuitively clear to them
how classification accuracies can be used to quantify information in
brain scans.  However, a more thorough examination of the practice
raises many questions with regards to the use of classification
accuracy as a coefficient of information: this is one motivation for the
current work.  But taking a step back, it would seem valuable at this
historical juncture to examine the intuitive properties of
``information'' as a measure of multivariate dependence, and not only
consider whether classification accuracy can be considered or used to
derive a new information coefficient, but whether other such coefficients might
also exist, and whether a unified theory can be developed to account
for all of them.  This is the larger purpose of the current work, and
towards that end we not only propose a general class of information
coefficients which unifies both information-theoretic and
supervised-learning-based approaches, but with an eye toward practical
applications, we also examine the question of inferring these
quantities from data.  An initial result in this direction is the
derivation of nonparametric lower confidence bounds for average Bayes
accuracy (a novel information coefficient closely related to classification
accuracy,) and an inequality between average Bayes accuracy and mutual
information, which, combined with the preceding result, yields a novel
lower confidence bound for mutual information.

\subsection{Organization}

The rest of the paper is organized as follows.  Section \ref{sec:info}
plays the role of a ``background'' section that gives the basics of
mutual information and supervised learning as they are used in
neuroscience, as well as practical issues related to estimation.  In
section \ref{sec:gen_class}, we present an axiomatic characterization
of information coefficients, and introduce a general class of information
coefficients which satisfies our axioms.  We define a new information
coefficient belonging to this class, average Bayes accuracy, and we also
show how mutual information can be considered an ``extended member''
of the class.  In section \ref{sec:inference} we develop the basic
theory of what kinds of inferences about our information coefficients are
possible discuss the kinds of experimental designs and supervised
learning pipelines which are needed to enable such inference.
Concretely, we develop a lower confidence bound for average Bayes
accuracy. In section \ref{sec:connections} we outline a comparative
theory for different coefficients within our framework: how are the
different information coefficients related?  We discuss the calculus of
variations as a possible general technique for establishing
inequalities between different information coefficients, and in particular
we derive a ``randomized Fano's inequality'': a lower bound for mutual
information as a function of average Bayes accuracy.  Combined with
our lower confidence bound for average Bayes accuracy, this yields a
novel lower confidence bound for mutual information.  We provide a
practical data analysis example in section \ref{sec:applications}.  A
discussion section includes future directions and loose ends are
treated, and most of the technical proofs and lemmas are found in the
appendix.

\section{Background}\label{sec:info}

\subsection{Mutual information and its usage}\label{sec:background_mi}
%It is in the context of communication system that Shannon first
%proposed a quantification of the concept of ``the amount of
%information that $\bY$ carries about $\bX$'', in the form
%of \emph{mutual information}: 
While Shannon's theory of information was motivated by the problem of
designing communications system, the applicability of mutual
information was quickly recognized by neuroscientists.  Only four
years after Shannon's seminal paper in information theory (1948),
McKay and McCullough (1952) inaugurated the application of mutual
information to neuroscience.  If $\bX$ and $\bY$ have joint density
$p(\bx, \by)$ with respect to the product measure $\mu_x \times \mu_y$, then the mutual information is defined as
\[
\text{I}(\bX;\bY) = \int p(\bx, \by) \log \frac{p(\bx, \by)}{p(\bx)p(\by)}d\mu_x(\bx) d\mu_y(\by).
\]
where $p(\bx)$ and $p(\by)$ are the marginal densities with respect to
$\mu_x$ and $\mu_y$\footnote{Note that the mutual information is invariant with respect to change-of-measure.}.  Since then, mutual information has enjoyed a
celebrated position in both experimental and theoretical neuroscience.
Experimentally, mutual information has been used to detect strong
dependencies between stimulus features and features derived from
neural recordings, which can be used to draw conclusions about the
kinds of stimuli that a neural subsystem is designed to detect, or to
distinguish between signal and noise in the neural output.
Theoretically, the assumption that neural systems maximize mutual
information between salient features of the stimulus and neural output
has allowed scientists to predict neural codes from signal processing
models: for instance, the center-surround structure of human retinal
neurons matches theoretical constructions for the optimal filter based
on correlations found in natural images [cite].

The mutual information measures the information ``capacity'' of a
channel consisting of an input $\bX$ and an output $\bY$, and
satisfies a number of important properties.
\begin{enumerate}
\item The channel input $\bX$ and output $\bY$ can be random vectors of arbitrary dimension, and the mutual information remains a scalar functional of the joint distribution $P$ of $(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\text{I}(\bX; \bY) = 0$; otherwise, $\text{I}(\bX; \bY) > 0$.
\item The data-processing inequality: for any vector-valued function $\vec{f}$ of the output space,
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
\item Symmetry: $\text{I}(\bX; \bY) = \text{I}(\bY; \bX)$.
\item Independent additivity: if $(\bX_1,\bY_1)$ is independent of $(\bX_2, \bY_2)$, then
\[
\text{I}((\bX_1,\bY_1); (\bX_2, \bY_2)) = \text{I}(\bX_1; \bY_1) + \text{I}(\bX_2; \bY_2).
\]
\end{enumerate}
Three additional consequences result from the data-processing inequality:
\begin{itemize}
\item \emph{Stochastic data-processing inequality}  If $\vec{f}$ is a stochastic function independent of both $\bX$ and $\bY$, then
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
This can be shown as follows: any stochastic function $\vec{f}(\bY)$
can be expressed as a deterministic function $\vec{g}(\bY, W)$, where
$W$ is a random variable independent of $\bX$ and $\bY$.
By independent additivity,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)).
\]
Then, by the data-processing inequality,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)) \geq \text{I}(\bX; \vec{g}(\bY, W)) = \text{I}(\bX; \vec{f}(\bY)).
\]
\item \emph{Invariance under bijections.} If $\vec{f}$ has an inverse $\vec{f}^{-1}$, then 
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY) = \text{I}(\bX; \vec{f}^{-1}(\vec{f}(\bY))) \leq \text{I}(\bX; \vec{f}(\bY)),
\]
therefore, $\text{I}(\bX; \vec{f}(\bY)) = \text{I}(\bX; \bY)$.
\item \emph{Monotonicity with respect to inclusion of outputs.}  Suppose we have an output ensemble $(\bY_1,\bY_2)$.  Then the individual component $\bY_1$ can be obtained as a projection of the ensemble.  By the data-processing inequality, we therefore have
\[
\text{I}(\bX; \bY_1) \leq \text{I}(\bX; (\bY_1, \bY_2)).
\]
Intuitively, if we observe both $\bY_1$ and $\bY_2$, this can
only \emph{increase} the information we have about $\bX$ compared to
the case where we only observe $\bY_1$ by itself.
\end{itemize}
And it is the property of \emph{invariance under bijections},
inclusive of non-linear bijections, which qualifies mutual information
as a \emph{non-linear measure of dependence.}  Linear correlations are
invariant under scaling and translation, but not invariant
to \emph{nonlinear} bijections.

%As for
%the completeness of the five listed properties: as we know, Shannon's
%mutual information (up to arbitrary scaling factor) is the only
%functional proposed in the literature which satisfies all five
%properties.
Besides the formal definition, there are a number of well-known alternative
characterizations of mutual information in terms of other
information-theoretic quantities: the \emph{entropy} $\text{H}$:
\[
\text{H}_\mu(\bX) = -\int p(\bX) \log p(\bX) d\mu(\bX),
\]
and the \emph{conditional entropy}:
\[
\text{H}_\mu(\bX|\bY) = -\int p(\bY) d\mu_y(\bY) \int p(\bX|\bY) \log p(\bX|\bY) d\mu_x(\bX).
\]
Some care needs to be taken with entropy and conditional entropy since
they are not invariant with respect to change-of-measure: hence the
use of the subscript in the notation $\text{H}_\mu$.  In particular,
there is a difference between \emph{discrete entropy} (when $\mu$ is
the counting measure) and \emph{differential entropy} (when $\mu$ is
$p$-dimensional Lesbegue measure.)  Intutively, entropy measures an
observer's uncertainty of the random variable $\bX$, supposing the
observer has no prior information other than the distribution of
$\bX$. Conditional entropy measures the \emph{expected uncertainty} of
$\bX$ supposing the observer observes $\bY$.

The following identities characterize mutual information in terms of entropy:
\[
\text{I}(\bX; \bY) = \text{H}_{\mu_x \times \mu_y}((\bX, \bY)) - \text{H}_{\mu_x}(\bX) - \text{H}_{\mu_y}(\bY).
\]
\begin{equation}\label{eq:ce_ident}
\text{I}(\bX; \bY) = \text{H}_\mu(\bY) - \text{H}_\mu(\bY|\bX).
\end{equation}
The second identity \eqref{eq:ce_ident} is noteworthy
as being practically important for estimation of mutual information.
Since the entropies in question only depend on the marginal and
conditional distributions of $\bY$, the problem of estimating
$\text{I}(\bX; \bY)$ can be reduced from a $\dim(\bX)
+ \dim(\bY)$-dimensional nonparametric estimation problem to a
$\dim(\bY)$-dimensional problem: hence this identity is a basis of
several methods of estimation used in neuroscience, such as Gastpar
(2014).

However, by symmetry, we also have the flipped identity
\begin{equation}\label{eq:ce_ident2}
\text{I}(\bX; \bY) = \text{H}_\mu(\bX) - \text{H}_\mu(\bX|\bY).
\end{equation}
In neuroscience studies, where $\bX$ is the controlled stimulus, and
$\bY$ is the neural activity, the two mirror pairs \eqref{eq:ce_ident}
and \eqref{eq:ce_ident2} have different interpretations.  Rather than
providing a basis for practical estimation, \eqref{eq:ce_ident2}
provides an \emph{interpretation} of the mutual information.  Loosely
speaking, $\text{H}_\mu(\bX)$ is the uncertainty of $\bX$ before
having observed $\bY$, and $\text{H}_\mu(\bX|\bY)$ is the uncertainty
of $\bX$ after having observed $\bY$, hence $\text{H}_\mu(\bX)
- \text{H}_\mu(\bX|\bY)$ is how much the observation of $\bY$
has \emph{reduced} the uncertainty of $\bX$.  Stated in words,
\[
\text{I}(\bX; \bY) = \text{average reduction of uncertainty about $\bX$ upon observing $\bY$}.
\]

We list these properties of mutual information in preparation for
section \ref{sec:axiom_info}, where we prepare a ``minimal'' set of
properties for an information coefficient, and consider how much of
the functionality of the mutual information would be preserved by an
alternative information coefficient satisfying only those minimal
properties.

But what, exactly, is the functionality of mutual information in
neuroscience?  How it is used in practice?  A nice summary of the
applications of mutual information is provided in the introduction of
Gastpar (2014).  Taking their list as a starting point, we briefly
overview the main use-cases of mutual information, and illustrate each
with a representative example.

\subsubsection{Selecting decoding models.}
Neurons carry information via \emph{spike trains}, which are temporal
point processes.  In response to stimulus $\bX$, the neuron produces a
spike train $Y(t)$ where $Y(t) = 1$ indicates a spike at time $t$ and
$Y(t) = 0$ indicates no spiking, for $t \in [0, T]$.

An open question in neuroscience that of how information is encoded in
the spike train.  Put loosely, what is `signal' in the spike train
$Y(t)$ and what is `noise'?  Presumably there exists
some \emph{decoder}--some function $\vec{g}$ of the time series, which
compresses $Y(t)$ to a small dimension while preserving most of the
information about $\bX$.

Nelken et. al. (2005) investigated the neural code in the A1 auditory
cortex of a cat, in response to recorded birdsongs.  The stimulus $\bX
= (X_1, X_2)$ takes the form of 15 different auditory recordings
presented in 24 spatial directions: $X_1 \in \{1,\hdots, 15\}$ indexes
the recording and $X_2 \in \{1,\hdots, 24\}$ indexes the direction of
presentation.  The response $Y(t)$ takes the form of a spike train.

%First, Nelken et al. found a discretization bin width $\delta t$ which
%appeared to preserve most of the information in $Y(t)$.  Let $\bY$
%represent the discretized signal, where
%\[
%\bY_i = I\{\max_{t \in [(i-1)\delta t, i \delta t]} Y(t)\}
%\]
%for $i = 1,\hdots, \lceil \frac{T}{\delta t}\rceil$.  Making the
%working assumption that $\text{I}(\bX;
%Y(t)) \approx \text{I}(\bX; \bY)$, Nelken estimate
%$\text{I}(\bX; \bY)$ as the information in the spike train.

Nelkin et. al. compare the following \emph{decoders} $\vec{g}$ in terms
of the information $\text{I}(\bX; \vec{g}(Y(t)))$.
\begin{itemize}
\item The total spike count $\vec{g}_1(Y(t)) = \sum_t Y(t)$.
\item The mean response time $\vec{g}_2(Y(t)) = \frac{1}{\sum_t Y(t)} \sum_t t Y(t)$.
\item The combination of the two codes: $\vec{g}_{1+2}(Y(t)) = (\vec{g}_1(Y(t)), \vec{g}_2(Y(t)))$.
\end{itemize}
The information of each decoder is compared to the full information of
the signal, $\text{I}(\bX; Y(t))$, which is estimated via binning.

Nelkin et al. find that while the decoder $\vec{g}_1$ reduces the
mutual information by 20 to 90 percent, the information loss from
$\vec{g}_2$ is much less, and barely any information at all is lost
when both decoders are used jointly in $\vec{g}_{1+2}$.  The
scientific conclusion that can be drawn is that since
$\text{I}(\bX; \vec{g}_{1+2}(Y(t)))$ is not much smaller than
$\text{I}(\bX; Y(t))$, the ``signal'' in the spike train is mostly
captured by the spike counts and response times: beyond that, the
detailed temporal pattern of spiking is likely to be ``noise.''  Of
course, an important caveat to their conclusions is
only \emph{individual} neurons are considered: the analysis did not
rule out the possibility that the temporal spiking pattern could yield
information within an \emph{ensemble} of neurons.


%\item Example: Comparison of decoders in Nelken.  
%Property (i) is important to enable model comparison.  Property (iii)
%is needed because relationships may be nonlinear.
\begin{itemize}
\item Example: Redundancy in population code of retina.  
Property (i)-(iii) and (v) are needed to obtain a meaningful measure
of redundancy.
\item In general, symmetry not important, but additivity is desirable 
for measures of redundancy.  Property (ii) can usually be enforced
since any measure needs to have a unique ``minimum'' value for the
case of independence.
\end{itemize}

\subsection{Supervised learning}\label{sec:background_sl}

\begin{itemize}
\item Supervised learning task is defined using a prediction task.
\item 1. A predictive model is learned using training data
\item 2. The performance of the model on the prediction task is estimated using independent test data
\item Classical examples of prediction tasks: regression and classification
\item Third example: identification
\item Definition of Bayes prediction model
\item General definition of supervised learning task
\item SL performance can be a scalar
\item SL can be used to test for independence
\item Bayes performance satisfies data-processing inequality
\item How SL is interpreted in MVPA as information
\item Section 3, we'll see how Bayes performance can be legitimately considered a measure of information
\end{itemize}

\subsection{Connections}

\begin{itemize}
\item Information theory and decoding. Fano's inequality
\item Quiroga's method
\item MVPA people use them interchangeably
\end{itemize}

\section{Information, uncertainty and prediction}\label{sec:gen_class}

In the previous section, we examined how mutual information and
supervised learning are used in neuroscience.  By analyzing the
use-cases of each method side-by-side, and by reviewing the known
connections between the two methods, we hoped to suggest the notion
that both mutual information and supervised learning are being
employed as means of quantifying the same underlying concept.  In this
section, we propose a reification of this underlying concept of
``information'', and propose the \emph{minimal} properties needed for
a functional to be considered an \emph{information coefficient.}  We
argue that these minimal properties are sufficient to support many of
the existing use-cases of mutual information and supervised learning
in neuroscience.

\subsection{Axiomatic characterization of information}\label{sec:axiom_info}

We claim that neuroscientists use both mutual information and
supervised learning to quantifying a common concept of
``information.''  Furthermore, we claim that neuroscientists largely
share a set of common intuitions about information.

\begin{itemize}
\item \emph{
Intuition 1: Information is a measure of dependence.}   If $\bX$ and $\bY$ are statistically independent, then
$\bX$ gives no information about $\bY$, and vice-versa.
\end{itemize}
This
intuition is employed when researchers test the null hypothesis of
chance accuracy for classification.  If the null is accepted, the
researcher concludes that there is no information in the predictors
about the response.

\begin{itemize}
\item \emph{
Intuition 2a: Monotonicity with respect to inclusion of outputs.} If $\bY_1$ and $\bY_2$ are ensembles of neurons (or
individual neurons), then the combined ensemble $(\bY_1, \bY_2)$ has equal or
more information about $\bX$ than either component by itself.
\end{itemize}

\begin{itemize}
\item \emph{
Intuition 2b: Noise adds no information.} If, in the previous example, $\bY_2$ is independent of
$\bX$, then $\bY_2$ adds no information to the ensemble.
\end{itemize}
The non-informativity of noise is vital for the purpose
of \emph{localizing} information within fMRI voxels (as is done in
searchlight analysis.)  Since noise voxels fail to improve
classification performance (and indeed, sometimes harm empirical
performance,) the optimal searchlight radius will concentrate on
clusters of signal voxels, and minimize the inclusion of noise voxels.

\begin{itemize}
\item \emph{Intuition 3:}
Information can be used as a basis of model selection.  Among multiple
encoding/decoding models, a more accurate model should tend to have
greater information relative to less accurate models.  
\end{itemize}
Compared to the first two, this third intuition is somewhat less
obvious, but nevertheless appears as an important use-case for mutual
information, as seen in the application of mutual information to
choose encoding models.

We discuss a fourth intuition, regarding the use of information as a
means of assessing redundancy, in section \ref{sec:redundancy}.

Given the first two intuitions, we find that an essential property of
information is that it can be said to `increase'--i.e., there exists
at least a partial ordering on information.  Furthermore, there should
exist a minimal element in this ordering, which is the information
between independent variables--that is, `no information.' However,
this is fully consonant with either information being quantified as a
scalar quantity, or as a positive-definite matrix.  Indeed, Fisher
information could be taken as an example of a matrix-valued
information coefficient.  However, the problem with matrix-valued
coefficients is that channels may be incomparable within the partial
ordering.  Thus, if we consider the third intuition an important
property of an information coefficient, then it should be scalar to
enable model comparison.

We find that the first three intuitions follow from the following
axioms.

\noindent\textbf{Axioms of information}

Let $\mathcal{I}(\bX; \bY)$ denote an \emph{information coefficient}.
Then,
\begin{enumerate}
\item $\mathcal{I}(\bX; \bY)$ is a scalar functional of the joint distribution $P$ of $(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\mathcal{I}(\bX; \bY) = 0$; otherwise, $\mathcal{I}(\bX; \bY) \geq 0$.
\item 
The stochastic data-processing inequality.  If $\vec{f}$ is stochastic
vector-valued function of the output space indepedent of $(\bX, \bY)$,
then
\[
\mathcal{I}(\bX; \vec{f}(\bY)) \leq \mathcal{I}(\bX; \bY).
\]
\end{enumerate}

Since this minimal list of properties is a subset of the properties of
mutual information, it is clear that mutual information satisfies the
axioms.  However, classification accuracy does not satisfy the
axioms--we will see in section \ref{sec:gen_class_sub} how to define
a proper information coefficient based on supervised learning.

Now let us check that any information coefficient
$\mathcal{I}(\bX; \bY)$ necessarily satisfies the intuitions.
Intuition 1 is satisfied by property (ii).  We gave a general argument
in \ref{sec:background_mi} how Intuition 2a follows from the
data-processing inequality.  Intuition 2b follows from the stochastic
data-processing inequality, taking $\vec{f}(\bY_1) = (\bY_1, \bY_2)$.
Now, in order to justify intuition 3, we need to formalize the notion
of ``model selection.''

\subsection{Information as a means of model selection}

A complete discussion of model selection falls outside the scope of
the paper, so we limit the discussions to the important special case
of selecting an encoding model.

Let $\bX$ be some environmental stimulus, and
\[
\bY = \vec{f}(g(\bX))
\]
where $\vec{f}$ is a stochastic function independent of $\bX$, and $g$ is
an \emph{encoding function}, which is known to lie in some class of
functions $\mathcal{G}$.  The goal of model selection is estimate $g$.

Given multiple competing models $(\hat{S}_1, \hat{g}_1),\hdots,
(\hat{S}_k, \hat{g}_k)$, we obtain a lower confidence bound on the maximum score,
\[
M = \max_{i=1}^k \mathcal{I}(\bY; \hat{g}_i(\bX)).
\]
We can then test each individual model for the hypothesis
$H_i: \mathcal{I}(\bY; \hat{g}_i(\bX)) < M$ at level $\alpha/k$.  All of the models which
are rejected are considered `candidate models.'  While a common next
step is to select a single model from the set of candidates
(e.g. using a measure of complexity), this is not essential to our
discussion.  For now, we are only concerned that the model selection
procedure should at the very least, \emph{not reject}
the \emph{correct} model $g$ in the circumstance that $g$ is included
in the initial set of candidate models.  A necessary condition for
this criterion is that the correct model $g$ maximizes the
information:
\begin{equation}\label{eq:model_selection}
\mathcal{I}(\bY; g(\bX)) = \sup_{g \in \mathcal{G}} \mathcal{I}(\bY; g(\bX)).
\end{equation}

Indeed, the criterion \eqref{eq:model_selection} is a consequence of
the stochastic data-processing inequality.  First observe that letting
$V = g(\bX)$, we have $\bX$ and $\bY$ conditionally independent given
$V$.  Therefore, we can write
\[
\bX = \vec{h}(V)
\]
where $\vec{h}$ is a stochastic function independent of $V$ and $\bY$.
It follows from the stochastic data-processing inequality that
\[
\mathcal{I}(\bY; \bX) = \mathcal{I}(\bY; \vec{h}(V)) = \mathcal{I}(\bY; V) = \mathcal{I}(\bY; g(\bX)).
\]
Now consider an alternative encoding model $\hat{g}$.  From the stochastic data-processing inequality,
\[
\mathcal{I}(\bY; \bX) \geq \mathcal{I}(\bY; \hat{g}(\bX)).
\]
Therefore,
\[
\mathcal{I}(\bY; g(\bX)) \geq \mathcal{I}(\bY; \hat{g}(\bX))
\]
as needed.

For \emph{decoding models}, a similar argument can be carried out, but
this depends on the existence of a \emph{sufficient statistic} for the
conditional distribution of $\bY|\bX$.  Details of the argument can be
found in Nelken et. al. 2005: although their paper only deals with
mutual information, their argument generalizes to our setting as well.

\subsection{Composition of information coefficients}

There are several ways to \emph{transform} or \emph{compose} existing
information coefficients to form new ones.  

First, \emph{increasing monotonic transformations} of information
coefficients are information coefficients, as long as the
transformation maps zero to zero.  
%In fact, due to this fact it might
%be more reasonable to think of information coefficients as
%an \emph{order relation} rather than a real-valued function.

Secondly, positive linear combinations of information coefficients are
information coefficients.  If $\mathcal{I}_1$ and $\mathcal{I}_2$ are information coefficients, then
\[
\mathcal{I} = \alpha \mathcal{I}_1 + \beta \mathcal{I}_2
\]
is an information coefficient for $\alpha, \beta > 0$.

Thirdly, products of information coefficients are information coefficients:
\[
\mathcal{I} = \mathcal{I}_1 \mathcal{I}_2
\]
is an information coefficient if $\mathcal{I}_1,\ \mathcal{I}_2$ both are.

But all three of these transformations are special cases of the same
rule.  We say that a multivariate real-valued function $f(x_1,\hdots,
x_n)$ is \emph{increasing}
 if it is increasing in all of its elements.  Then, if $f$ is a
nonnegative and increasing real-valued function which takes the value
zero when any of its arguments are zero, and
$\mathcal{I}_1,\hdots, \mathcal{I}_n$ are all information
coefficients,
\[
\mathcal{I}(\bX; \bY) = f(\mathcal{I}(\bX, \bY),\hdots, \mathcal{I}(\bX, \bY))
\]
is an information coefficient.

All of these transformation properties follow from the fact that the
axioms of information are all essentially concerned
with \emph{ordering} properties of the information coefficient.
Furthermore, the application of \emph{model selection} only requires
ordering properties. In certain applications, it may be more
convenient to work with the equivalent \emph{information ordering},
rather than the information coefficient itself.

However, there is one application where the arithmetic structure of
the information coefficient matters, which is the quantification of
redundancy.

\subsection{Quantifying Redundancy}\label{sec:redundancy}

As we saw from Section \ref{sec:background_mi}, a recent new
application of mutual information is for quantifying redundancy, say
between neurons $Y_1$ and $Y_2$.  But what, exactly, does it mean for
two neurons to be \emph{redundant} or \emph{non-redundant}?  What is
the scientific interpretation?

We have been unable to find an detailed explanation in the literature
of how to interpret proposed measures of redundancy.  However, we can
guess from the quantitative definitions what would be a reasonable
definition of redundancy from first principles.

It is a common intuition [][][] that neurons $Y_1$ and $Y_2$
are \emph{redundant} with respect to the stimulus $X$ if
\[
\text{I}(X; (Y_1, Y_2)) < \text{I}(X; Y_1) + \text{I}(X; Y_2).
\]
If, conversely
\[
\text{I}(X; (Y_1, Y_2)) > \text{I}(X; Y_1) + \text{I}(X; Y_2).
\]
then we say that the neurons $Y_1$ and $Y_2$ have \emph{synergy}
(negative redundancy.)

Hence, one proposed measure of redundancy between two neurons is
\begin{equation}\label{eq:redundancy}
\text{R}(Y_1;Y_2|X) = \text{I}(X; Y_1) + \text{I}(X; Y_2) - \text{I}(X; (Y_1, Y_2)).
\end{equation}
There are also extensions of the definition for more than two neurons,
and variations which employ normalization so that the redundancy is
bounded.

However, if we only rely on our three axioms of information, which we
saw are preserved under a large class of monotonic transformations,
then there is no hope of formulating a generalized measure of
redundancy.  After all, if we substitute different information
coefficients in \ref{eq:redundancy}, we could obtain either positive
or negative values depending on how we choose to scale the information
coefficient.

%Therefore, what would be ideal is if we could find an additional axiom
%to impose a standard on what pairs of neurons are to be considered
%``redundant'' or ``synergistic'' independent of any particular
%information coefficient (e.g. mutual information.)  Furthermore,
%respecting the established tradition, it should be the case that
%mutual information is compatible with this ``axiom of redundancy.''
Our proposal for addressing this issue is to formulate a new ``axiom
of redundancy'', that sets a standard for how general measures of
redundancy should work.  The axiom is as follows.

\begin{itemize}
\item \emph{Axiom of redundancy.}  
Given random vectors $\bX, \bY_1, \bY_2$, we say that $\bY_1$ and
$\bY_2$ are \emph{nonredundant} with respect to $\bX$ if and only if both (i) $\bY_1$ and $\bY_2$ are unconditionally independent:
\[\bY_1 \perp \bY_2\]
and (ii) $\bY_1$ and $\bY_2$ are conditionally independent given $\bX_1$
\[\bY_1 \perp \bY_2 | \bX.\]
We say that an information metric $\mathcal{I}$ is \emph{compatible} with the axiom of redundancy if and only if
\[
\text{R}_{\mathcal{I}}(\bY_1;\bY_2|X) = \mathcal{I}(\bX; (\bY_1, \bY_2)) - \mathcal{I}(\bX; \bY_1) + \mathcal{I}(\bX; \bY_2) = 0.
\]
for all triples $(\bX, \bY_1, \bY_2)$ in which $\bY_1$ and $\bY_2$ are
nonredundant with respect to $\bX$.
\end{itemize}

We obtained this axiom of redundancy by reasoning backwards: what
property of $(X, Y_1, Y_2)$ leads to the identity
\[
\text{I}(X; (Y_1, Y_2)) = \text{I}(X; Y_1) + \text{I}(X; Y_2)?
\]
Both unconditional independence and conditional independence of $Y_1,
Y_2$ are needed, as can be seen via the following counterexamples.
\begin{itemize}
\item 
Let $X = Y_1 + Y_2$ for $Y_1, Y_2$ i.i.d. $N(0,1)$: then
$\text{R}_{\text{I}}(Y_1,Y_2|X) = -\infty$.  Hence unconditional
independence is not sufficient.
\item 
Let $X=Y_1=Y_2$ for $X \sim \text{Unif}\{0, 1\}$.  Then $Y_1$ and
$Y_2$ are conditionally independent, but
$\text{R}_{\text{I}}(Y_1,Y_2|X) = \log(2)$.  Hence conditional
independence is not sufficient.
\end{itemize}
However, with both conditional independence and independence, we have the factorizations
\[
p(y_1,y_2) = p(y_1) p(y_2)
\]
and
\[
p(x, y_1, y_2) = p(x) p(y_1|x) p(y_2|x).
\]
Hence,
\begin{align*}
\text{I}(X; (Y_1, Y_2)) =& \int p(x, y_1, y_2) \log \frac{p(x, y_1, y_2)}{p(x) p(y_1,y_2)} dx dy_1 dy_2
\\=& \int p(x, y_1, y_2) \log \frac{p(x) p(y_1|x) p(y_2|x)}{p(x) p(y_1) p(y_2)} dx dy_1 dy_2
\\=& \int p(x, y_1, y_2) \left[ \log \frac{p(y_1|x)}{p(y_1)} + \log \frac{p(y_2|x)}{p(y_2)} \right] dx dy_1 dy_2
\\=& \int p(x, y_1, y_2) \left[ \log \frac{p(x, y_1)}{p(x)p(y_1)} + \log \frac{p(x,y_2)}{p(x)p(y_2)} \right] dx dy_1 dy_2
\\=& \int p(x, y_1) \log \frac{p(x, y_1)}{p(x)p(y_1)} dx dy_1 \int p(y_2|x) dy_2 
\\&+ \int p(x, y_2) \log \frac{p(x,y_2)}{p(x)p(y_2)} dx dy_2 \int p(y_1|x) dy_1
\\=& \ \text{I}(X; Y_1) + \text{I}(X; Y_2).
\end{align*}

But do there exist other information coefficients which are compatible
with the axiom of redundancy?  We have not found any so far.  In this
paper, we will attempt to look for information coefficients which
are \emph{approximately} compatible, in that
$\text{R}_{\mathcal{I}} \approx 0$ for nonredundant $\bY_1, \bY_2$.

\subsection{General characterization of supervised learning}

As the central theme of the paper is the link between supervised
learning and information, we now give an generalized definition of
supervised learning to complement our axiomatic characterization of
information, in preparation for the synthesis of the two in
section \ref{sec:gen_class_sub}.

A \emph{supervised learning problem} is given by a \emph{prediction
task}, and a \emph{sampling scheme} for training and test data.

Let us first define the notion of a sampling scheme.  Given a joint
density $p(\bx, \by)$ with respect to $\mu_x \times \mu_y$, define the marginal densities $p(\bx)$,
$p(\by)$ and conditional densities $p(\by|\bx)$, $p(\bx|\by)$.
A \emph{sample} is defined as a vector of observations
$(\bX_1,\hdots, \bX_{n_X}, \bY_1,\hdots, \bY_{n_Y}).$ The joint
distribution of the sample is given by a colored graph $G$,
called the \emph{sampling scheme}.  The graph satisfies the following properties:
\begin{itemize}
\item $G$ is bipartite with vertex sets $V_X$ and $V_Y$, directed, and acyclic.
\item $V_X$ has $n_X$ vertices, labeled $\bX_1,\hdots, \bX_{n_X}$.  All vertices in $V_X$ are colored red.
\item $V_Y$ has $n_Y$ vertices, labeled $\bY_1,\hdots, \bY_{n_Y}$.  All vertices in $V_X$ are colored blue.
\end{itemize}
Let $R_X = \{i\}$ be the set of indices corresponding to vertices in
$V_X$ without parents, and define $R_Y$ analagously.  Let $E_{XY}
= \{(i, j)\}$ denote the set of directed edges from $V_X$ to $V_Y$,
and define $E_{YX}$ analogously.  Then, the distribution of the sample
is given by the density
\[
p_{samp}(\bx_1,\hdots, \bx_{n_X}, \by_1,\hdots, \by_{n_Y}) 
= \prod_{i \in V_X} p(\bx_i) \prod_{j \in V_Y} p(\by_j) \prod_{(a, b) \in E_{XY}} p(\by_i|\bx_i) \prod_{(c,d) \in E_{YX}} p(\bx_i|\by_i).
\]
with respect to the product measure $\mu_x^{n_X} \times \mu_y^{n_Y}.$
And in practice, one can sample from $p_{samp}$ as follows:
\begin{enumerate}
\item For all $i \in R_X$, sample $\bx_i$ from $p(\bx_i)d\mu_x(\bx_i)$.
\item For all $i \in R_Y$, sample $\by_i$ from $p(\by_i)d\mu_y(\by_i)$.
\item Iterate the following until all components have been sampled:
\begin{itemize}
\item Define $S_X$ ($S_Y$) to be the set of all vertices in $V_X$ ($V_Y$) that have already been sampled.
\item For all edges $(i, j)$ in $E_{XY}$ such that $i \in S_X$ and $j \notin S_Y$, sample $\by_j$ from $p(\by_j|\bx_i)d\mu_x(\bx_i)$.
\item For all edges $(i, j)$ in $E_{YX}$ such that $i \in S_Y$ and $j \notin S_X$, sample $\bx_j$ from $p(\bx_j|\by_i)d\mu_y(\by_i)$.
\end{itemize}
\end{enumerate}

Two common examples of sampling schemes are as follows.
\begin{itemize}
\item 
\emph{Pair-sampling.}
The sample consists of i.i.d. pairs $(\bX_i, \bY_i)$ drawn from the joint
distribution.  The sampling scheme $G$ is a graph where the
only edges are from $\bX_i$ to $\bY_i$ for $i = 1,\hdots, n$.
\item
\emph{Repeated measures.}
From each $\bX_i$, one draws $r$ conditionally independent responses
$\bY_i^1,\hdots, \bY_i^r$.  The sampling scheme $G$ is a graph where
the only edges are from $\bX_i$ to $\bY_i^j$ for $i = 1,\hdots, n$, $j
= 1,\hdots, r$.
\end{itemize}

The \emph{prediction task} involves a \emph{prediction sampling
scheme}, given by a graph $G_{task}$, a pair of functions
$\vec{u}, \vec{v}$, and a \emph{cost function} $C$, and
an \emph{auxillary} random variable $W$ which is independent of the
sample.  Let $p_{task}$ denote the density specified by $G_{task}$,
and let $S_{task}$ denote the support of the $p_{task}$.
The \emph{predictors} of the prediction task, $\bU$ is given by
$\vec{u}$, a function of components of $\bS_{task}$ and possibly of
$W$, and the \emph{prediction target} $\bV$, is given by $\vec{v}$,
which is a function of components of $\bS_{task}$ and possibly of $W$.
The cost function $C(\bv, \hat{\bv})$ specifies the \emph{cost}
incurred when the true target is $\bv$ and the prediction is
$\hat{\bv}$.

A \emph{predictive model} is a function which predicts the target
$\bV$ as a function of the predictors $\bU$.  The prediction task and
joint density $p(\bx,\by)$ jointly define a \emph{risk function} for
any predictive model $\vec{f}$:
\[
\text{Risk}(\vec{f}) = \E[C(\bV, \hat{f}(\bV))].
\]
The \emph{Bayes risk} is the infimal risk over all predictive models:
\[
\text{R}_{Bayes}[p(\bx,\by)] = \inf_{\vec{f}} \text{Risk}(\vec{f}).
\]
In anticipation of the next section, we write $\text{R}_{Bayes}$ as a
functional of the joint density.

The training data is given by a sampling scheme $G_{train}$, and the
testing data is given by a sampling scheme $G_{test}$.
A \emph{learning algorithm} outputs a predictive model $\vec{f}$ given
the training sample $\bS_{train}$ as input.

The analysis culminates in an unbiased estimate of the risk of the
learned model, $\text{Risk}(\vec{f})$.  Define a \emph{rooted
subgraph} $H$ of a graph $G$ as a subgraph where all parentless
vertices in $H$ are also parentless in $G$.  Then, let $G_1,\hdots,
G_m$ be the rooted subgraphs of $G_{test}$ that are color-isomorphic
to $G_{task}$ (i.e. the graph isomorphism preserves vertex color.)
Let $\bS_1,\hdots, \bS_m$ be the corresponding samples obtained from
the subgraphs $G_1,\hdots, G_m$, and let $(\bu_i, \bv_i) =
(\vec{u}(\bS_i, W), \vec{v}(\bS_i, W))$ for $i = 1,\hdots, m$.  An unbiased estimate of $\text{Risk}(\vec{f})$ is given by
\[
\widehat{\text{Risk}(\vec{f})} = \frac{1}{m}\sum_{i=1}^m \E[C(\vec{v}_i, \vec{f}(\vec{u}_i))|\bS_{test}]
\]
where the expectation is taken over the distribution of $W$.

Now we revisit the examples of supervised learning given in
section \ref{sec:background_sl}.
\begin{itemize}
\item \emph{Regression.}  
The task, training and test sampling schemes are all pair-sampling,
with $n= 1$ for the task sampling scheme.
\[
\vec{u}(\bx) = \bx
\]
and
\[
\vec{v}(\by) = \by.
\]
The cost function for squared-error loss is
\[
C(\by, \hat{\by}) = ||\by - \hat{\by}||^2.
\]
\item \emph{Classification.}  
Here, $\bX$ is the class label and $\bY$ is the feature vector. The
sampling schemes are the same as in regression.  The predictor and
response are flipped:
\[
\vec{u}(\bx) = \bx,
\]
\[
\vec{v}(\by) = \by.
\]
And since $\bX$ is discrete, it becomes reasonable to adopt the zero-one loss
\[
C(\bx,  \hat{\bx}) = I\{\bx \neq \hat{\bx}\}.
\]
\item \emph{Identification}.  
Let $k$ be the number of classes in the test set. The sampling schemes
are pair-sampling, with $k$ pairs for the task sampling scheme.  Let
$W$ be an independent uniform variate drawn from $\{1,\hdots, k\}$.
We have
\[
\vec{u}((\bx_1,\by_1),\hdots, (\bx_k, \by_k), W) = (\bx_1,\hdots, \bx_k, \by_W).
\]
\[
\vec{v}(W) = W.
\]
The cost function $C$ is the zero-one loss.
\end{itemize}

\subsection{A general class of information coefficients}\label{sec:gen_class_sub}

Now that we have the axioms of information and a general definition of
supervised learning, we can deliver the central concept in the paper:
a general class of information coefficients based on supervised
learning.

First, recall the intuitive characterization of information as
``reduction of uncertainty.''  If we measure uncertainty using Shannon
entropy $\text{H}(\bX)$, then the resulting information coefficient is
mutual information $\text{I}(\bX; \bY)$.  Therefore, to generalize
this definition, we use a \emph{general prediction task} to define
uncertainty.

Let $\bS$ be a sample drawn from sampling scheme $p_{task}$,
$C: \mathcal{V} \times \mathcal{V} \to \mathbb{R}$ be a cost function,
and let $\vec{v}$ be a function, with $\bV = \vec{v}(\bS)$.  Define
the \emph{unconditional uncertainty} of the task to be
\[
\inf_{\hat{\bv}} \E[C(\bV, \hat{\bv})].
\]
Note that this is the Bayes risk for the prediction problem in the
case that $\vec{u}$ is a trivial (constant) function.
How much is the uncertainty reduced upon seeing $\bU$?
The \emph{conditional uncertainty} is
\[
\inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))].
\]
Therefore, the resulting definition for a putative information coefficient is
\[
\mathcal{I}(\bX; \bY) = \inf_{\hat{\bv}} \E[C(\bV, \hat{\bv})] - \inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))].
\]
However, some additional conditions on the prediction task are needed
to ensure that $\mathcal{I}$ is indeed an information coefficient.
Firstly, it is clear that $\mathcal{I}$ is a functional of
$p(\bx, \by)$, and that it must be non-negative.  However, we need to
find conditions which ensure that $\mathcal{I}(\bX; \bY) = 0$ in the
case of independence, and to ensure the stochastic data-processing
inequality.

We say that \emph{V,U-orthogonality} holds for $(\vec{u}, \vec{v})$ if
$\bX \perp \bY$ implies $\vec{u}(\bS) \perp \vec{v}(\bS)$ for all
marginal distributions of $\bX$ and $\bY$.  
%A sufficient condition
%for V,U-orthogonality is that $\vec{u}$ and $\vec{v}$ have
%disjoint arguments.  
This is clearly met in the regression and classification case.  The
property holds in the identification case as well, since if
$\bX \perp \bY$ then $\by_W$ is independent of $W$.

We say that a function $\vec{f}(\bx_1,\hdots, \bx_p)$ is \emph{faithful} with respect to the arguments $\{\bx_1,\hdots, \bx_j\}$
if for any function $\vec{h}(\bx)$, one can find a function $\vec{g}$ such that
\[
\vec{f}(\vec{h}(\bx_1),\hdots, h(\bx_j), \bx_{j+1}, \hdots, \bx_p) = \vec{g}(\vec{f}(\bx_1,\hdots, \bx_p)).
\]
A sufficient condition for the data-processing inequality is that:
\begin{itemize}
\item \emph{V-compatibility:} $\vec{v}$ not have any arguments in $V_Y$, and also
\item \emph{U-compatibility:} $\vec{u}$ is faithful with respect to its arguments in $V_Y$.
\end{itemize}
To show the
data-processing inequality, consider the joint distribution of
$(\bS, \tilde{bS})$, where $\bS$ is drawn from the task sampling
scheme with the joint distribution $P$ of $(\bX, \bY)$, and
$\tilde{\bS}$ is obtained by replacing $\bY_i$ in $\bS$ with
$\vec{h}(\bY_i)$.  Letting $W$ be an auxillary variable, obtain $\bV$
by applying $\vec{v}$ to $(\bS, W)$, obtain $\tilde{bV}$ by applying
$\vec{v}$ to $(\tilde{\bS}, W)$, and define $\bU$ and $\tilde{\bU}$
analogously.  We have
\[
\mathcal{I}(\bX; \bY) = \inf_{\hat{\bv}} \E[C(\bV, \hat{\bv})] - \inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))],
\]
\[
\mathcal{I}(\bX; \vec{h}(\bY)) = \inf_{\hat{\bv}} \E[C(\tilde{\bV}, \hat{\bv})] - \inf_{\vec{f}} \E[C(\tilde{\bV}, \vec{f}(\tilde{\bU}))].
\]
where all expectations are with respect to $P$.
Because $\vec{v}$ has no arguments in $V_Y$, we have
\[
\bV = \tilde{\bV}.
\]
Therefore,
\[
\mathcal{I}(\bX; \bY) - \mathcal{I}(\bX; \vec{h}(\bY)) =  \inf_{\vec{f}} \E[C(\bV, \vec{f}(\tilde{\bU}))] - \inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))].
\]
However, due to faithfulness, $\tilde{\bU}$ is a function of $\bU$.  Therefore, for any $\vec{f}$, we can find $\vec{g}$ such that
\[
\vec{f}(\tilde{\bU})) = \vec{g}(\bU).
\]
This implies that
\[
\inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))] \leq \inf_{\vec{f}} \E[C(\bV, \vec{f}(\tilde{\bU}))].
\]
and hence,
\[
\mathcal{I}(\bX; \bY) \geq \mathcal{I}(\bX; \vec{h}(\bY)).
\]

We summarize our findings in the following theorem.
\begin{theorem}
Let $(G_{task}, \vec{v}, \vec{u}, C)$ define a prediction task.  Then,
if $(\vec{v}, \vec{u})$ satisfy V,U-orthogonality, V-compatibility and
U-compatibility, then
\[
\mathcal{I}(\bX; \bY) = \inf_{\hat{\bv}} \E[C(\bV, \hat{\bv})] - \inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))]
\]
defines an information coefficient, where $(\bV, \bU)$ are the targets
and predictions obtained from a task sample drawn using the joint
distribution of $(\bX, \bY)$.
\end{theorem}
$\Box$.

We call such information coefficients \emph{prediction-based}
information coefficients.

Both classification and identification satisfy the three sufficient
conditions of V,U-orthogonality and V/U-compatibility.  Thus,
classification and identification yield information coefficients of
the form $\mathcal{I}(\bX; \bY)$.  Regression satisfies
V,U-orthogonality but violates V/U-compatibility. But, if we switch
the place of $\bX$ and $\bY$, then V/U compatibility holds.
Therefore, the Bayes risk of regression can be used to obtain an
information coefficient of the form $\mathcal{I}(\bY; \bX)$.

We will take a closer look at some of these new information
coefficients in section \ref{sec:estimability}.  
However, first we show that \emph{mutual information} is also a member of the same class.

\subsection{Mutual information as a prediction-based information coefficient}

What is prediction task yields mutual information as the associated
information coefficient?  It suffices to find a prediction task where
the Shannon entropy is the Bayes risk.  There is such a prediction
task: \emph{distribution estimation under logarithmic loss.}

Take a discrete random variable $X$ with probability mass function $p(x)$.
The prediction task is to estimate the mass function $p$.
The loss is evaluated by drawing a new observation $X \sim p(x)$:
\[
L(X, \hat{p}) = -\log \hat{p}(X).
\]
Defining the risk as the expected loss, then it is a well-known result
in information theory and statistics that taking $\hat{p} = p$
minimizes the risk, and the value of the minimal risk is the discrete
entropy $\text{H}_{discrete}(X)$. And, if one is allowed to observe
$Y$ before making the prediction, then the risk becomes
$\text{H}_{discrete}(X|Y)$.  So the distribution estimation problem is
exactly what we need.

Using the language of our framework for supervised learning,
\begin{itemize}
\item The sampling scheme for the task is to draw a single pair $(X, Y)$.
\item $\vec{u}(y) = y$.
\item $\vec{v}(x) = \delta_x$, a vector with the value 1 at $x$ and 0 in other entries.
\item The cost function is
\[
C(\bv, \hat{\bv}) = \sum_{x \in \mathcal{X}} \bv(x) \log \hat{\bv}(x).
\]
\end{itemize}
The information coefficient for the prediction task is $\mathcal{I}(\bX; \bY) = \text{I}(\bX; \bY)$.
Therefore, mutual information is a prediction-based information coefficient.

However, there is yet another characterization of mutual information
in terms of prediction-based information coefficients, due to
consequences of the noisy channel theorem.  Consider the problem
of \emph{decoding from a random codebook}.  An information channel is
given by $p(y|x)$ and a source distribution is given by $p(x)$.
First, one forms a codebook consisting of $M$ random codewords
$\{\bX_1,\hdots, \bX_M\}$, where each codeword is a length-$M$ vector
\[
\bX_i = (X_{(i-1)k+1},\hdots, X_{(i-1)k}).
\]
The codebook is shared with both the sender and the reciever.  Next,
the sender chooses a codeword at random to send over the channel,
$\bX_W$.  The reciever observes the message
\[
\bY = (Y_{(W-1)k+1}, \hdots, Y_{(W-1)k}).
\]
Then, the decoding task is for the reciever to guess the index of the
message, $W$, based on $\bY$.  Let $R_{Bayes}$ be the minimal
probability of decoding error over all decoding schemes.  The noisy
channel theorem tells us that if we take $k \to \infty$ and $M
= \exp[k\iota]$, then
\[
\lim_{k \to \infty} R_{Bayes} = \begin{cases}
0 & \text{ if }\iota \leq \text{I}(X; Y),\\
1 & \text{ otherwise.}
\end{cases}
\]
Now, observe that decoding from a random codebook is an example of a
prediction task.  In our framework, we express it as
\begin{itemize}
\item \emph{$k, M$-decoding for a random codebook}.  $X, Y$ are discrete random variables. The sampling schemes are pair-sampling, with $Mk$ pairs for the task sampling scheme.
Let $W$ be an
independent uniform variate drawn from $\{1,\hdots, M\}$.  We have
\[
\vec{u}((x_1,y_1),\hdots, (x_{Mk}, y_{Mk}), W) = (x_1,\hdots, x_{Mk}, y_{(W-1)k + 1},\hdots, y_{(W-1)k}).
\]
\[
\vec{v}(W) = W.
\]
The cost function $C$ is the zero-one loss.
\end{itemize}
Let $\mathcal{I}_{k, M}(\bX; \bY)$ denote the associated information coefficient.  It follows that
\[
\text{I}(\bX; \bY) = \lim_{k \to \infty} \int_0^\infty \mathcal{I}_{k, \lfloor \exp[kx] \rfloor}(\bX; \bY) dx.
\]
That is, $\text{I}(\bX; \bY)$ can be expressed as a limit of a linear
combination of information coefficients.

These two characterization of mutual information as information
coefficients have implications in estimation, as we will see in the
following section.

\section{Statistical inference}\label{sec:inference}

\subsection{Estimability}\label{sec:estimability}

Having defined prediction-based information coefficients, what are the
practical consequences?  Consider the case where the joint
distribution of $\bX$ and $\bY$ are unknown.  By applying a learning
algorithm to training data and evaluating on test data, one can obtain
an unbiased estimate of $\text{Risk}(\vec{f})$.  But in turn,
$\text{Risk}(\vec{f})$ is an upper bound for $R_{Bayes}$.  
Now define
\[
R_0 = \inf_{\hat{\bv}} \E[C(\bV, \hat{\bv})].
\]
Then,
\[
\mathcal{I}(\bX; \bY) =  R_0 - R_{Bayes} \geq R_0 - \text{Risk}(\vec{f}).
\]
Therefore, as long as we have a good estimate of $R_0$, it follows
that we can obtain an \emph{underestimate} of $\mathcal{I}(\bX; \bY)$
from supervised learning,
\[
\underline{\mathcal{I}}(\bX; \bY) = \hat{R}_0 - \widehat{\text{Risk}}(\vec{f}).
\]
This underestimate can be converted into a $(1-\alpha)$ lower
confidence bound supposing that we have bounds on the estimation error
of $\hat{R}_0$ and $\widehat{\text{Risk}}(\vec{f})$.

How about upper bounds of $\mathcal{I}$?  This would require
estimating $R_{Bayes}$ directly, rather than relying on supervised
learning.  In the case of $(\bX,\bY)$ discrete, one can apply general
tools for nonparametric estimation of functionals of discrete
distributions (Yanjun et al.) to estimate $R_{Bayes}$.  In the general
case, the outlook is less positive.  In the case of classification,
regression, and distributional estimation, it is known that nontrivial
upper bounds for $R_{Bayes}$ are not possible without additional
assumptions [cite].

Now, let us consider the classification, regresssion, and
identification information coefficients in greater detail.

\subsection{Prediction-based coefficients}

\subsection{Inference of mutual information}

\subsection{Experimental setups}



\section{Lower confidence bounds for average Bayes accuracy}









\section{Connections}\label{sec:connections}

\section{Applications}\label{sec:applications}

\section{Discussion}\label{sec:discussion}

\end{document}












The word ``mutual'' is appropriate given the symmetry of
$I(\bX; \bY)$.  However, at the intuitive level it does not
seem \emph{a priori} necessary that the measure of information be
symmetric; indeed, generalizations of mutual information (Verd\'{u}
2015) are asymmetric.  Having a quantitative definition of
information, such as Shannon's $I(\bX;\bY)$, opens the possibility of
computing, estimating or inferring the quantity for $(\bX, \bY)$ pairs
of interest in the natural world.  Classical neuroscience experiments
estimated the mutual information between the reaching angle of a
monkey's arm, $X$, and the average firing rate of particular motor
neurons, $Y$.  Once estimates are available for individual pairs,
cross-pair comparisons are possible.  Scientists can check if motor
neurons in one particular region of the brain are more ``informative''
(on an individual level) than motor neurons in another brain region.
Even more interestingly, one can take an ensemble of neurons as $Y$.
Without collecting any additional data, scientists can compare the
information of individual neurons $Y_1$ and $Y_2$, and compare each to
the information about $X$ carried by the ensemble $(Y_1,Y_2)$.
Comparisons between ensembles motivate thinking about a ``calculus of
information.''  If
\[
I(X; Y_1) + I(X; Y_2) > I(X; (Y_1, Y_2)),
\]
then we say that $Y_1$ and $Y_2$ carry \emph{redundant} information.
If, on the other hand,
\[
I(X; Y_1) + I(X; Y_2) < I(X; (Y_1, Y_2)),
\]
one can say that $Y_1$ and $Y_2$ have \emph{synergy}--the information
carried by the whole is greater than the sum of the information of
each part.









However, the dominance of mutual information in neuroscience started
to wane as new experimental approaches shifted the focus from pairwise
comparisons to questions of \emph{population coding}.  The extreme
reductionist approach fails to account for the complex ways in which
neurons cooperatively encode complex information, and to get a richer
picture of neural dynamics, it becomes necessary to consider
multivariate measures of dependence.  Also, the adoption of
``high-throughput'' recording technologies such as EEG and fMRI
naturally led in the direction of considering the system-level
dynamics of the whole brain.  More importantly, the technology for
studying the relationships between brain structure, function, and
behavior have enabled a wider population of investigators
(psychologists and neurologists) to quantitatively examine brain
activity, but with motivations which tend to be more holistic and
instrumental (e.g. finding neural correlates of mental disorders) when
compared to the reductionist orientation of classical neuroscience.
For both reasons, the demand for multivariate statistical techniques
in neuroscience has increased dramatically in recent years. While the
theoretical properties of mutual information extend gracefully to the
multivariate setting, the difficulty of estimating mutual information
increases exponentially in the dimension in the absence of strong
modeling assumptions [cite].  Partially for this reason, alternative
measures of multivariate dependence started to enjoy increasing usage
in studies of population coding or in systems-level investigations of
the brain: these include measures of linear dependence (canonical
correlation and multivariate $R^2$) and Fisher information.  However,
while both correlation-based statistics and Fisher information may be
easier to estimate than mutual information in high-dimensional
settings, they are both less flexible in terms of capturing nonlinear
relationships.  Correlation-based statistics can only capture linear
dependence, and Fisher information requires the assumption of a
parametric model.











The concept of ``information'' plays a key role in areas as diverse as
game theory, biology, neuroscience, and human engineering.  A random
quantity $\bY$ carries information about $\bX$ if observing $\bY$ reduces
our uncertainty about $\bX$.  In game theory, $\bY$ could be the
opponent's bet, which reveals information about $\bX$, the opponent's
hand.  In neuroscience, $\bY$ is brain activity which correlates to
visual stimulus, $\bX$.  In communications, $\bX$ is a plaintext message
which is coded and transmitted as $\bY$, a series of recieved bits.

