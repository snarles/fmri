\title{Prediction, information, and inference: with application to neuroimaging}
\author{Charles Zheng and Yuval Benjamini}
\date{\today}

\documentclass[12pt]{article} 

% packages with special commands
\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}

\definecolor{grey}{rgb}{0.5,0.5,0.5}

\begin{document}
\maketitle

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\Cor}{\text{Cor}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bV}{\boldsymbol{V}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]



\begin{abstract}
Neuroscientists have a variety of tools for quantifying multivariate
dependence: mutual information, linear correlation-based statistics,
Fisher information, and more recently, measures of performance on
supervised learning tasks such as classification.  We argue that both
mutual information and classification accuracy capture intuitive
properties of an ``information coefficient'' for a channel, and we proceed
to develop a general axiomatic characterization of information coefficients
for channels consisting of a pair of input and output random
variables.  Arguably, the key properties of an information coefficient are
that (i) it is a scalar measure of multivariate dependence, and (ii)
that it satisfies a \emph{stochastic data-processing inequality}: any
coefficient with such properties can be used for model selection.
%The key axioms of an information coefficient are that (i) it is
%a scalar functional of the joint distribution of the input-output
%pair, (ii) it is zero for independent variables, and positive for
%dependent variables, (iii) satisfies a
%generalized \emph{data-processing} inequality, where transformations
%of the output can only preserve or reduce the information.  
We show how prediction tasks can be used to define a general class of
information coefficients which includes mutual information, as well as a
novel information coefficient, \emph{average Bayes accuracy}, which can be
considered an ``idealization'' of classification accuracy.
Furthermore, we consider the possibility of developing a general
theory of statistical inference for this class of information coefficients.
Concretely, we derive a lower confidence bound for average Bayes
accuracy as well as a novel lower confidence bound for mutual
information.
\end{abstract}


\section{Introduction}

Historically, neuroscience has largely taken a reductionist approach
to understanding the nervous system, proceeding by defining elements
and subelements of the nervous system (e.g. neurons), and
investigating relationship between two different elements, or the
response of an element to external stimulation: say, the response of a
neuron's average firing rate to skin temperature.  At one level of
abstraction, neuroscientists might seek to characterize the functional
relationship between elements, but at a higher level of abstraction,
it may be sufficient to report scalar measures of dependence.  Since
neural dynamics are generally both stochastic and nonlinear, it was a
natural choice for early neuroscientists to adopt
Shannon's \emph{mutual information} as a quantitative measure of
dependence.  But as new technologies enabled the recording of neural
data at larger scales and resolution, the traditional reductionist
goals of neuroscience were supplemented by increasingly ambitious
attempts within neuroscience to understand the dynamics of neural
ensembles, and by efforts originating within psychology and medicine
to link the structure and function of the entire human brain to
behavior or disease.  The larger scope of the data and the questions
being asked of the data created an increasing demand for multivariate
statistical methods for analyzing neural data of increasingly high
dimension.  Due to the complexity, variety, and practical difficulties
of multivariate statistical analysis of the brain, alternative
measures of multivariate dependence such as linear-based correlational
statistics, or Fisher information, started to gain traction.  For the
most part, alternative measures of dependence sacrifice flexibility
for a gain in practical convenience: linear-based statistics such as
canonical correlation or correlation coefficients fail to capture
nonlinear dependencies, and Fisher information requires strong
parametric assumptions.  Therefore, it was of considerable interest
when Haxby (2001) introduced the usage of \emph{supervised learning}
(classification tasks) for the purpose of quantifying stimulus
information in task fMRI scans.  Since then, an entire subfield of
neuroimaging, multivariate pattern analysis (MVPA) has been
established dedicated to quantifying multivariate information in the
brain, and both mutual information and classification accuracy are
used by practitioners within the field.  Judging from the language
used by the practioners themselves, it is intuitively clear to them
how classification accuracies can be used to quantify information in
brain scans.  However, a more thorough examination of the practice
raises many questions with regards to the use of classification
accuracy as a coefficient of information: this is one motivation for the
current work.  But taking a step back, it would seem valuable at this
historical juncture to examine the intuitive properties of
``information'' as a measure of multivariate dependence, and not only
consider whether classification accuracy can be considered or used to
derive a new information coefficient, but whether other such coefficients might
also exist, and whether a unified theory can be developed to account
for all of them.  This is the larger purpose of the current work, and
towards that end we not only propose a general class of information
coefficients which unifies both information-theoretic and
supervised-learning-based approaches, but with an eye toward practical
applications, we also examine the question of inferring these
quantities from data.  An initial result in this direction is the
derivation of nonparametric lower confidence bounds for average Bayes
accuracy (a novel information coefficient closely related to classification
accuracy,) and an inequality between average Bayes accuracy and mutual
information, which, combined with the preceding result, yields a novel
lower confidence bound for mutual information.

\subsection{Organization}

The rest of the paper is organized as follows.  Section \ref{sec:info}
plays the role of a ``background'' section that gives the basics of
mutual information and supervised learning as they are used in
neuroscience, as well as practical issues related to estimation.  In
section \ref{sec:gen_class}, we present an axiomatic characterization
of information coefficients, and introduce a general class of information
coefficients which satisfies our axioms.  We define a new information
coefficient belonging to this class, average Bayes accuracy, and we also
show how mutual information can be considered an ``extended member''
of the class.  In section \ref{sec:inference} we develop the basic
theory of what kinds of inferences about our information coefficients are
possible discuss the kinds of experimental designs and supervised
learning pipelines which are needed to enable such inference.
Concretely, we develop a lower confidence bound for average Bayes
accuracy. In section \ref{sec:connections} we outline a comparative
theory for different coefficients within our framework: how are the
different information coefficients related?  We discuss the calculus of
variations as a possible general technique for establishing
inequalities between different information coefficients, and in particular
we derive a ``randomized Fano's inequality'': a lower bound for mutual
information as a function of average Bayes accuracy.  Combined with
our lower confidence bound for average Bayes accuracy, this yields a
novel lower confidence bound for mutual information.  We provide a
practical data analysis example in section \ref{sec:applications}.  A
discussion section includes future directions and loose ends are
treated, and most of the technical proofs and lemmas are found in the
appendix.

\section{Background}\label{sec:info}

\subsection{Mutual information and its usage}\label{sec:background_mi}
%It is in the context of communication system that Shannon first
%proposed a quantification of the concept of ``the amount of
%information that $\bY$ carries about $\bX$'', in the form
%of \emph{mutual information}: 
While Shannon's theory of information was motivated by the problem of
designing communications system, the applicability of mutual
information was quickly recognized by neuroscientists.  Only four
years after Shannon's seminal paper in information theory (1948),
McKay and McCullough (1952) inaugurated the application of mutual
information to neuroscience.  If $\bX$ and $\bY$ have joint density
$p(\bx, \by)$ with respect to the product measure $\mu_x \times \mu_y$, then the mutual information is defined as
\[
\text{I}(\bX;\bY) = \int p(\bx, \by) \log \frac{p(\bx, \by)}{p(\bx)p(\by)}d\mu(\bx) d\mu(\by).
\]
where $p(\bx)$ and $p(\by)$ are the marginal densities with respect to
$\mu_x$ and $\mu_y$\footnote{Note that the mutual information is invariant with respect to change-of-measure.}.  Since then, mutual information has enjoyed a
celebrated position in both experimental and theoretical neuroscience.
Experimentally, mutual information has been used to detect strong
dependencies between stimulus features and features derived from
neural recordings, which can be used to draw conclusions about the
kinds of stimuli that a neural subsystem is designed to detect, or to
distinguish between signal and noise in the neural output.
Theoretically, the assumption that neural systems maximize mutual
information between salient features of the stimulus and neural output
has allowed scientists to predict neural codes from signal processing
models: for instance, the center-surround structure of human retinal
neurons matches theoretical constructions for the optimal filter based
on correlations found in natural images [cite].

The mutual information measures the information ``capacity'' of a
channel consisting of an input $\bX$ and an output $\bY$, and
satisfies a number of important properties.
\begin{enumerate}
\item The channel input $\bX$ and output $\bY$ can be random vectors of arbitrary dimension, and the mutual information remains a scalar functional of the joint distribution $P$ of $(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\text{I}(\bX; \bY) = 0$; otherwise, $\text{I}(\bX; \bY) > 0$.
\item The data-processing inequality: for any vector-valued function $\vec{f}$ of the output space,
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
\item Symmetry: $\text{I}(\bX; \bY) = \text{I}(\bY; \bX)$.
\item Independent additivity: if $(\bX_1,\bY_1)$ is independent of $(\bX_2, \bY_2)$, then
\[
\text{I}((\bX_1,\bY_1); (\bX_2, \bY_2)) = \text{I}(\bX_1; \bY_1) + \text{I}(\bX_2; \bY_2).
\]
\end{enumerate}
Three additional consequences result from the data-processing inequality:
\begin{itemize}
\item \emph{Stochastic data-processing inequality}  If $\vec{f}$ is a stochastic function independent of both $\bX$ and $\bY$, then
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
This can be shown as follows: any stochastic function $\vec{f}(\bY)$
can be expressed as a deterministic function $\vec{g}(\bY, W)$, where
$W$ is a random variable independent of $\bX$ and $\bY$.
By independent additivity,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)).
\]
Then, by the data-processing inequality,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)) \geq \text{I}(\bX; \vec{g}(\bY, W)) = \text{I}(\bX; \vec{f}(\bY)).
\]
\item \emph{Invariance under bijections.} If $\vec{f}$ has an inverse $\vec{f}^{-1}$, then 
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY) = \text{I}(\bX; \vec{f}^{-1}(\vec{f}(\bY))) \leq \text{I}(\bX; \vec{f}(\bY)),
\]
therefore, $\text{I}(\bX; \vec{f}(\bY)) = \text{I}(\bX; \bY)$.
\item \emph{Monotonicity with respect to inclusion of outputs.}  Suppose we have an output ensemble $(\bY_1,\bY_2)$.  Then the individual component $\bY_1$ can be obtained as a projection of the ensemble.  By the data-processing inequality, we therefore have
\[
\text{I}(\bX; \bY_1) \leq \text{I}(\bX; (\bY_1, \bY_2)).
\]
Intuitively, if we observe both $\bY_1$ and $\bY_2$, this can
only \emph{increase} the information we have about $\bX$ compared to
the case where we only observe $\bY_1$ by itself.
\end{itemize}
And it is the property of \emph{invariance under bijections},
inclusive of non-linear bijections, which qualifies mutual information
as a \emph{non-linear measure of dependence.}  Linear measures, such
as Pearson correlation, are not invariant under bijections.  

%As for
%the completeness of the five listed properties: as we know, Shannon's
%mutual information (up to arbitrary scaling factor) is the only
%functional proposed in the literature which satisfies all five
%properties.
Besides the formal definition, there are a number of well-known alternative
characterizations of mutual information in terms of other
information-theoretic quantities: the \emph{entropy} $\text{H}$:
\[
\text{H}_\mu(\bX) = -\int p(\bX) \log p(\bX) d\mu(\bX),
\]
and the \emph{conditional entropy}:
\[
\text{H}_\mu(\bX|\bY) = -\int p(\bY) d\mu_y(\bY) \int p(\bX|\bY) \log p(\bX|\bY) d\mu_x(\bX).
\]
Some care needs to be taken with entropy and conditional entropy since
they are not invariant with respect to change-of-measure: hence the
use of the subscript in the notation $\text{H}_\mu$.  In particular,
there is a difference between \emph{discrete entropy} (for counting
measure) and \emph{differential entropy} (for Lesbegue measure.)
Intutively, entropy measures an observer's uncertainty of the random
variable $\bX$, supposing the observer has no prior information other
than the distribution of $\bX$. Conditional entropy measures
the \emph{expected uncertainty} of $\bX$ supposing the observer
observes $\bY$.

However, regardless of the base measure, the following identities hold:
\[
\text{I}(\bX; \bY) = \text{H}_{\mu_x \times \mu_y}((\bX, \bY)) - \text{H}_{\mu_x}(\bX) - \text{H}_{\mu_y}(\bY).
\]
\begin{equation}\label{eq:ce_ident}
\text{I}(\bX; \bY) = \text{H}_\mu(\bY) - \text{H}_\mu(\bY|\bX).
\end{equation}
The second identity \eqref{eq:ce_ident} is noteworthy
as being practically important for estimation of mutual information.
Since the entropies in question only depend on the marginal and
conditional distributions of $\bY$, the problem of estimating
$\text{I}(\bX; \bY)$ can be reduced from a $\dim(\bX)
+ \dim(\bY)$-dimensional nonparametric estimation problem to a
$\dim(\bY)$-dimensional problem: hence this identity is a basis of
several methods of estimation used in neuroscience, such as Gastpar
(2014).

However, by symmetry, we also have the flipped identity
\begin{equation}\label{eq:ce_ident2}
\text{I}(\bX; \bY) = \text{H}_\mu(\bX) - \text{H}_\mu(\bX|\bY).
\end{equation}
In neuroscience studies, where $\bX$ is the controlled stimulus, and
$\bY$ is the neural activity, the two mirror pairs \eqref{eq:ce_ident}
and \eqref{eq:ce_ident2} have different interpretations.  Rather than
providing a basis for practical estimation, \eqref{eq:ce_ident2}
provides an \emph{interpretation} of the mutual information.  Loosely
speaking, $\text{H}_\mu(\bX)$ is the uncertainty of $\bX$ before
having observed $\bY$, and $\text{H}_\mu(\bX|\bY)$ is the uncertainty
of $\bX$ after having observed $\bY$, hence $\text{H}_\mu(\bX)
- \text{H}_\mu(\bX|\bY)$ is how much the observation of $\bY$
has \emph{reduced} the uncertainty of $\bX$.  Stated in words,
\[
\text{I}(\bX; \bY) = \text{average reduction of uncertainty about $\bX$ upon observing $\bY$}.
\]

We list these properties of mutual information in preparation for
section \ref{sec:axiom_info}, where we prepare a ``minimal'' set of
properties for an information coefficient, and consider how much of
the functionality of the mutual information would be preserved by an
alternative information coefficient satisfying only those minimal
properties.

But what, exactly, is the functionality of mutual information in
neuroscience?  How it is used in practice?  A nice summary of the
applications of mutual information is provided in the introduction of
Gastpar (2014).  Taking their list as a starting point, we briefly
overview the main use-cases of mutual information, and illustrate each
with a representative example.
%We explore this concept of ``information as reduction of uncertainty''
%much further when we propose our general class of information coefficients:
%there, we see that properties (i)-(iii) of the mutual information are
%consequences of this second characterization of mutual information.

%But how what is the practical import of these properties of mutual
%information?  Supposing we identify a number of properties as being
%essential for scientific purposes (and others as non-essential), this
%then suggests that alternative measures of information, also
%satisfying the same essential properties, could be just as effective
%for scientific work as mutual information.  And some might have
%additional advantages.

%Towards our goal of formulating a general theory of information
%coefficients, it is important to evaluate exactly \emph{how important} each
%of the listed properties.

\begin{itemize}
\item Example: Comparison of decoders in Nelken.  
Property (i) is important to enable model comparison.  Property (iii)
is needed because relationships may be nonlinear.
\item Example: Redundancy in population code of retina.  
Property (i)-(iii) and (v) are needed to obtain a meaningful measure
of redundancy.
\item In general, symmetry not important, but additivity is desirable 
for measures of redundancy.  Property (ii) can usually be enforced
since any measure needs to have a unique ``minimum'' value for the
case of independence.
\end{itemize}

\subsection{Supervised learning}\label{sec:background_sl}

\begin{itemize}
\item Supervised learning task is defined using a prediction task.
\item 1. A predictive model is learned using training data
\item 2. The performance of the model on the prediction task is estimated using independent test data
\item Classical examples of prediction tasks: regression and classification
\item Third example: identification
\item Definition of Bayes prediction model
\item General definition of supervised learning task
\item SL performance can be a scalar
\item SL can be used to test for independence
\item Bayes performance satisfies data-processing inequality
\item How SL is interpreted in MVPA as information
\item Section 3, we'll see how Bayes performance can be legitimately considered a measure of information
\end{itemize}

\subsection{Connections}

\begin{itemize}
\item Information theory and decoding. Fano's inequality
\item Quiroga's method
\item MVPA people use them interchangeably
\end{itemize}

\section{Information, uncertainty and prediction}\label{sec:gen_class}

In the previous section, we examined how mutual information and
supervised learning are used in neuroscience.  By analyzing the
use-cases of each method side-by-side, and by reviewing the known
connections between the two methods, we hoped to suggest the notion
that both mutual information and supervised learning are being
employed as means of quantifying the same underlying concept.  In this
section, we propose a reification of this underlying concept of
``information'', and propose the \emph{minimal} properties needed for
a functional to be considered an \emph{information coefficient.}  We
argue that these minimal properties are sufficient to support many of
the existing use-cases of mutual information and supervised learning
in neuroscience.

\subsection{Axiomatic characterization of information}\label{sec:axiom_info}

We claim that neuroscientists use both mutual information and
supervised learning to quantifying a common concept of
``information.''  Furthermore, we claim that neuroscientists largely
share a set of common intuitions about information.

\begin{itemize}
\item \emph{
Intuition 1: Information is a measure of dependence.}   If $\bX$ and $\bY$ are statistically independent, then
$\bX$ gives no information about $\bY$, and vice-versa.
\end{itemize}
This
intuition is employed when researchers test the null hypothesis of
chance accuracy for classification.  If the null is accepted, the
researcher concludes that there is no information in the predictors
about the response.

\begin{itemize}
\item \emph{
Intuition 2a: Monotonicity with respect to inclusion of outputs.} If $\bY_1$ and $\bY_2$ are ensembles of neurons (or
individual neurons), then the combined ensemble $(\bY_1, \bY_2)$ has equal or
more information about $\bX$ than either component by itself.
\end{itemize}

\begin{itemize}
\item \emph{
Intuition 2b: Noise adds no information.} If, in the previous example, $\bY_2$ is independent of
$\bX$, then $\bY_2$ adds no information to the ensemble.
\end{itemize}
The non-informativity of noise is vital for the purpose
of \emph{localizing} information within fMRI voxels (as is done in
searchlight analysis.)  Since noise voxels fail to improve
classification performance (and indeed, sometimes harm empirical
performance,) the optimal searchlight radius will concentrate on
clusters of signal voxels, and minimize the inclusion of noise voxels.

\begin{itemize}
\item \emph{Intuition 3:}
Information can be used as a basis of model selection.  Among multiple
encoding/decoding models, a more accurate model should tend to have
greater information relative to less accurate models.  
\end{itemize}
Compared to the first two, this third intuition is somewhat less
obvious, but nevertheless appears as an important use-case for mutual
information, as seen in the application of mutual information to
choose encoding models.

Given the first two intuitions, we find that an essential property of
information is that it can be said to `increase'--i.e., there exists
at least a partial ordering on information.  Furthermore, there should
exist a minimal element in this ordering, which is the information
between independent variables--that is, `no information.' However,
this is fully consonant with either information being quantified as a
scalar quantity, or as a positive-definite matrix.  Indeed, Fisher
information could be taken as an example of a matrix-valued
information coefficient.  However, the problem with matrix-valued
coefficients is that channels may be incomparable within the partial
ordering.  Thus, if we consider the third intuition an important
property of an information coefficient, then it should be scalar to
enable model comparison.

We find that all of the preceding intuitions follow from the following
axioms.

\noindent\textbf{Axioms of information}

Let $\mathcal{I}(\bX; \bY)$ denote an \emph{information coefficient}.
Then,
\begin{enumerate}
\item $\mathcal{I}(\bX; \bY)$ is a scalar functional of the joint distribution $P$ of $(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\mathcal{I}(\bX; \bY) = 0$; otherwise, $\mathcal{I}(\bX; \bY) \geq 0$.
\item 
The stochastic data-processing inequality.  If $\vec{f}$ is stochastic
vector-valued function of the output space indepedent of $(\bX, \bY)$,
then
\[
\mathcal{I}(\bX; \vec{f}(\bY)) \leq \mathcal{I}(\bX; \bY).
\]
\end{enumerate}

Since this minimal list of properties is a subset of the properties of
mutual information, it is clear that mutual information satisfies the
axioms.  However, classification accuracy does not satisfy the
axioms--we will see in section \ref{sec:gen_class_sub} how to define
a proper information coefficient based on supervised learning.

Now let us check that any information coefficient
$\mathcal{I}(\bX; \bY)$ necessarily satisfies the intuitions.
Intuition 1 is satisfied by property (ii).  We gave a general argument
in \ref{sec:background_mi} how Intuition 2a follows from the
data-processing inequality.  Intuition 2b follows from the stochastic
data-processing inequality, taking $\vec{f}(\bY_1) = (\bY_1, \bY_2)$.

Now, in order to justify intuition 3, we need to formalize the notion
of ``model selection.''  A complete discussion of model selection
falls outside the scope of the paper, so we limit the discussions
to the important special case of selecting an encoding model.

Let $\bX$ be some environmental stimulus, and
\[
\bY = \vec{f}(g(\bX))
\]
where $\vec{f}$ is a stochastic function independent of $\bX$, and $g$ is
an \emph{encoding function}, which is known to lie in some class of
functions $\mathcal{G}$.  The goal of model selection is estimate $g$.

Given multiple competing models $(\hat{S}_1, \hat{g}_1),\hdots,
(\hat{S}_k, \hat{g}_k)$, we obtain a lower confidence bound on the maximum score,
\[
M = \max_{i=1}^k \mathcal{I}(\bY; \hat{g}_i(\bX)).
\]
We can then test each individual model for the hypothesis
$H_i: \mathcal{I}(\bY; \hat{g}_i(\bX)) < M$ at level $\alpha/k$.  All of the models which
are rejected are considered `candidate models.'  While a common next
step is to select a single model from the set of candidates
(e.g. using a measure of complexity), this is not essential to our
discussion.  For now, we are only concerned that the model selection
procedure should at the very least, \emph{not reject}
the \emph{correct} model $g$ in the circumstance that $g$ is included
in the initial set of candidate models.  A necessary condition for
this criterion is that the correct model $g$ maximizes the
information:
\begin{equation}\label{eq:model_selection}
\mathcal{I}(\bY; g(\bX)) = \sup_{g \in \mathcal{G}} \mathcal{I}(\bY; g(\bX)).
\end{equation}

Indeed, the criterion \eqref{eq:model_selection} is a consequence of
the stochastic data-processing inequality.  First observe that letting
$V = g(\bX)$, we have $\bX$ and $\bY$ conditionally independent given
$V$.  Therefore, we can write
\[
\bX = \vec{h}(V)
\]
where $\vec{h}$ is a stochastic function independent of $V$ and $\bY$.
It follows from the stochastic data-processing inequality that
\[
\mathcal{I}(\bY; \bX) = \mathcal{I}(\bY; \vec{h}(V)) = \mathcal{I}(\bY; V) = \mathcal{I}(\bY; g(\bX)).
\]
Now consider an alternative encoding model $\hat{g}$.  From the stochastic data-processing inequality,
\[
\mathcal{I}(\bY; \bX) \geq \mathcal{I}(\bY; \hat{g}(\bX)).
\]
Therefore,
\[
\mathcal{I}(\bY; g(\bX)) \geq \mathcal{I}(\bY; \hat{g}(\bX))
\]
as needed.

\subsection{General characterization of supervised learning}

As the central theme of the paper is the link between supervised
learning and information, we now give an generalized definition of
supervised learning to complement our axiomatic characterization of
information, in preparation for the synthesis of the two in
section \ref{sec:gen_class_sub}.

A \emph{supervised learning problem} is given by a \emph{prediction
task}, and a \emph{sampling scheme} for training and test data.

Let us first define the notion of a sampling scheme.  Given a joint
density $p(\bx, \by)$ with respect to $\mu_x \times \mu_y$, define the marginal densities $p(\bx)$,
$p(\by)$ and conditional densities $p(\by|\bx)$, $p(\bx|\by)$.
A \emph{sample} is defined as a vector of observations
$(\bX_1,\hdots, \bX_{n_X}, \bY_1,\hdots, \bY_{n_Y}).$ The joint
distribution of the sample is given by a colored graph $G$,
called the \emph{sampling scheme}.  The graph satisfies the following properties:
\begin{itemize}
\item $G$ is bipartite with vertex sets $V_X$ and $V_Y$, directed, and acyclic.
\item $V_X$ has $n_X$ vertices, labeled $\bX_1,\hdots, \bX_{n_X}$.  All vertices in $V_X$ are colored red.
\item $V_Y$ has $n_Y$ vertices, labeled $\bY_1,\hdots, \bY_{n_Y}$.  All vertices in $V_X$ are colored blue.
\end{itemize}
Let $R_X = \{i\}$ be the set of indices corresponding to vertices in
$V_X$ without parents, and define $R_Y$ analagously.  Let $E_{XY}
= \{(i, j)\}$ denote the set of directed edges from $V_X$ to $V_Y$,
and define $E_{YX}$ analogously.  Then, the distribution of the sample
is given by the density
\[
p_{samp}(\bx_1,\hdots, \bx_{n_X}, \by_1,\hdots, \by_{n_Y}) 
= \prod_{i \in V_X} p(\bx_i) \prod_{j \in V_Y} p(\by_j) \prod_{(a, b) \in E_{XY}} p(\by_i|\bx_i) \prod_{(c,d) \in E_{YX}} p(\bx_i|\by_i).
\]
with respect to the product measure $\mu_x^{n_X} \times \mu_y^{n_Y}.$
And in practice, one can sample from $p_{samp}$ as follows:
\begin{enumerate}
\item For all $i \in R_X$, sample $\bx_i$ from $p(\bx_i)d\mu_x(\bx_i)$.
\item For all $i \in R_Y$, sample $\by_i$ from $p(\by_i)d\mu_y(\by_i)$.
\item Iterate the following until all components have been sampled:
\begin{itemize}
\item Define $S_X$ ($S_Y$) to be the set of all vertices in $V_X$ ($V_Y$) that have already been sampled.
\item For all edges $(i, j)$ in $E_{XY}$ such that $i \in S_X$ and $j \notin S_Y$, sample $\by_j$ from $p(\by_j|\bx_i)d\mu_x(\bx_i)$.
\item For all edges $(i, j)$ in $E_{YX}$ such that $i \in S_Y$ and $j \notin S_X$, sample $\bx_j$ from $p(\bx_j|\by_i)d\mu_y(\by_i)$.
\end{itemize}
\end{enumerate}

Two common examples of sampling schemes are as follows.
\begin{itemize}
\item 
\emph{Pair-sampling.}
The sample consists of i.i.d. pairs $(\bX_i, \bY_i)$ drawn from the joint
distribution.  The sampling scheme $G$ is a graph where the
only edges are from $\bX_i$ to $\bY_i$ for $i = 1,\hdots, n$.
\item
\emph{Repeated measures.}
From each $\bX_i$, one draws $r$ conditionally independent responses
$\bY_i^1,\hdots, \bY_i^r$.  The sampling scheme $G$ is a graph where
the only edges are from $\bX_i$ to $\bY_i^j$ for $i = 1,\hdots, n$, $j
= 1,\hdots, r$.
\end{itemize}

The \emph{prediction task} involves a \emph{prediction sampling
scheme}, given by a graph $G_{task}$, a pair of functions
$\vec{u}, \vec{v}$, and a \emph{cost function} $C$, and
an \emph{auxillary} random variable $W$ which is independent of the
sample.  Let $p_{task}$ denote the density specified by $G_{task}$,
and let $S_{task}$ denote the support of the $p_{task}$.
The \emph{predictors} of the prediction task, $\bU$ is given by
$\vec{u}$, a function of components of $\bS_{task}$ and possibly of
$W$, and the \emph{prediction target} $\bV$, is given by $\vec{v}$,
which is a function of components of $\bS_{task}$ and possibly of $W$.
The cost function $C(\bv, \hat{\bv})$ specifies the \emph{cost}
incurred when the true target is $\bv$ and the prediction is
$\hat{\bv}$.

A \emph{predictive model} is a function which predicts the target
$\bV$ as a function of the predictors $\bU$.  The prediction task and
joint density $p(\bx,\by)$ jointly define a \emph{risk function} for
any predictive model $\vec{f}$:
\[
\text{Risk}(\vec{f}) = \E[C(\bV, \hat{f}(\bV))].
\]
The \emph{Bayes risk} is the infimal risk over all predictive models:
\[
\text{R}_{Bayes}[p(\bx,\by)] = \inf_{\vec{f}} \text{Risk}(\vec{f}).
\]
In anticipation of the next section, we write $\text{R}_{Bayes}$ as a
functional of the joint density.

The training data is given by a sampling scheme $G_{train}$, and the
testing data is given by a sampling scheme $G_{test}$.
A \emph{learning algorithm} outputs a predictive model $\vec{f}$ given
the training sample $\bS_{train}$ as input.

The analysis culminates in an unbiased estimate of the risk of the
learned model, $\text{Risk}(\vec{f})$.  Define a \emph{rooted
subgraph} $H$ of a graph $G$ as a subgraph where all parentless
vertices in $H$ are also parentless in $G$.  Then, let $G_1,\hdots,
G_m$ be the rooted subgraphs of $G_{test}$ that are color-isomorphic
to $G_{task}$ (i.e. the graph isomorphism preserves vertex color.)
Let $\bS_1,\hdots, \bS_m$ be the corresponding samples obtained from
the subgraphs $G_1,\hdots, G_m$, and let $(\bu_i, \bv_i) =
(\vec{u}(\bS_i, W), \vec{v}(\bS_i, W))$ for $i = 1,\hdots, m$.  An unbiased estimate of $\text{Risk}(\vec{f})$ is given by
\[
\widehat{\text{Risk}(\vec{f})} = \frac{1}{m}\sum_{i=1}^m \E[C(\vec{v}_i, \vec{f}(\vec{u}_i))|\bS_{test}]
\]
where the expectation is taken over the distribution of $W$.

Now we revisit the examples of supervised learning given in
section \ref{sec:background_sl}.
\begin{itemize}
\item \emph{Regression.}  
The task, training and test sampling schemes are all pair-sampling,
with $n= 1$ for the task sampling scheme.
\[
\vec{u}(\bx) = \bx
\]
and
\[
\vec{v}(\by) = \by.
\]
The cost function for squared-error loss is
\[
C(\by, \hat{\by}) = ||\by - \hat{\by}||^2.
\]
\item \emph{Classification.}  
Here, $\bX$ is the class label and $\bY$ is the feature vector. The
sampling schemes are the same as in regression.  The predictor and
response are flipped:
\[
\vec{u}(\bx) = \bx,
\]
\[
\vec{v}(\by) = \by.
\]
And since $\bX$ is discrete, it becomes reasonable to adopt the zero-one loss
\[
C(\bx,  \hat{\bx}) = I\{\bx \neq \hat{\bx}\}.
\]
\item \emph{Identification}.  
Let $k$ be the number of classes in the test set. The sampling schemes
are pair-sampling, with $k$ pairs for the task sampling scheme.  Let
$W$ be an independent uniform variate drawn from $\{1,\hdots, k\}$.
We have
\[
\vec{u}((\bx_1,\by_1),\hdots, (\bx_k, \by_k), W) = (\bx_1,\hdots, \bx_k, \by_W).
\]
\[
\vec{v}(W) = W.
\]
The cost function $C$ is the zero-one loss.
\end{itemize}

We give one more example of a supervised learning task, due to its
connection to information theory.
\begin{itemize}
\item \emph{$M, k$-decoding for a random codebook}.  $X, Y$ are discrete random variables. The sampling schemes are pair-sampling, with $Mk$ pairs for the task sampling scheme.
Let $W$ be an
independent uniform variate drawn from $\{1,\hdots, M\}$.  We have
\[
\vec{u}((x_1,y_1),\hdots, (x_{Mk}, y_{Mk}), W) = (x_1,\hdots, x_{Mk}, y_{(W-1)k + 1},\hdots, y_{(W-1)k}).
\]
\[
\vec{v}(W) = W.
\]
The cost function $C$ is the zero-one loss.
\end{itemize}
The prediction task is the classical example of \emph{decoding from a
random codebook} seen in information theory.  An information channel
is given by $p(y|x)$ and a source distribution is given by $p(x)$.
First, one forms a codebook consisting of $M$ random codewords
$\{\bX_1,\hdots, \bX_M\}$, where each codeword is a length-$M$ vector
\[
\bX_i = (X_{(i-1)k+1},\hdots, X_{(i-1)k}).
\]
The codebook is shared with both the sender and the reciever.  Next,
the sender chooses a codeword at random to send over the channel,
$\bX_W$.  The reciever observes the message
\[
\bY = (Y_{(W-1)k+1}, \hdots, Y_{(W-1)k}).
\]
Then, the decoding task is for the reciever to guess the index of the
message, $W$, based on $\bY$.  It is a consequence of a famous result
in information theory, \emph{the channel-coding theorem}, that taking
$k \to \infty$ and $M = \exp[k\iota]$, then
\[
\lim_{k \to \infty} R_{Bayes} = \begin{cases}
0 & \text{ if }\iota \leq \text{I}(X; Y),\\
1 & \text{ otherwise.}
\end{cases}
\]
Later, we make use of this result to relate mutual information to our
general class of information coefficients.

\subsection{A general class of information coefficients}\label{sec:gen_class_sub}

Now that we have the axioms of information and a general definition of
supervised learning, we can deliver the central concept in the paper:
a general class of information coefficients based on supervised
learning.

First, recall the intuitive characterization of information as
``reduction of uncertainty.''  If we measure uncertainty using Shannon
entropy $\text{H}(\bX)$, then the resulting information coefficient is
mutual information $\text{I}(\bX; \bY)$.  Therefore, to generalize
this definition, we use a \emph{general prediction task} to define
uncertainty.

Let $\bS$ be a sample drawn from sampling scheme $p_{task}$,
$C: \mathcal{V} \times \mathcal{V} \to \mathbb{R}$ be a cost function,
and let $\vec{v}$ be a function, with $\bV = \vec{v}(\bS)$.  Define
the \emph{unconditional uncertainty} of the task to be
\[
\inf_{\hat{\bv}} \E[C(\bV, \hat{\bv})].
\]
Note that this is the Bayes risk for the prediction problem in the
case that $\vec{u}$ is a trivial (constant) function.
How much is the uncertainty reduced upon seeing $\bU$?
The \emph{conditional uncertainty} is
\[
\inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))].
\]
Therefore, the resulting definition for a putative information coefficient is
\[
\mathcal{I}(\bX; \bY) = \inf_{\hat{\bv}} \E[C(\bV, \hat{\bv})] - \inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))].
\]
However, some additional conditions on the prediction task are needed
to ensure that $\mathcal{I}$ is indeed an information coefficient.
Firstly, it is clear that $\mathcal{I}$ is a functional of
$p(\bx, \by)$, and that it must be non-negative.  However, we need to
find conditions which ensure that $\mathcal{I}(\bX; \bY) = 0$ in the
case of independence, and to ensure the stochastic data-processing
inequality.

We say that $(\vec{u}, \vec{v})$ are an \emph{orthogonal pair} if
$\bX \perp \bY$ implies $\vec{u}(\bS) \perp \vec{v}(\bS)$ for all
marginal distributions of $\bX$ and $\bY$.  A sufficient condition for
$(\vec{u}, \vec{v})$ to be an \emph{orthogonal pair} is that $\vec{u}$
and $\vec{v}$ have disjoint arguments.  This is clearly met in the
regression and classification case.  The sufficient condition is not
met in the identification case, but $(\vec{u}, \vec{v})$ are still an
orthogonal pair, since if $\bX \perp \bY$ then $\by_W$ is independent
of $W$.

A sufficient condition for the data-processing inequality is that
$\vec{v}$ not have any arguments in $V_Y$.  To show the
data-processing inequality, consider the joint distribution of
$(\bS, \tilde{bS})$, where $\bS$ is drawn from the task sampling
scheme with the joint distribution $P$ of $(\bX, \bY)$, and
$\tilde{\bS}$ is obtained by replacing $\bY_i$ in $\bS$ with
$\vec{h}(\bY_i)$.  Letting $W$ be an auxillary variable, obtain $\bV$
by applying $\vec{v}$ to $(\bS, W)$, obtain $\tilde{bV}$ by applying
$\vec{v}$ to $(\tilde{\bS}, W)$, and define $\bU$ and $\tilde{\bU}$
analogously.  We have
\[
\mathcal{I}(\bX; \bY) = \inf_{\hat{\bv}} \E[C(\bV, \hat{\bv})] - \inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))],
\]
\[
\mathcal{I}(\bX; \vec{h}(\bY)) = \inf_{\hat{\bv}} \E[C(\tilde{\bV}, \hat{\bv})] - \inf_{\vec{f}} \E[C(\tilde{\bV}, \vec{f}(\tilde{\bU}))].
\]
where all expectations are with respect to $P$.
Because $\vec{v}$ has no arguments in $V_Y$, we have
\[
\bV = \tilde{\bV}.
\]
Therefore,
\[
\mathcal{I}(\bX; \bY) - \mathcal{I}(\bX; \vec{h}(\bY)) =  \inf_{\vec{f}} \E[C(\bV, \vec{f}(\bU))] -  \inf_{\vec{f}} \E[C(\bV, \vec{f}(\tilde{\bU}))].
\]
Now we claim that there exists a function 







In the case of classification, we get a information coefficient
$\mathcal{I}(\bX; \bY)$ and in the case of regression, we get
$\mathcal{I}(\bY; \bX)$: these are different concepts, since in
general $\mathcal{I}$ is not symmetric.  The story is much less clear
for the case of identification due to the complicated structure of the
prediction task; we will see later that the result can be expressed as
an information coefficient $\mathcal{I}(\bX; \bY)$ as in
classification.





\begin{itemize}
\item 
Prediction loss is a measure of uncertainty.  Mean-square loss
corresponds to variance.
\item 
Consider two prediction problems: in one case you are given no side
information, and in the second case you are given $\bY$.  The
difference in risk between the two problems gives a measure of
uncertainty reduction.
\item
In regression, this gives ``variance explained'' as a measure of
information.
\item
In classification, we get a normalized form of Bayes accuracy.
\item
In randomized classification, we get (normalized) average Bayes accuracy.
\item
We can prove that any prediction task yields an information measure
i.e. satisfying the three axioms.
\item
If the no-information risk is known in advance, we say the information
measure is estimable.
\item
Mutual information can be characterized this way, but it is not estimable.
\item
Mutual information can be derived as a limit of linear combinations of
normalized ABA, due to channel coding theorem.  Still not estimable
due to limit property.
\end{itemize}

\section{Statistical inference}\label{sec:inference}

\begin{itemize}
\item Only lower bounds are possible, because...
\end{itemize}

\section{Connections}\label{sec:connections}

\section{Applications}\label{sec:applications}

\section{Discussion}\label{sec:discussion}

\end{document}












The word ``mutual'' is appropriate given the symmetry of
$I(\bX; \bY)$.  However, at the intuitive level it does not
seem \emph{a priori} necessary that the measure of information be
symmetric; indeed, generalizations of mutual information (Verd\'{u}
2015) are asymmetric.  Having a quantitative definition of
information, such as Shannon's $I(\bX;\bY)$, opens the possibility of
computing, estimating or inferring the quantity for $(\bX, \bY)$ pairs
of interest in the natural world.  Classical neuroscience experiments
estimated the mutual information between the reaching angle of a
monkey's arm, $X$, and the average firing rate of particular motor
neurons, $Y$.  Once estimates are available for individual pairs,
cross-pair comparisons are possible.  Scientists can check if motor
neurons in one particular region of the brain are more ``informative''
(on an individual level) than motor neurons in another brain region.
Even more interestingly, one can take an ensemble of neurons as $Y$.
Without collecting any additional data, scientists can compare the
information of individual neurons $Y_1$ and $Y_2$, and compare each to
the information about $X$ carried by the ensemble $(Y_1,Y_2)$.
Comparisons between ensembles motivate thinking about a ``calculus of
information.''  If
\[
I(X; Y_1) + I(X; Y_2) > I(X; (Y_1, Y_2)),
\]
then we say that $Y_1$ and $Y_2$ carry \emph{redundant} information.
If, on the other hand,
\[
I(X; Y_1) + I(X; Y_2) < I(X; (Y_1, Y_2)),
\]
one can say that $Y_1$ and $Y_2$ have \emph{synergy}--the information
carried by the whole is greater than the sum of the information of
each part.









However, the dominance of mutual information in neuroscience started
to wane as new experimental approaches shifted the focus from pairwise
comparisons to questions of \emph{population coding}.  The extreme
reductionist approach fails to account for the complex ways in which
neurons cooperatively encode complex information, and to get a richer
picture of neural dynamics, it becomes necessary to consider
multivariate measures of dependence.  Also, the adoption of
``high-throughput'' recording technologies such as EEG and fMRI
naturally led in the direction of considering the system-level
dynamics of the whole brain.  More importantly, the technology for
studying the relationships between brain structure, function, and
behavior have enabled a wider population of investigators
(psychologists and neurologists) to quantitatively examine brain
activity, but with motivations which tend to be more holistic and
instrumental (e.g. finding neural correlates of mental disorders) when
compared to the reductionist orientation of classical neuroscience.
For both reasons, the demand for multivariate statistical techniques
in neuroscience has increased dramatically in recent years. While the
theoretical properties of mutual information extend gracefully to the
multivariate setting, the difficulty of estimating mutual information
increases exponentially in the dimension in the absence of strong
modeling assumptions [cite].  Partially for this reason, alternative
measures of multivariate dependence started to enjoy increasing usage
in studies of population coding or in systems-level investigations of
the brain: these include measures of linear dependence (canonical
correlation and multivariate $R^2$) and Fisher information.  However,
while both correlation-based statistics and Fisher information may be
easier to estimate than mutual information in high-dimensional
settings, they are both less flexible in terms of capturing nonlinear
relationships.  Correlation-based statistics can only capture linear
dependence, and Fisher information requires the assumption of a
parametric model.











The concept of ``information'' plays a key role in areas as diverse as
game theory, biology, neuroscience, and human engineering.  A random
quantity $\bY$ carries information about $\bX$ if observing $\bY$ reduces
our uncertainty about $\bX$.  In game theory, $\bY$ could be the
opponent's bet, which reveals information about $\bX$, the opponent's
hand.  In neuroscience, $\bY$ is brain activity which correlates to
visual stimulus, $\bX$.  In communications, $\bX$ is a plaintext message
which is coded and transmitted as $\bY$, a series of recieved bits.

