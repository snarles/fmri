\title{Prediction, information, and inference: with application to neuroimaging}
\author{Charles Zheng and Yuval Benjamini}
\date{\today}

\documentclass[12pt]{article} 

% packages with special commands
\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}

\definecolor{grey}{rgb}{0.5,0.5,0.5}

\begin{document}
\maketitle

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\Cor}{\text{Cor}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]



\begin{abstract}
Neuroscientists have a variety of tools for quantifying multivariate
dependence: mutual information, linear correlation-based statistics,
Fisher information, and more recently, measures of performance on
supervised learning tasks such as classification.  We argue that both
mutual information and classification accuracy capture intuitive
properties of an ``information metric'' for a channel, and we proceed
to develop a general axiomatic characterization of information metrics
for channels consisting of a pair of input and output random
variables.  The key axioms of an information metric are that (i) it is
a scalar functional of the joint distribution of the input-output
pair, (ii) it is zero for independent variables, and positive for
dependent variables, (iii) satisfies a
generalized \emph{data-processing} inequality, where transformations
of the output can only preserve or reduce the information.  We show
how prediction tasks can be used to define a general class of
information metrics which includes mutual information, as well as a
novel information metric, \emph{average Bayes accuracy}, which can be
considered an ``idealization'' of classification accuracy.
Furthermore, we consider the possibility of developing a general
theory of statistical inference for this class of information metrics.
Concretely, we derive a lower confidence bound for average Bayes
accuracy as well as a novel lower confidence bound for mutual
information.
\end{abstract}


\section{Introduction}

Historically, neuroscience has largely taken a reductionist approach
to understanding the nervous system, proceeding by defining elements
and subelements of the nervous system (e.g. neurons), and
investigating relationship between two different elements, or the
response of an element to external stimulation: say, the response of a
neuron's average firing rate to skin temperature.  At one level of
abstraction, neuroscientists might seek to characterize the functional
relationship between elements, but at a higher level of abstraction,
it may be sufficient to report scalar measures of dependence.  Since
neural dynamics are generally both stochastic and nonlinear, it was a
natural choice for early neuroscientists to adopt
Shannon's \emph{mutual information} as a quantitative measure of
dependence.  But as new technologies enabled the recording of neural
data at larger scales and resolution, the traditional reductionist
goals of neuroscience were supplemented by increasingly ambitious
attempts within neuroscience to understand the dynamics of neural
ensembles, and by efforts originating within psychology and medicine
to link the structure and function of the entire human brain to
behavior or disease.  The larger scope of the data and the questions
being asked of the data created an increasing demand for multivariate
statistical methods for analyzing neural data of increasingly high
dimension.  Due to the complexity, variety, and practical difficulties
of multivariate statistical analysis of the brain, alternative
measures of multivariate dependence such as linear-based correlational
statistics, or Fisher information, started to gain traction.  For the
most part, alternative measures of dependence sacrifice flexibility
for a gain in practical convenience: linear-based statistics such as
canonical correlation or correlation coefficients fail to capture
nonlinear dependencies, and Fisher information requires strong
parametric assumptions.  Therefore, it was of considerable interest
when Haxby (2001) introduced the usage of \emph{supervised learning}
(classification tasks) for the purpose of quantifying stimulus
information in task fMRI scans.  Since then, an entire subfield of
neuroimaging, multivariate pattern analysis (MVPA) has been
established dedicated to quantifying multivariate information in the
brain, and both mutual information and classification accuracy are
used by practitioners within the field.  Judging from the language
used by the practioners themselves, it is intuitively clear to them
how classification accuracies can be used to quantify information in
brain scans.  However, a more thorough examination of the practice
raises many questions with regards to the use of classification
accuracy as a metric of information: this is one motivation for the
current work.  But taking a step back, it would seem valuable at this
historical juncture to examine the intuitive properties of
``information'' as a measure of multivariate dependence, and not only
consider whether classification accuracy can be considered or used to
derive a new information metric, but whether other such metrics might
also exist, and whether a unified theory can be developed to account
for all of them.  This is the larger purpose of the current work, and
towards that end we not only propose a general class of information
metrics which unifies both information-theoretic and
supervised-learning-based approaches, but with an eye toward practical
applications, we also examine the question of inferring these
quantities from data.  An initial result in this direction is the
derivation of nonparametric lower confidence bounds for average Bayes
accuracy (a novel information metric closely related to classification
accuracy,) and an inequality between average Bayes accuracy and mutual
information, which, combined with the preceding result, yields a novel
lower confidence bound for mutual information.

\subsection{Organization}

The rest of the paper is organized as follows.  Section \ref{sec:info}
plays the role of a ``background'' section that gives the basics of
mutual information and supervised learning as they are used in
neuroscience, but also introduces our axiomatization of information.
In section \ref{sec:gen_class}, we further explore some of the themes
developed in the preceding section relating information, uncertainty,
and prediction, and introduce a general class of information metrics
which satisfies our axioms.  We define a new information metric
belonging to this class, average Bayes accuracy, and we also show how
mutual information can be considered an ``extended member'' of the
class.  In section \ref{sec:inference} we develop the basic theory of
what kinds of inferences about our information metrics are possible
discuss the kinds of experimental designs and supervised learning
pipelines which are needed to enable such inference.  Concretely, we
develop a lower confidence bound for average Bayes accuracy. In
section \ref{sec:connections} we outline a comparative theory for
different metrics within our framework: how are the different
information metrics related?  We discuss the calculus of variations as
a possible general technique for establishing inequalities between
different information metrics, and in particular we derive a
``randomized Fano's inequality'': a lower bound for mutual information
as a function of average Bayes accuracy.  Combined with our lower
confidence bound for average Bayes accuracy, this yields a novel lower
confidence bound for mutual information.  We provide a practical data
analysis example in section \ref{sec:applications}.  A discussion
section includes future directions and loose ends are treated, and
most of the technical proofs and lemmas are found in the appendix.

\section{Measures of information}\label{sec:info}

\subsection{Mutual information and its usage}
%It is in the context of communication system that Shannon first
%proposed a quantification of the concept of ``the amount of
%information that $\bY$ carries about $\bX$'', in the form
%of \emph{mutual information}: 
While Shannon's theory of information was motivated by the problem of
designing communications system, the applicability of mutual
information was quickly recognized by neuroscientists.  Only four
years after Shannon's seminal paper in information theory (1948),
McKay and McCullough (1952) inaugurated the application of mutual
information to neuroscience.  If $\bX$ and $\bY$ have joint density
$p(\bx, \by)$ with respect to the product measure $\mu_x \times \mu_y$, then the mutual information is defined as
\[
\text{I}(\bX;\bY) = \int p(\bx, \by) \log \frac{p(\bx, \by)}{p(\bx)p(\by)}d\mu(\bx) d\mu(\by).
\]
where $p(\bx)$ and $p(\by)$ are the marginal densities with respect to
$\mu_x$ and $\mu_y$\footnote{Note that the mutual information is invariant with respect to change-of-measure.}.  Since then, mutual information has enjoyed a
celebrated position in both experimental and theoretical neuroscience.
Experimentally, mutual information has been used to detect strong
dependencies between stimulus features and features derived from
neural recordings, which can be used to draw conclusions about the
kinds of stimuli that a neural subsystem is designed to detect, or to
distinguish between signal and noise in the neural output.
Theoretically, the assumption that neural systems maximize mutual
information between salient features of the stimulus and neural output
has allowed scientists to predict neural codes from signal processing
models: for instance, the center-surround structure of human retinal
neurons matches theoretical constructions for the optimal filter based
on correlations found in natural images [cite].

The mutual information measures the information ``capacity'' of a
channel consisting of an input $\bX$ and an output $\bY$, and
satisfies a number of important properties.
\begin{enumerate}
\item The channel input $\bX$ and output $\bY$ can be random vectors of arbitrary dimension, and the mutual information remains a scalar functional of the joint distribution $p(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\text{I}(\bX; \bY) = 0$; otherwise, $\text{I}(\bX; \bY) > 0$.
\item The data-processing inequality: for any vector-valued function $\vec{f}$ of the output space,
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
\item Symmetry: $\text{I}(\bX; \bY) = \text{I}(\bY; \bX)$.
\item Additivity: if $(\bX_1,\bY_1)$ is independent of $(\bX_2, \bY_2)$, then
\[
\text{I}((\bX_1,\bY_1); (\bX_2, \bY_2)) = \text{I}(\bX_1; \bY_1) + \text{I}(\bX_2; \bY_2).
\]
\end{enumerate}
Two additional consequences result from the data-processing inequality:
\begin{itemize}
\item \emph{Invariance under bijections.} If $\vec{f}$ has an inverse $\vec{f}^{-1}$, then 
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY) = \text{I}(\bX; \vec{f}^{-1}(\vec{f}(\bY))) \leq \text{I}(\bX; \vec{f}(\bY)),
\]
therefore, $\text{I}(\bX; \vec{f}(\bY)) = \text{I}(\bX; \bY)$.
\item \emph{Monotonicity with respect to inclusion of outputs.}  Suppose we have an output ensemble $(\bY_1,\bY_2)$.  Then the individual component $\bY_1$ can be obtained as a projection of the ensemble.  By the data-processing inequality, we therefore have
\[
\text{I}(\bX; \bY_1) \leq \text{I}(\bX; (\bY_1, \bY_2)).
\]
Intuitively, if we observe both $\bY_1$ and $\bY_2$, this can
only \emph{increase} the information we have about $\bX$ compared to
the case where we only observe $\bY_1$ by itself.
\end{itemize}
And it is the property of \emph{invariance under bijections},
inclusive of non-linear bijections, which qualifies mutual information
as a \emph{non-linear measure of dependence.}  Linear measures, such
as Pearson correlation, are not invariant under bijections.  

%As for
%the completeness of the five listed properties: as we know, Shannon's
%mutual information (up to arbitrary scaling factor) is the only
%functional proposed in the literature which satisfies all five
%properties.
Besides the formal definition, there are a number of well-known alternative
characterizations of mutual information in terms of other
information-theoretic quantities: the \emph{entropy} $\text{H}$:
\[
\text{H}_\mu(\bX) = -\int p(\bX) \log p(\bX) d\mu(\bX),
\]
and the \emph{conditional entropy}:
\[
\text{H}_\mu(\bX|\bY) = -\int p(\bY) d\mu_y(\bY) \int p(\bX|\bY) \log p(\bX|\bY) d\mu_x(\bX).
\]
Some care needs to be taken with entropy and conditional entropy since
they are not invariant with respect to change-of-measure: hence the
use of the subscript in the notation $\text{H}_\mu$.  In particular,
there is a difference between \emph{discrete entropy} (for counting
measure) and \emph{differential entropy} (for Lesbegue measure.)
Intutively, entropy measures an observer's uncertainty of the random
variable $\bX$, supposing the observer has no prior information other
than the distribution of $\bX$. Conditional entropy measures
the \emph{expected uncertainty} of $\bX$ supposing the observer
observes $\bY$.

However, regardless of the base measure, the following identities hold:
\[
\text{I}(\bX; \bY) = \text{H}_{\mu_x \times \mu_y}((\bX, \bY)) - \text{H}_{\mu_x}(\bX) - \text{H}_{\mu_y}(\bY).
\]
\begin{equation}\label{eq:ce_ident}
\text{I}(\bX; \bY) = \text{H}_\mu(\bY) - \text{H}_\mu(\bY|\bX).
\end{equation}
The second identity \eqref{eq:ce_ident} is noteworthy
as being practically important for estimation of mutual information.
Since the entropies in question only depend on the marginal and
conditional distributions of $\bY$, the problem of estimating
$\text{I}(\bX; \bY)$ can be reduced from a $\dim(\bX)
+ \dim(\bY)$-dimensional nonparametric estimation problem to a
$\dim(\bY)$-dimensional problem: hence this identity is a basis of
several methods of estimation used in neuroscience, such as Gastpar
(2014).

However, by symmetry, we also have the flipped identity
\begin{equation}\label{eq:ce_ident2}
\text{I}(\bX; \bY) = \text{H}_\mu(\bX) - \text{H}_\mu(\bX|\bY).
\end{equation}
In neuroscience studies, where $\bX$ is the controlled stimulus, and
$\bY$ is the neural activity, the two mirror pairs \eqref{eq:ce_ident}
and \eqref{eq:ce_ident2} have different interpretations.  Rather than
providing a basis for practical estimation, \eqref{eq:ce_ident2}
provides an \emph{interpretation} of the mutual information.  Loosely
speaking, $\text{H}_\mu(\bX)$ is the uncertainty of $\bX$ before
having observed $\bY$, and $\text{H}_\mu(\bX|\bY)$ is the uncertainty
of $\bX$ after having observed $\bY$, hence $\text{H}_\mu(\bX)
- \text{H}_\mu(\bX|\bY)$ is how much the observation of $\bY$
has \emph{reduced} the uncertainty of $\bX$.  Stated in words,
\[
\text{I}(\bX; \bY) = \text{average reduction of uncertainty about $\bX$ upon observing $\bY$}.
\]
We explore this concept of ``information as reduction of uncertainty''
much further when we propose our general class of information metrics:
there, we see that properties (i)-(iii) of the mutual information are
consequences of this second characterization of mutual information.

But how what is the practical import of these properties of mutual
information?  Supposing we identify a number of properties as being
essential for scientific purposes (and others as non-essential), this
then suggests that alternative measures of information, also
satisfying the same essential properties, could be just as effective
for scientific work as mutual information.  And some might have
additional advantages.

Towards our goal of formulating a general theory of information
metrics, it is important to evaluate exactly \emph{how important} each
of the listed properties.

[to be continued: applications of mutual information]

\begin{itemize}
\item Example: Comparison of decoders in Nelken.  
Property (i) is important to enable model comparison.  Property (iii)
is needed because relationships may be nonlinear.
\item Example: Redundancy in population code of retina.  
Property (i)-(iii) and (v) are needed to obtain a meaningful measure
of redundancy.
\item In general, symmetry not important, but additivity is desirable 
for measures of redundancy.  Property (ii) can usually be enforced
since any measure needs to have a unique ``minimum'' value for the
case of independence.
\end{itemize}

\subsection{Supervised learning}

\begin{itemize}
\item Supervised learning task is defined using a prediction task.
\item 1. A predictive model is learned using training data
\item 2. The performance of the model on the prediction task is estimated using independent test data
\item Classical examples of prediction tasks: regression and classification
\item Third example: identification
\item Definition of Bayes prediction model
\item General definition of supervised learning task
\item SL performance can be a scalar
\item SL can be used to test for independence
\item Bayes performance satisfies data-processing inequality
\item How SL is interpreted in MVPA as information
\item Section 3, we'll see how Bayes performance can be legitimately considered a measure of information
\end{itemize}

\subsection{Axiomatic characterization of information}

Let $\mathcal{I}(\bX; \bY)$ denote a general measure of information.
\begin{enumerate}
\item Information is a scalar functional of the joint distribution $p(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\mathcal{I}(\bX; \bY) = 0$; otherwise, $\mathcal{I}(\bX; \bY) > 0$.
\item The data-processing inequality: for any vector-valued function $\vec{f}$ of the output space,
\[
\mathcal{I}(\bX; \vec{f}(\bY)) \leq \mathcal{I}(\bX; \bY).
\]
\end{enumerate}

Mutual information satisfies the property.  Bayes performance
satisfies it in regression case, but not in identification case due to
dependence on exemplars.

In the next section, we see how both MI and Bayes perf are examples of
a large class of information measures.

\section{Information, uncertainty and prediction}\label{sec:gen_class}

The general idea is to look at reduction of uncertainty.

\begin{itemize}
\item 
Prediction loss is a measure of uncertainty.  Mean-square loss
corresponds to variance.
\item 
Consider two prediction problems: in one case you are given no side
information, and in the second case you are given $\bY$.  The
difference in risk between the two problems gives a measure of
uncertainty reduction.
\item
In regression, this gives ``variance explained'' as a measure of
information.
\item
In classification, we get a normalized form of Bayes accuracy.
\item
In randomized classification, we get (normalized) average Bayes accuracy.
\item
We can prove that any prediction task yields an information measure
i.e. satisfying the three axioms.
\item
If the no-information risk is known in advance, we say the information
measure is estimable.
\item
Mutual information can be characterized this way, but it is not estimable.
\item
Mutual information can be derived as a limit of linear combinations of
normalized ABA, due to channel coding theorem.  Still not estimable
due to limit property.
\end{itemize}

\section{Statistical inference}\label{sec:inference}

\begin{itemize}
\item Only lower bounds are possible, because...
\end{itemize}

\section{Connections}\label{sec:connections}

\section{Applications}\label{sec:applications}

\section{Discussion}\label{sec:discussion}

\end{document}












The word ``mutual'' is appropriate given the symmetry of
$I(\bX; \bY)$.  However, at the intuitive level it does not
seem \emph{a priori} necessary that the measure of information be
symmetric; indeed, generalizations of mutual information (Verd\'{u}
2015) are asymmetric.  Having a quantitative definition of
information, such as Shannon's $I(\bX;\bY)$, opens the possibility of
computing, estimating or inferring the quantity for $(\bX, \bY)$ pairs
of interest in the natural world.  Classical neuroscience experiments
estimated the mutual information between the reaching angle of a
monkey's arm, $X$, and the average firing rate of particular motor
neurons, $Y$.  Once estimates are available for individual pairs,
cross-pair comparisons are possible.  Scientists can check if motor
neurons in one particular region of the brain are more ``informative''
(on an individual level) than motor neurons in another brain region.
Even more interestingly, one can take an ensemble of neurons as $Y$.
Without collecting any additional data, scientists can compare the
information of individual neurons $Y_1$ and $Y_2$, and compare each to
the information about $X$ carried by the ensemble $(Y_1,Y_2)$.
Comparisons between ensembles motivate thinking about a ``calculus of
information.''  If
\[
I(X; Y_1) + I(X; Y_2) > I(X; (Y_1, Y_2)),
\]
then we say that $Y_1$ and $Y_2$ carry \emph{redundant} information.
If, on the other hand,
\[
I(X; Y_1) + I(X; Y_2) < I(X; (Y_1, Y_2)),
\]
one can say that $Y_1$ and $Y_2$ have \emph{synergy}--the information
carried by the whole is greater than the sum of the information of
each part.









However, the dominance of mutual information in neuroscience started
to wane as new experimental approaches shifted the focus from pairwise
comparisons to questions of \emph{population coding}.  The extreme
reductionist approach fails to account for the complex ways in which
neurons cooperatively encode complex information, and to get a richer
picture of neural dynamics, it becomes necessary to consider
multivariate measures of dependence.  Also, the adoption of
``high-throughput'' recording technologies such as EEG and fMRI
naturally led in the direction of considering the system-level
dynamics of the whole brain.  More importantly, the technology for
studying the relationships between brain structure, function, and
behavior have enabled a wider population of investigators
(psychologists and neurologists) to quantitatively examine brain
activity, but with motivations which tend to be more holistic and
instrumental (e.g. finding neural correlates of mental disorders) when
compared to the reductionist orientation of classical neuroscience.
For both reasons, the demand for multivariate statistical techniques
in neuroscience has increased dramatically in recent years. While the
theoretical properties of mutual information extend gracefully to the
multivariate setting, the difficulty of estimating mutual information
increases exponentially in the dimension in the absence of strong
modeling assumptions [cite].  Partially for this reason, alternative
measures of multivariate dependence started to enjoy increasing usage
in studies of population coding or in systems-level investigations of
the brain: these include measures of linear dependence (canonical
correlation and multivariate $R^2$) and Fisher information.  However,
while both correlation-based statistics and Fisher information may be
easier to estimate than mutual information in high-dimensional
settings, they are both less flexible in terms of capturing nonlinear
relationships.  Correlation-based statistics can only capture linear
dependence, and Fisher information requires the assumption of a
parametric model.











The concept of ``information'' plays a key role in areas as diverse as
game theory, biology, neuroscience, and human engineering.  A random
quantity $\bY$ carries information about $\bX$ if observing $\bY$ reduces
our uncertainty about $\bX$.  In game theory, $\bY$ could be the
opponent's bet, which reveals information about $\bX$, the opponent's
hand.  In neuroscience, $\bY$ is brain activity which correlates to
visual stimulus, $\bX$.  In communications, $\bX$ is a plaintext message
which is coded and transmitted as $\bY$, a series of recieved bits.

