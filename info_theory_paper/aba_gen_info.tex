\title{Prediction, information, and inference: with application to neuroimaging}
\author{Charles Zheng and Yuval Benjamini}
\date{\today}

\documentclass[12pt]{article} 

% packages with special commands
\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}

\definecolor{grey}{rgb}{0.5,0.5,0.5}

\begin{document}
\maketitle

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\Cor}{\text{Cor}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]



\begin{abstract}
Neuroscientists have a variety of tools for quantifying multivariate
dependence: mutual information, linear correlation-based statistics,
Fisher information, and more recently, measures of performance on
supervised learning tasks such as classification.  We argue that both
mutual information and classification accuracy capture intuitive
properties of an ``information coefficient'' for a channel, and we proceed
to develop a general axiomatic characterization of information coefficients
for channels consisting of a pair of input and output random
variables.  Arguably, the key properties of an information coefficient are
that (i) it is a scalar measure of multivariate dependence, and (ii)
that it satisfies a \emph{stochastic data-processing inequality}: any
coefficient with such properties can be used for model selection.
%The key axioms of an information coefficient are that (i) it is
%a scalar functional of the joint distribution of the input-output
%pair, (ii) it is zero for independent variables, and positive for
%dependent variables, (iii) satisfies a
%generalized \emph{data-processing} inequality, where transformations
%of the output can only preserve or reduce the information.  
We show how prediction tasks can be used to define a general class of
information coefficients which includes mutual information, as well as a
novel information coefficient, \emph{average Bayes accuracy}, which can be
considered an ``idealization'' of classification accuracy.
Furthermore, we consider the possibility of developing a general
theory of statistical inference for this class of information coefficients.
Concretely, we derive a lower confidence bound for average Bayes
accuracy as well as a novel lower confidence bound for mutual
information.
\end{abstract}


\section{Introduction}

Historically, neuroscience has largely taken a reductionist approach
to understanding the nervous system, proceeding by defining elements
and subelements of the nervous system (e.g. neurons), and
investigating relationship between two different elements, or the
response of an element to external stimulation: say, the response of a
neuron's average firing rate to skin temperature.  At one level of
abstraction, neuroscientists might seek to characterize the functional
relationship between elements, but at a higher level of abstraction,
it may be sufficient to report scalar measures of dependence.  Since
neural dynamics are generally both stochastic and nonlinear, it was a
natural choice for early neuroscientists to adopt
Shannon's \emph{mutual information} as a quantitative measure of
dependence.  But as new technologies enabled the recording of neural
data at larger scales and resolution, the traditional reductionist
goals of neuroscience were supplemented by increasingly ambitious
attempts within neuroscience to understand the dynamics of neural
ensembles, and by efforts originating within psychology and medicine
to link the structure and function of the entire human brain to
behavior or disease.  The larger scope of the data and the questions
being asked of the data created an increasing demand for multivariate
statistical methods for analyzing neural data of increasingly high
dimension.  Due to the complexity, variety, and practical difficulties
of multivariate statistical analysis of the brain, alternative
measures of multivariate dependence such as linear-based correlational
statistics, or Fisher information, started to gain traction.  For the
most part, alternative measures of dependence sacrifice flexibility
for a gain in practical convenience: linear-based statistics such as
canonical correlation or correlation coefficients fail to capture
nonlinear dependencies, and Fisher information requires strong
parametric assumptions.  Therefore, it was of considerable interest
when Haxby (2001) introduced the usage of \emph{supervised learning}
(classification tasks) for the purpose of quantifying stimulus
information in task fMRI scans.  Since then, an entire subfield of
neuroimaging, multivariate pattern analysis (MVPA) has been
established dedicated to quantifying multivariate information in the
brain, and both mutual information and classification accuracy are
used by practitioners within the field.  Judging from the language
used by the practioners themselves, it is intuitively clear to them
how classification accuracies can be used to quantify information in
brain scans.  However, a more thorough examination of the practice
raises many questions with regards to the use of classification
accuracy as a coefficient of information: this is one motivation for the
current work.  But taking a step back, it would seem valuable at this
historical juncture to examine the intuitive properties of
``information'' as a measure of multivariate dependence, and not only
consider whether classification accuracy can be considered or used to
derive a new information coefficient, but whether other such coefficients might
also exist, and whether a unified theory can be developed to account
for all of them.  This is the larger purpose of the current work, and
towards that end we not only propose a general class of information
coefficients which unifies both information-theoretic and
supervised-learning-based approaches, but with an eye toward practical
applications, we also examine the question of inferring these
quantities from data.  An initial result in this direction is the
derivation of nonparametric lower confidence bounds for average Bayes
accuracy (a novel information coefficient closely related to classification
accuracy,) and an inequality between average Bayes accuracy and mutual
information, which, combined with the preceding result, yields a novel
lower confidence bound for mutual information.

\subsection{Organization}

The rest of the paper is organized as follows.  Section \ref{sec:info}
plays the role of a ``background'' section that gives the basics of
mutual information and supervised learning as they are used in
neuroscience, as well as practical issues related to estimation.  In
section \ref{sec:gen_class}, we present an axiomatic characterization
of information coefficients, and introduce a general class of information
coefficients which satisfies our axioms.  We define a new information
coefficient belonging to this class, average Bayes accuracy, and we also
show how mutual information can be considered an ``extended member''
of the class.  In section \ref{sec:inference} we develop the basic
theory of what kinds of inferences about our information coefficients are
possible discuss the kinds of experimental designs and supervised
learning pipelines which are needed to enable such inference.
Concretely, we develop a lower confidence bound for average Bayes
accuracy. In section \ref{sec:connections} we outline a comparative
theory for different coefficients within our framework: how are the
different information coefficients related?  We discuss the calculus of
variations as a possible general technique for establishing
inequalities between different information coefficients, and in particular
we derive a ``randomized Fano's inequality'': a lower bound for mutual
information as a function of average Bayes accuracy.  Combined with
our lower confidence bound for average Bayes accuracy, this yields a
novel lower confidence bound for mutual information.  We provide a
practical data analysis example in section \ref{sec:applications}.  A
discussion section includes future directions and loose ends are
treated, and most of the technical proofs and lemmas are found in the
appendix.

\section{Background}\label{sec:info}

\subsection{Mutual information and its usage}\label{sec:background_mi}
%It is in the context of communication system that Shannon first
%proposed a quantification of the concept of ``the amount of
%information that $\bY$ carries about $\bX$'', in the form
%of \emph{mutual information}: 
While Shannon's theory of information was motivated by the problem of
designing communications system, the applicability of mutual
information was quickly recognized by neuroscientists.  Only four
years after Shannon's seminal paper in information theory (1948),
McKay and McCullough (1952) inaugurated the application of mutual
information to neuroscience.  If $\bX$ and $\bY$ have joint density
$p(\bx, \by)$ with respect to the product measure $\mu_x \times \mu_y$, then the mutual information is defined as
\[
\text{I}(\bX;\bY) = \int p(\bx, \by) \log \frac{p(\bx, \by)}{p(\bx)p(\by)}d\mu(\bx) d\mu(\by).
\]
where $p(\bx)$ and $p(\by)$ are the marginal densities with respect to
$\mu_x$ and $\mu_y$\footnote{Note that the mutual information is invariant with respect to change-of-measure.}.  Since then, mutual information has enjoyed a
celebrated position in both experimental and theoretical neuroscience.
Experimentally, mutual information has been used to detect strong
dependencies between stimulus features and features derived from
neural recordings, which can be used to draw conclusions about the
kinds of stimuli that a neural subsystem is designed to detect, or to
distinguish between signal and noise in the neural output.
Theoretically, the assumption that neural systems maximize mutual
information between salient features of the stimulus and neural output
has allowed scientists to predict neural codes from signal processing
models: for instance, the center-surround structure of human retinal
neurons matches theoretical constructions for the optimal filter based
on correlations found in natural images [cite].

The mutual information measures the information ``capacity'' of a
channel consisting of an input $\bX$ and an output $\bY$, and
satisfies a number of important properties.
\begin{enumerate}
\item The channel input $\bX$ and output $\bY$ can be random vectors of arbitrary dimension, and the mutual information remains a scalar functional of the joint distribution $p(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\text{I}(\bX; \bY) = 0$; otherwise, $\text{I}(\bX; \bY) > 0$.
\item The data-processing inequality: for any vector-valued function $\vec{f}$ of the output space,
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
\item Symmetry: $\text{I}(\bX; \bY) = \text{I}(\bY; \bX)$.
\item Independent additivity: if $(\bX_1,\bY_1)$ is independent of $(\bX_2, \bY_2)$, then
\[
\text{I}((\bX_1,\bY_1); (\bX_2, \bY_2)) = \text{I}(\bX_1; \bY_1) + \text{I}(\bX_2; \bY_2).
\]
\end{enumerate}
Three additional consequences result from the data-processing inequality:
\begin{itemize}
\item \emph{Stochastic data-processing inequality}  If $\vec{f}$ is a stochastic function independent of both $\bX$ and $\bY$, then
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
This can be shown as follows: any stochastic function $\vec{f}(\bY)$
can be expressed as a deterministic function $\vec{g}(\bY, W)$, where
$W$ is a random variable independent of $\bX$ and $\bY$.
By independent additivity,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)).
\]
Then, by the data-processing inequality,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)) \geq \text{I}(\bX; \vec{g}(\bY, W)) = \text{I}(\bX; \vec{f}(\bY)).
\]
\item \emph{Invariance under bijections.} If $\vec{f}$ has an inverse $\vec{f}^{-1}$, then 
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY) = \text{I}(\bX; \vec{f}^{-1}(\vec{f}(\bY))) \leq \text{I}(\bX; \vec{f}(\bY)),
\]
therefore, $\text{I}(\bX; \vec{f}(\bY)) = \text{I}(\bX; \bY)$.
\item \emph{Monotonicity with respect to inclusion of outputs.}  Suppose we have an output ensemble $(\bY_1,\bY_2)$.  Then the individual component $\bY_1$ can be obtained as a projection of the ensemble.  By the data-processing inequality, we therefore have
\[
\text{I}(\bX; \bY_1) \leq \text{I}(\bX; (\bY_1, \bY_2)).
\]
Intuitively, if we observe both $\bY_1$ and $\bY_2$, this can
only \emph{increase} the information we have about $\bX$ compared to
the case where we only observe $\bY_1$ by itself.
\end{itemize}
And it is the property of \emph{invariance under bijections},
inclusive of non-linear bijections, which qualifies mutual information
as a \emph{non-linear measure of dependence.}  Linear measures, such
as Pearson correlation, are not invariant under bijections.  

%As for
%the completeness of the five listed properties: as we know, Shannon's
%mutual information (up to arbitrary scaling factor) is the only
%functional proposed in the literature which satisfies all five
%properties.
Besides the formal definition, there are a number of well-known alternative
characterizations of mutual information in terms of other
information-theoretic quantities: the \emph{entropy} $\text{H}$:
\[
\text{H}_\mu(\bX) = -\int p(\bX) \log p(\bX) d\mu(\bX),
\]
and the \emph{conditional entropy}:
\[
\text{H}_\mu(\bX|\bY) = -\int p(\bY) d\mu_y(\bY) \int p(\bX|\bY) \log p(\bX|\bY) d\mu_x(\bX).
\]
Some care needs to be taken with entropy and conditional entropy since
they are not invariant with respect to change-of-measure: hence the
use of the subscript in the notation $\text{H}_\mu$.  In particular,
there is a difference between \emph{discrete entropy} (for counting
measure) and \emph{differential entropy} (for Lesbegue measure.)
Intutively, entropy measures an observer's uncertainty of the random
variable $\bX$, supposing the observer has no prior information other
than the distribution of $\bX$. Conditional entropy measures
the \emph{expected uncertainty} of $\bX$ supposing the observer
observes $\bY$.

However, regardless of the base measure, the following identities hold:
\[
\text{I}(\bX; \bY) = \text{H}_{\mu_x \times \mu_y}((\bX, \bY)) - \text{H}_{\mu_x}(\bX) - \text{H}_{\mu_y}(\bY).
\]
\begin{equation}\label{eq:ce_ident}
\text{I}(\bX; \bY) = \text{H}_\mu(\bY) - \text{H}_\mu(\bY|\bX).
\end{equation}
The second identity \eqref{eq:ce_ident} is noteworthy
as being practically important for estimation of mutual information.
Since the entropies in question only depend on the marginal and
conditional distributions of $\bY$, the problem of estimating
$\text{I}(\bX; \bY)$ can be reduced from a $\dim(\bX)
+ \dim(\bY)$-dimensional nonparametric estimation problem to a
$\dim(\bY)$-dimensional problem: hence this identity is a basis of
several methods of estimation used in neuroscience, such as Gastpar
(2014).

However, by symmetry, we also have the flipped identity
\begin{equation}\label{eq:ce_ident2}
\text{I}(\bX; \bY) = \text{H}_\mu(\bX) - \text{H}_\mu(\bX|\bY).
\end{equation}
In neuroscience studies, where $\bX$ is the controlled stimulus, and
$\bY$ is the neural activity, the two mirror pairs \eqref{eq:ce_ident}
and \eqref{eq:ce_ident2} have different interpretations.  Rather than
providing a basis for practical estimation, \eqref{eq:ce_ident2}
provides an \emph{interpretation} of the mutual information.  Loosely
speaking, $\text{H}_\mu(\bX)$ is the uncertainty of $\bX$ before
having observed $\bY$, and $\text{H}_\mu(\bX|\bY)$ is the uncertainty
of $\bX$ after having observed $\bY$, hence $\text{H}_\mu(\bX)
- \text{H}_\mu(\bX|\bY)$ is how much the observation of $\bY$
has \emph{reduced} the uncertainty of $\bX$.  Stated in words,
\[
\text{I}(\bX; \bY) = \text{average reduction of uncertainty about $\bX$ upon observing $\bY$}.
\]

We list these properties of mutual information in preparation for
section \ref{sec:axiom_info}, where we prepare a ``minimal'' set of
properties for an information coefficient, and consider how much of
the functionality of the mutual information would be preserved by an
alternative information coefficient satisfying only those minimal
properties.

But what, exactly, is the functionality of mutual information in
neuroscience?  How it is used in practice?  A nice summary of the
applications of mutual information is provided in the introduction of
Gastpar (2014).  Taking their list as a starting point, we briefly
overview the main use-cases of mutual information, and illustrate each
with a representative example.
%We explore this concept of ``information as reduction of uncertainty''
%much further when we propose our general class of information coefficients:
%there, we see that properties (i)-(iii) of the mutual information are
%consequences of this second characterization of mutual information.

%But how what is the practical import of these properties of mutual
%information?  Supposing we identify a number of properties as being
%essential for scientific purposes (and others as non-essential), this
%then suggests that alternative measures of information, also
%satisfying the same essential properties, could be just as effective
%for scientific work as mutual information.  And some might have
%additional advantages.

%Towards our goal of formulating a general theory of information
%coefficients, it is important to evaluate exactly \emph{how important} each
%of the listed properties.

\begin{itemize}
\item Example: Comparison of decoders in Nelken.  
Property (i) is important to enable model comparison.  Property (iii)
is needed because relationships may be nonlinear.
\item Example: Redundancy in population code of retina.  
Property (i)-(iii) and (v) are needed to obtain a meaningful measure
of redundancy.
\item In general, symmetry not important, but additivity is desirable 
for measures of redundancy.  Property (ii) can usually be enforced
since any measure needs to have a unique ``minimum'' value for the
case of independence.
\end{itemize}

\subsection{Supervised learning}

\begin{itemize}
\item Supervised learning task is defined using a prediction task.
\item 1. A predictive model is learned using training data
\item 2. The performance of the model on the prediction task is estimated using independent test data
\item Classical examples of prediction tasks: regression and classification
\item Third example: identification
\item Definition of Bayes prediction model
\item General definition of supervised learning task
\item SL performance can be a scalar
\item SL can be used to test for independence
\item Bayes performance satisfies data-processing inequality
\item How SL is interpreted in MVPA as information
\item Section 3, we'll see how Bayes performance can be legitimately considered a measure of information
\end{itemize}

\subsection{Connections}

\begin{itemize}
\item Information theory and decoding. Fano's inequality
\item Quiroga's method
\item MVPA people use them interchangeably
\end{itemize}

\section{Information, uncertainty and prediction}\label{sec:gen_class}

In the previous section, we examined how mutual information and
supervised learning are used in neuroscience.  By analyzing the
use-cases of each method side-by-side, and by reviewing the known
connections between the two methods, we hoped to suggest the notion
that both mutual information and supervised learning are being
employed as means of quantifying the same underlying concept.  In this
section, we propose a reification of this underlying concept of
``information'', and propose the \emph{minimal} properties needed for
a functional to be considered an \emph{information coefficient.}  We
argue that these minimal properties are sufficient to support many of
the existing use-cases of mutual information and supervised learning
in neuroscience.

\subsection{Axiomatic characterization of information}\label{sec:axiom_info}

We claim that neuroscientists use both mutual information and
supervised learning to quantifying a common concept of
``information.''  Furthermore, we claim that neuroscientists largely
share a set of common intuitions about information.

\begin{itemize}
\item \emph{
Intuition 1: Information is a measure of dependence.}   If $\bX$ and $\bY$ are statistically independent, then
$\bX$ gives no information about $\bY$, and vice-versa.
\end{itemize}
This
intuition is employed when researchers test the null hypothesis of
chance accuracy for classification.  If the null is accepted, the
researcher concludes that there is no information in the predictors
about the response.

\begin{itemize}
\item \emph{
Intuition 2a: Monotonicity with respect to inclusion of outputs.} If $\bY_1$ and $\bY_2$ are ensembles of neurons (or
individual neurons), then the combined ensemble $(\bY_1, \bY_2)$ has equal or
more information about $\bX$ than either component by itself.
\end{itemize}

\begin{itemize}
\item \emph{
Intuition 2b: Noise adds no information.} If, in the previous example, $\bY_2$ is independent of
$\bX$, then $\bY_2$ adds no information to the ensemble.
\end{itemize}

The third intuition is somewhat less obvious, but nevertheless appears
as an important justification for the use of mutual information and
supervised learning.

\begin{itemize}
\item \emph{Intuition 3:}
Information can be used as a basis of model selection.  Among multiple
encoding/decoding models, a more accurate model should tend to have
greater information relative to less accurate models.  
\end{itemize}

Given the first two intuitions, we find that an essential property of
information is that it can be said to `increase'--i.e., there exists
at least a partial ordering on information.  Furthermore, there should
exist a minimal element in this ordering, which is the information
between independent variables--that is, `no information.' However,
this is fully consonant with either information being quantified as a
scalar quantity, or as a positive-definite matrix.  Indeed, Fisher
information could be taken as an example of a matrix-valued
information coefficient.  However, the problem with matrix-valued
coefficients is that channels may be incomparable within the partial
ordering.  Thus, if we consider the third intuition an important
property of an information coefficient, then it should be scalar to
enable model comparison.

We find that all of the preceding intuitions follow from the following
axioms.

\noindent\textbf{Axioms of information}

Let $\mathcal{I}(\bX; \bY)$ denote an \emph{information coefficient}.
Then,
\begin{enumerate}
\item $\mathcal{I}(\bX; \bY)$ is a scalar functional of the joint distribution $p(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\mathcal{I}(\bX; \bY) = 0$; otherwise, $\mathcal{I}(\bX; \bY) \geq 0$.
\item 
The stochastic data-processing inequality.  If $\vec{f}$ is stochastic
vector-valued function of the output space indepedent of $(\bX, \bY)$,
then
\[
\mathcal{I}(\bX; \vec{f}(\bY)) \leq \mathcal{I}(\bX; \bY).
\]
\end{enumerate}

Since this minimal list of properties is a subset of the properties of
mutual information, it is clear that mutual information satisfies the
axioms.  However, classification accuracy does not satisfy the
axioms--we will see in section \label{sec:gen_class_sub} how to define
a proper information coefficient based on supervised learning.

Now let us check that any information coefficient
$\mathcal{I}(\bX; \bY)$ necessarily satisfies the intuitions.
Intuition 1 is satisfied by property (ii).  We gave a general argument
in \ref{background_mi} how Intuition 2a follows from the
data-processing inequality.  Intuition 2b follows from the stochastic
data-processing inequality, taking $\vec{f}(\bY_1) = (\bY_1, \bY_2)$.

Now, in order to justify intuition 3, we need to formalize the notion of ``model selection.''


\subsection{Axiomatic characterization of supervised learning}

\subsection{A general class of information coefficients}\label{sec:gen_class_sub}

The general idea is to look at reduction of uncertainty.

\begin{itemize}
\item 
Prediction loss is a measure of uncertainty.  Mean-square loss
corresponds to variance.
\item 
Consider two prediction problems: in one case you are given no side
information, and in the second case you are given $\bY$.  The
difference in risk between the two problems gives a measure of
uncertainty reduction.
\item
In regression, this gives ``variance explained'' as a measure of
information.
\item
In classification, we get a normalized form of Bayes accuracy.
\item
In randomized classification, we get (normalized) average Bayes accuracy.
\item
We can prove that any prediction task yields an information measure
i.e. satisfying the three axioms.
\item
If the no-information risk is known in advance, we say the information
measure is estimable.
\item
Mutual information can be characterized this way, but it is not estimable.
\item
Mutual information can be derived as a limit of linear combinations of
normalized ABA, due to channel coding theorem.  Still not estimable
due to limit property.
\end{itemize}

\section{Statistical inference}\label{sec:inference}

\begin{itemize}
\item Only lower bounds are possible, because...
\end{itemize}

\section{Connections}\label{sec:connections}

\section{Applications}\label{sec:applications}

\section{Discussion}\label{sec:discussion}

\end{document}












The word ``mutual'' is appropriate given the symmetry of
$I(\bX; \bY)$.  However, at the intuitive level it does not
seem \emph{a priori} necessary that the measure of information be
symmetric; indeed, generalizations of mutual information (Verd\'{u}
2015) are asymmetric.  Having a quantitative definition of
information, such as Shannon's $I(\bX;\bY)$, opens the possibility of
computing, estimating or inferring the quantity for $(\bX, \bY)$ pairs
of interest in the natural world.  Classical neuroscience experiments
estimated the mutual information between the reaching angle of a
monkey's arm, $X$, and the average firing rate of particular motor
neurons, $Y$.  Once estimates are available for individual pairs,
cross-pair comparisons are possible.  Scientists can check if motor
neurons in one particular region of the brain are more ``informative''
(on an individual level) than motor neurons in another brain region.
Even more interestingly, one can take an ensemble of neurons as $Y$.
Without collecting any additional data, scientists can compare the
information of individual neurons $Y_1$ and $Y_2$, and compare each to
the information about $X$ carried by the ensemble $(Y_1,Y_2)$.
Comparisons between ensembles motivate thinking about a ``calculus of
information.''  If
\[
I(X; Y_1) + I(X; Y_2) > I(X; (Y_1, Y_2)),
\]
then we say that $Y_1$ and $Y_2$ carry \emph{redundant} information.
If, on the other hand,
\[
I(X; Y_1) + I(X; Y_2) < I(X; (Y_1, Y_2)),
\]
one can say that $Y_1$ and $Y_2$ have \emph{synergy}--the information
carried by the whole is greater than the sum of the information of
each part.









However, the dominance of mutual information in neuroscience started
to wane as new experimental approaches shifted the focus from pairwise
comparisons to questions of \emph{population coding}.  The extreme
reductionist approach fails to account for the complex ways in which
neurons cooperatively encode complex information, and to get a richer
picture of neural dynamics, it becomes necessary to consider
multivariate measures of dependence.  Also, the adoption of
``high-throughput'' recording technologies such as EEG and fMRI
naturally led in the direction of considering the system-level
dynamics of the whole brain.  More importantly, the technology for
studying the relationships between brain structure, function, and
behavior have enabled a wider population of investigators
(psychologists and neurologists) to quantitatively examine brain
activity, but with motivations which tend to be more holistic and
instrumental (e.g. finding neural correlates of mental disorders) when
compared to the reductionist orientation of classical neuroscience.
For both reasons, the demand for multivariate statistical techniques
in neuroscience has increased dramatically in recent years. While the
theoretical properties of mutual information extend gracefully to the
multivariate setting, the difficulty of estimating mutual information
increases exponentially in the dimension in the absence of strong
modeling assumptions [cite].  Partially for this reason, alternative
measures of multivariate dependence started to enjoy increasing usage
in studies of population coding or in systems-level investigations of
the brain: these include measures of linear dependence (canonical
correlation and multivariate $R^2$) and Fisher information.  However,
while both correlation-based statistics and Fisher information may be
easier to estimate than mutual information in high-dimensional
settings, they are both less flexible in terms of capturing nonlinear
relationships.  Correlation-based statistics can only capture linear
dependence, and Fisher information requires the assumption of a
parametric model.











The concept of ``information'' plays a key role in areas as diverse as
game theory, biology, neuroscience, and human engineering.  A random
quantity $\bY$ carries information about $\bX$ if observing $\bY$ reduces
our uncertainty about $\bX$.  In game theory, $\bY$ could be the
opponent's bet, which reveals information about $\bX$, the opponent's
hand.  In neuroscience, $\bY$ is brain activity which correlates to
visual stimulus, $\bX$.  In communications, $\bX$ is a plaintext message
which is coded and transmitted as $\bY$, a series of recieved bits.

