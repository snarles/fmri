\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{xcolor}
\usepackage{multirow}
\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\Cor}{\text{Cor}}

\definecolor{color1}{RGB}{128,13,13}
\definecolor{color2}{RGB}{70,128,13}
\definecolor{color3}{RGB}{13,128,128}
\definecolor{color4}{RGB}{70,13,128}

\title{How many faces can be recognized? Performance extrapolation for
  multi-class classification}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Charles Y.~Zheng \\
  Department of Statistics\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{snarles@stanford.edu} \\
  %% examples of more authors
  \And
  Rakesh ~Achanta \\
  Department of Statistics\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{rakesha@stanford.edu} \\
  \And
  Yuval ~Benjamini \\
  Department of Statistics \\
  Hebrew University\\
  Jerusalem, Israel\\
  \texttt{yuval.benjamini@mail.huji.ac.il}
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
The difficulty of multi-class classification generally increases with
the number of classes.  Using data from a subset of the classes, 
can we predict how well a classifier will scale with an
increased number of classes?  Under the assumption that the classes
are sampled exchangeably, and under the assumption that
the classifier is generative (e.g. QDA or Naive Bayes), we show that the expected accuracy
when the classifier is trained on $k$ classes is the $k-1$st moment
of a \emph{conditional accuracy distribution}, which can be estimated from data.
We discuss estimation approaches based on pseudolikelihood, 
unbiased estimation, and high-dimensional asymptotics.
We compare these methods in simulations and real data.
\end{abstract}

\section{Introduction}

In multi-class classification, one observes pairs $(z, y)$ where $y \in \mathcal{Y} \subset \mathbb{R}^p$ are feature vectors,
and $z$ are unknown labels, which lie in a countable label set $\mathcal{Z}$.  The goal is to construct a classification rule for
predicting the label of a new data point; generally, the classification rule $h: \mathcal{Y} \to \mathcal{Z}$
is learned from previously observed data points.  In many applications of multi-class classification,
such as face recognition or image recognition, the space of potential labels is practically infinite.
However, one considers the classification problem on a finite subset of the labels $\mathcal{Z}_1 \subset \mathcal{Z}$:
for instance, classifying the faces of 100 selected individuals from the population.
At a later time, one might consider a larger (but still finite) classification problem on $\mathcal{Z}_2 \subset \mathcal{Z}$
with $\mathcal{Z}_2 \supset \mathcal{Z}_1$.
In general, consider an infinite sequence of classification problems on subsets $\mathcal{Z}_1 \subset \cdots \subset \mathcal{Z}_t \subset \cdots$.  
Let $S_i$ represent the training data available for the $i$th classification problem,
and let $h^{(i)}: \mathcal{Y} \to \mathcal{Z}_i$ be the learned classification rule.
Define the accuracy for the $i$th problem as
\[
\text{acc}^{(i)} = \Pr[h^{(i)}(Y) = Z|Z \in \mathcal{Z}_i].
\]
where the probability is taken over the joint distribution of $(Z, Y)$.
Using data from only $\mathcal{Z}_k$, can one predict the accuracy achieved on the larger label set $\mathcal{Z}_K$, with $K> k$?  This is the problem of \emph{prediction extrapolation}.

A practical instance of prediction extrapolation occurs in neuroimaging studies,
Kay et al. (2008) obtain fMRI brain scans which record how a single subject's visual cortex responds to natural images.
The label set $\mathcal{Z}$ corresponds to the space of all grayscale photographs of natural images,
and the set $\mathcal{Z}_1$ is a subset of 1750 photographs used in the experiment.
Kay et al. construct a classifier based on a combination of regularized multiple-response regression
and Naive Bayes: they achieve over 0.75 accuracy on the subset of 1750 photographs,
which by itself is already a convincing demonstration of the richness of the information contained in the fMRI scan.
However, it would also be of interest to know what accuracy could be achieved on a larger set of photographs.
Kay et al. calculated (based on exponential extrapolation) that it would take on the order of $10^{9.5}$ photographs
before the accuracy of the model drops below 0.10!  Directly validating this estimate would take immense resources,
so it would be useful to develop the theory needed to understand how to compute such extrapolations
in a principled way. 

However, in the fully general setting, it is impossible on construct
non-trivial bounds on the accuracy achieved on the new classes $\mathcal{Z}_K \setminus \mathcal{Z}_k$
based only on knowledge of $\mathcal{Z}_k$: after all, $\mathcal{Z}_k$ could consist entirely of well-separated classes
while the new classes $\mathcal{Z}_K \setminus \mathcal{Z}_k$ consist entirely of highly inseparable classes, or vice-versa.
Thus, the most important assumption for our theory is that of \emph{exchangeable sampling}.
The labels in $\mathcal{Z}_i$ are assumed to be an exchangeable sample from $\mathcal{Z}$.
The exchangeability further implies that the marginal distributions of $z \in \mathcal{Z}$ 
are equiprobable within every subset $\mathcal{Z}_i$. 
The condition of exchangeability ensures that the separability of random subsets of $\mathcal{Z}$ can be inferred
by looking at the empirical distributions in $\mathcal{Z}_k$, and therefore that some estimate of the achievable
accuracy on $\mathcal{Z}_K$ can be obtained.

Unfortunately, the assumption of exchangeability is clearly violated in a
majority of instances of multi-class classification.  Many multi-class classification problems
have a hierarchical structure, where the initial label set $\mathcal{Z}_1$ corresponds to a coarse-grained partition of the instances, and an expanded label set $\mathcal{Z}_2$ corresponds to a refinement of the partition induced by $\mathcal{Z}_1$:
for instance, $\mathcal{Z}_1$ consists of the categories $\{\text{animal}, \text{vegetable}, \text{mineral}\}$,
while $\mathcal{Z}_2$ consists of subcategories 
$\{\text{mammal}, \text{bird}, \text{insect}, \text{reptile}, \text{fungus}, \text{tree}, \text{flower}, \text{rock}, \text{metal}\}$.
Not only is $\mathcal{Z}_2$ not a superset of $\mathcal{Z}_1$, but the marginal distributions within $\mathcal{Z}_2$ are necessarily more concentrated than the marginals of $\mathcal{Z}_1$.
Many non-hierarchical classification problems are also excluded by the requirement of exchangeability.
Consider the problem of annotating spoken words: the set $\mathcal{Z}_1$ might consist of data from the 100 most common words, while the set $\mathcal{Z}_2$ consists of data from the 1000 most common words.
Exchangeability is violated because the words $z \in \mathcal{Z}$ are not equiprobable, but rather follow a long-tail law.
It would be interesting to extend our theory to the hierarchical setting, or to handle non-hierarchical settings
with non-uniform prior class probabilities, but we leave the subject for future work.

In addition to the assumption of exchangeability, we restrict the set of classifiers considered.
We focus on \emph{generative classifiers}, which are classifiers which work by training
a model separately on each class.  This convenient property 
allows us to characterize the accuracy of the classifier by selectively conditioning on one class at a time:
in section 3, we use this technique to reveal an equivalence between 
the expected accuracies of $\mathcal{Z}_k$ to moments of a common distribution.
This moment equivalence result allows standard approaches in statistics, such as U-statistics and
nonparametric pseudolikelilood, to be directly applied to the extrapolation problem, as we discuss in section 4.
In non-generative classifiers, the classification rule has a joint dependence on the entire set of classes,
and cannot be analyzed by conditioning on individual classes.
Nevertheless, in Section 5, we see that our methods achieve similarly accurate extrapolation for both
generative and non-generative classifiers in real data examples.  Section 6 concludes.

\section{Setting}

\subsection{Prediction extrapolation}

Having motivated the problem of prediction extrapolation,
we now reformulate the problem for notational and theoretical convenience.
Instead of requiring $\mathcal{Z}_k$ to be a random subset of $\mathcal{Z}$ as we did in section 1, take
$\mathcal{Z}=\mathbb{N}$ and $\mathcal{Z}_k = \{1,\hdots, k\}$.
We fix the size of $\mathcal{Z}_k$ without losing generality, since any monotonic sequence of 
finite subsets can be embedded in a sequence with $|\mathcal{Z}_k| = k$.
In addition, rather than randomizing the labels, we will randomize the marginal distribution of each label;
Towards that end, let $\mathcal{Y} \subset \mathbb{R}^p$ be a space of feature vectors, and
let $\mathcal{P}(\mathcal{Y})$ be a measurable space of probability distributions on $\mathcal{Y}$.
Let $\mathcal{F}$ be a probability measure on $\mathcal{P}$,
and let $F_1, F_2,\hdots$ be an infinite sequence of i.i.d. draws from $\mathbb{F}$.
We refer to $\mathbb{F}$, a probability measure on probability measures, as a \emph{meta-distribution}.
The distributions $F_1,\hdots, F_k$ are the marginal distributions of the first $k$ classes.
We therefore rewrite the accuracy as
\[
\text{acc}^{(i)} = \frac{1}{t}\sum_{i=1}^t \Pr_{F_i}[h^{(t)}(Y) = i].
\]
where the probabilities are taken over $Y \sim F_i$.

In order to construct the classification rule $h^{(t)}$, we need data from the classes $F_1,\hdots, F_t$.
In most instances of multi-class classification, one observes independent observations from each $F_i$
which are used to construct the classifier.  Since the order of the observations
does not generally matter, a sufficient statistic for the training data for the $t$th classification problem
is the collection of empirical distributions
$\hat{F}_1^{(t)},\hdots,\hat{F}_t^{(t)}$ for each class.
Henceforth, we make the simplifying assumption that the training data for the $i$th class remains fixed
from $t =i, i+1,\hdots$, so we drop the superscript on $\hat{F}_i^{(t)}$.
Write $\hat{\mathbb{F}}(F)$ for the conditional distribution of $\hat{F}_i$ given  $F_i = F$;
also write $\hat{\mathbb{F}}$ for the marginal distribution of $\hat{F}$ when $F \sim \mathbb{F}.$
As an example, suppose every class has the number of training examples $r \in \mathbb{N}$; then $\hat{F}$
is the empirical distribution of $r$ i.i.d. observations from $F$, and $\hat{\mathbb{F}}(F)$ is the \emph{empirical meta-distribution} of $\hat{F}$.
Meanwhile, $\hat{\mathbb{F}}$ is the meta-distribution of the empirical distribution of $r$ i.i.d. draws from a random $F \sim \mathbb{F}$.


\subsection{Multi-class classification}


Combining the formalism of Tewari and Bartlett (2007),
we define a classifier as a collection of mappings
$\mathcal{M}_i: \mathcal{P}(\mathcal{Y})^k \times \mathcal{Y} \to \mathbb{R}$ called \emph{margin functions.}
Intuitively speaking, each margin function \emph{learns a model} from the first $k$ arguments, which are
the empirical marginals of the $k$ classes, which it uses to assign a \emph{margin} or \emph{score} to the
\emph{query point} $y \in \mathcal{Y}$.  A higher score $\mathcal{M}_i(\hat{F}_1,\hdots, \hat{F}_k, y)$ indicates a higher estimated probability that $y$ belongs to the $k$th class.  
Therefore, the classification rule corresponding to a classifier $\mathcal{M}_i$ assigns
a class with maximum margin to $y$:
\[
h(y) = \argmax_{i \in \{1,\hdots, k\}} \mathcal{M}_i(y).
\]
For our purposes, it is not important how ties are resolved.  We will also neglect discussion of randomized classifiers,
except to mention that they can be treated in our framework as probability distributions over deterministic classifiers;
we also neglect the incorporation of prior class probabilities into the classifier, since in our setting
the prior class probabilities are uniform.

One reason why we formalize a classifier in terms of \emph{empirical distributions} rather than data points
is to formalize the notion of \emph{continuity}.
The level sets $\{y: \argmax_i \mathcal{M}_i(\hat{F}_1,\hdots, \hat{F}_k, y) = k\}$ are called
\emph{decision regions.}  We say the classifier is \emph{continuous}
if and only if the \emph{decision regions} of $\mathcal{M}_{i=1}^k$ are
continuous in the first $k$ arguments with respect to the topology of
weak convergence.

To acquaint the reader with our formalism, we redefine a number of familiar classifiers according to our notation.

\noindent\emph{Example 1. (OVO)} Let $\mathcal{B}$ (the \emph{base classifier}) be a binary-valued mapping with three arguments: distributions $\hat{F}_0$,
$\hat{F}_1$, and query $y$.  A one-vs-one (OVO)
classifier is defined by
\[
\mathcal{M}_i(\hat{F}_1,\hdots, \hat{F}_k, y) = \sum_{j \neq i} \mathcal{B}(\hat{F}_j, \hat{F}_i, y).
\]

\noindent\emph{Example 2. (OVA)}  Let $\mathcal{D}$ (\emph{the base classifier}) be a real-valued mapping with three arguments: 
distributions $F_0$, $F_1$, and query $y$.  A one-vs-all (OVA) classifier is defined by
\[
\mathcal{M}_i(\hat{F}_1,\hdots, \hat{F}_k, y) = \mathcal{D}(\hat{F}_i, \frac{1}{k}\sum_{j = 1}^k \hat{F}_j, y).
\]

It is immediate that an OVO classifier is continuous if and only if the level sets of $\mathcal{B}$ are continuous with respect
to the topology of weak convergence,
and similarly that an OVA classifier is continuous if and only if $\mathcal{D}$ is continuous.

\noindent\emph{Example 3. ($\epsilon$-NN)}  
$\epsilon$-nearest neighbors can be thought of as $k$-nearest neighbors with $k = \epsilon n$ for fixed $\epsilon$.
Let $d$ be a distance metric on $\mathcal{Y}$.  
Let $D(y)$ denote the induced distribution of $d(Y, y)$ when $Y \sim \frac{1}{k}\sum_{i=1}^k \hat{F}_i$,
and let $d_{\epsilon}(y)$ denote the $\epsilon$-quantile of $D(y)$. 
An $\epsilon$-nearest neighbor classifier is defined by
\[
\mathcal{M}_i(\hat{F}_1,\hdots, \hat{F}_k, y) = \Pr_{Y \sim \hat{F}_i}[d(Y, y) < d_{\epsilon}(y)].
\]

Note that the $\epsilon$-NN classifier is also an example of a OVA classifier, with 
$\mathcal{D}(F_0, F_1, y) = F_1(B_{d_{\epsilon}(y)})$, where $B_r$ is the $d$-ball of radius $r$,
since one can define $d_{\epsilon, t}(y) = \sup_r \text{ s.t. }\frac{k-1}{k}F_0 + \frac{1}{k}F_1)(B_r(y)) \leq \epsilon$.

\noindent\emph{Example 4. (Multinomial logistic regression.)}   
Assume WLOG that $y_1 = 1$ for all $y \in \mathcal{Y}$, and let $B$ be a $p \times k$ matrix which minimizes the objective function
\[
-\E_{\hat{F}_j}\left[\langle Y, B_j \rangle - \log\left[\sum_{\ell=1}^{k} \exp[\langle Y, B_\ell \rangle]\right]\right].
\]
A multinomial logistic regression classifier is defined by
\[
\mathcal{M}_i(\hat{F}_1,\hdots, \hat{F}_k, y) = \langle y, B_i \rangle.
\]
By the convexity of the objective function, multinomial logistic regression is continuous wherever the minimizer $B$
is unique.

Throughout these examples we have neglected to discuss model selection or parameter tuning.
It is admittedly non-trivial to formalize procedures such as cross-validation
using our formalism, and since model selection lies beyond the scope of our paper,
we omit the definitions.

\subsection{Generative classifiers}

For some classifiers, the margin function $\mathcal{M}_i$ is especially simple
in that $\mathcal{M}_i$ is only a function of $\hat{F}_i$ and $y$.
Furthermore, due to symmetry, in such cases one can write
\[
\mathcal{M}_i(\hat{F}_1,\hdots, \hat{F}_k, y) = \mathcal{Q}(\hat{F}_i, y)
\]
where $\mathcal{Q}$ is called a \emph{single-class margin} (or simply \emph{margin}.)
For notational convenience, we assume that ties occur with probability
zero: that is, $\hat{\mathbb{F}}$ and $\mathcal{Q}$ jointly satisfy the
\emph{tie-breaking} property:
\begin{equation}\label{eq:tie}
\Pr[\mathcal{Q}(\hat{F}, y) = \mathcal{Q}(\hat{F}', y)] = 0.
\end{equation}
for all $y \in \mathcal{Y}$, where $\mathbb{F}, \mathbb{F}' \stackrel{iid}{\sim} \hat{\mathbb{F}}$.
Quadratic discriminant analysis and Naive Bayes are two examples of
generative classifiers.  For QDA, the margin is given by
\[
\mathcal{Q}_{QDA}(\hat{F}, y) = -(y - \mu(\hat{F}))^T \Sigma(\hat{F})^{-1} (y-\mu(\hat{F})) - \log\det(\Sigma(\hat{F}))
\]
where $\mu(F) = \int y dF(y)$ and $\Sigma(F) = \int (y-\mu(F))(y-\mu(F))^T dF(y)$.
In Naive Bayes, the margin is
\[
\mathcal{Q}_{NB}(\hat{F},  y) = \sum_{i=1}^n \log \hat{f}_i(y_i)
\]
where $\hat{f}_i$ is a density estimate for the $i$th component of
$\hat{F}$.

The \emph{generative} property allows us to prove strong results about the accuracy of the classifier
under the exchangeable sampling assumption.  For starters, we can show that the
expected accuracy follows a \emph{mixed exponential decay}, as stated by the following theorem.

\noindent\textbf{Theorem 2.1}
\emph{
Let $\mathcal{Q}$ be the scoring function of a generative classifier, and 
assume that $F_1,\hdots, F_k \stackrel{iid}{\sim} \mathbb{F}$ and $\hat{F}_i \sim \hat{\mathbb{F}}(F_i)$ independently,
following the notation of section 2.
Further assume that $\mathcal{Q}$ and $\mathbb{F}$ satisfy the tie-breaking property \eqref{eq:tie}.
Then, recalling the definition of accuracy,
\[
\text{acc}^{(t)} = \frac{1}{t}\sum_{i=1}^t \Pr_{Y \sim F_i}[\mathcal{Q}(\hat{F}_i, Y) > \argmax_{i > j} \mathcal{Q}(\hat{F}_i, Y)],
\]
there exists a measure $\alpha$ on $[0, \infty)$ such that
\[
\E[\text{acc}^{(t)}] = \int_{\mathbb{R}^{+}} e^{\kappa t} d\alpha(\kappa).
\]
}

(We give the proof in section 3.)

The theorem immediately suggests a method for predicting $\text{acc}^{(K)}$:
fit a mixed exponential decay to $a(t) = \text{acc}^{(t)}$ for $t = 2,\hdots, k$ and extrapolate the curve to $t = K$.
This is just one of many methods which can be developed for generative classifiers,
which we discuss more fully in Section 3 and 4.


\section{Prediction extrapolation for generative classifiers}

Let us specialize to the case of a generative classifier, with scoring rule $\mathcal{Q}$.
Consider estimating the expected accuracy at time $t$, \[p_t
\stackrel{def}{=} \E[\text{acc}^{(t)}].\]

Define the \emph{conditional accuracy} function $u(\hat{F}, y)$ which maps a
distribution $\hat{F}$ on $\mathcal{Y}$ and a \emph{test} observation $y$ to
a real number in $[0,1]$.  The conditional accuracy gives the
probability that for independently drawn $\hat{F}'$ from $\hat{\mathbb{F}}$, that
$\mathcal{Q}(\hat{F}, y)$ will be greater than $\mathcal{Q}(\hat{F}', y)$:
\[
u(\hat{F}, y) = \Pr_{\hat{F} \sim \hat{\mathbb{F}}}[\mathcal{Q}(\hat{F}, y) > \mathcal{Q}(\hat{F}', y)].
\]
Define the \emph{conditional accuracy} distribution $\nu$ as the law
of $u(\hat{F}, Y)$ where $\hat{F}$ and $Y$ are generated as follows:
(i) a true distribution $F$ is drawn from $\mathbb{F}$; (ii) the query $Y$ is drawn from $F$, 
and (iii) the empirical distribution $\hat{F}$ is drawn from $\hat{\mathbb{F}}(F)$ 
(e.g., the distribution of the empirical distribution of $r$ i.i.d. observations drawn from $F$),
with $Y$ independent of $\hat{F}$.  The significance of the conditional accuracy
distribution is that the expected generalization error $p_t$ can be
written in terms of its moments.

\noindent\textbf{Theorem 3.1.} \emph{
Let $\mathcal{Q}$ be a single-distribution margin, and let $\mathbb{F}$, $\hat{F}(F)$ be a distribution on $\mathcal{P}(\mathcal{Y}).$
Let $U$ be defined as the random variable
\[
U = u(\hat{F}, Y)
\]
for $F \sim \mathbb{F}$, $Y \sim F$, and $\hat{F} \sim \hat{\mathbb{F}}(F)$ with $Y \perp \hat{F}$.
Recall the definition
\[
p_k = \E[acc^{(k)}] = \E\left[\frac{1}{t}\sum_{i=1}^k \Pr_{Y \sim F_i}[\mathcal{Q}(\hat{F}_i, Y) > \max_{j \neq i}\mathcal{Q}(\hat{F}_j, Y)]\right].
\]
Then \[p_k = \E[U^{k-1}].\]
}

\noindent\textbf{Proof.}  
Write $q^{(i)}(y) = \mathcal{Q}(\hat{F}_i, y)$.
By using conditioning and
conditional independence, $p_k$ can be written
\begin{align*}
p_k &= \E\left[ \frac{1}{k}\sum_{i=1}^k  \Pr_{F_i}[q^{(i)}(Y) > \max_{j\neq i} q^{(j)}(Y)] \right]
\\&= \E\left[ \Pr_{F_1}[q^{(1)}(Y) > \max_{j\neq 1} q^{(j)}(Y)] \right]
\\&= \E_{F_1}[\Pr[q^{(1)}(Y) > \max_{j\neq 1} q^{(j)}(Y)|\hat{F}_1, Y]]
\\&= \E_{F_1}[\Pr[\cap_{j > 1} q^{(1)}(Y) > q^{(j)}(Y)|\hat{F}_1, Y]]
\\&= \E_{F_1}[\prod_{j > 1}\Pr[q^{(1)}(Y) > q^{(j)}(Y)|\hat{F}_1, Y]]
\\&= \E_{F_1}[\Pr[q^{(1)}(Y) > q^{(2)}(Y)|\hat{F}_1, Y]^{k-1}]
\\&= \E_{F_1}[u(\hat{F}_1, Y)^{k-1}] = \E[U^{k-1}].
\end{align*}
$\Box$

Theorem 3.1 tells us that the problem of extrapolation can be
approached by attempting to estimate the conditional accuracy
distribution.  The $(t-1)$th moment of $U$ gives us $p_t$, which will
in turn be a good estimate of $\text{acc}^{(t)}$.

\subsection{Properties of the conditional accuracy distribution}

The conditional error distribution $\nu$ is determined by $\mathbb{F}$
and $\mathcal{Q}$.  What can we say about the the conditional accuracy
distribution without making any assumptions on either $\mathbb{F}$ or
$\mathcal{Q}$?  The answer is: not much--for an arbitrary probability
measure $\nu'$ on $[0,1]$, one can construct $\mathbb{F}$ and
$\mathcal{Q}$ such that $\nu = \nu'$.

\noindent\textbf{Theorem 3.2.} \emph{ Let $U$ be defined as in Theorem
  2.1, and let $\nu$ denote the law of $U$.  Then, for any probability
  distribution $\nu'$ on $[0,1]$, one can construct a
  meta-distribution $\mathbb{F}$ and a scoring rule $\mathcal{Q}$ such
  that $\nu = \nu'$.  }

In practice, however, the scoring rule $\mathcal{Q}$ must approximate
a monotonic function of the conditional density $f = \frac{dF}{dy}$ in order to
yield an effective classifier.

It is therefore notable that in the case that $F$ has a density with
respect to Lesbegue measure, and where $\mathbb{F}$ has no atoms,
taking an \emph{optimal} scoring rule, with the property that
$\mathcal{Q}(\hat{F}, y) = g(f(y))$ for monotonic $g$, the
distribution of $U$ has a monotonically increasing density.

\noindent\textbf{Theorem 3.3.} \emph{ Let $U$ be defined as in Theorem
  3.1, and let $\nu$ denote the law of $U$.  Suppose $F$ has a density
  $f(y)$ with respect to Lebesgue measure on $\mathcal{Y}$ with
  probability one, $\mathbb{F}$ has no atoms, and ($\mathbb{F}$,
  $\mathcal{Q}$) jointly satisfy the property of monotonicity
  \[
  f(y) > f(y') \text{ implies } \mathcal{Q}(\hat{F}, 0, y) > \mathcal{Q}(\hat{F}, 0, y')
  \]
  and the property of tie-breaking \eqref{eq:tie} with probability one.
  Then $\mu$ has a density $\eta(u)$ on $[0, 1]$ which is monotonic in $u$.
}

\section{Nonparametric Estimation}

Let us assume that $U$ has a density $\eta(u)$.  While $U = u(\hat{F},
0, Y)$ cannot be directly observed, we can estimate $u(\hat{F}_i, 0,
y^{(i), r_1 + j})$ for any $1 \leq i \leq k$, $1 \leq j \leq r_2$ from
the data.

\noindent\textbf{Theorem 4.1.}\emph{
For given $p(x, y)$ and scoring rule $\mathcal{Q}$, assume that $U$ as defined in Theorem 3.1 has a density $\eta(u)$
and that $\mathcal{Q}$ satisfies the tie-breaking property \eqref{eq:tie}.
Define
\[
V_{i, j} = \sum_{i=1}^k I(q^{(i)}(y^{(i), j}) > q^{(j)}(y^{(i), j})).
\]
Then
\[
V_{i, j} \sim \text{Binomial}(k, u(\hat{F}_i, y^{(i), j})).
\]}

At a high level, we have a hierarchical model where $U$ is drawn from a density $\eta(u)$ on $[0, 1]$
and then $V_{i, j} \sim \text{Binomial}(k, U)$;
therefore the marginal distribution of $V_{i, j}$ can be written
\[
\Pr[V_{i,j} = \ell] = \begin{pmatrix}
k \\ \ell
\end{pmatrix}
\int_0^1 u^\ell (1-u)^{k-\ell} \eta(u) du.
\]
However, the observed $\{V_{i, j}\}$ do \emph{not} comprise an i.i.d. sample.

We discuss the following three approaches for estimating $p_t =
\E[U^{t-1}]$ based on $V_{i, j}$.  The first is \emph{unbiased
  estimation} based on binomial U-statistics, which is discussed in
Section 4.1.  The second is the \emph{psuedolikelihood} approach.  In
problems where the marginal distributions are known, but the
dependence structure between variables is unknown, the
\emph{psuedolikelihood} is defined as the product of the marginal
distributions.  For certain problems in time series analysis and
spatial statistics, the maximum psuedolikelihood estimator (MPLE) is
proved to be consistent (CITE).  We discuss psuedolikelihood-based
approaches in Sections 4.2 and 4.3.  

\subsection{Unbiased estimation}

If $V \sim \text{Binomial}(k, \eta)$, then an unbiased estimator $f_t(V)$ of $\eta^(t-1)$ exists
if and only if $0 \leq t \leq k$.

The theory of U-statistics provides the minimal variance unbiased estimator for $\eta^(t-1)$:
\[
\eta^t = \E\left[\frac{\begin{pmatrix}
V \\ t
\end{pmatrix}}{\begin{pmatrix}
k \\ t
\end{pmatrix}}\right].
\]

This result can be immediately applied to yield an unbiased estimator of $p_t$, when $t \leq k$:
\begin{equation}\label{eq:ustat}
\hat{p}_t^{UN} = \E\left[ \frac{1}{kr_2}\sum_{i=1}^k\sum_{j=1}^{r_2} \frac{\begin{pmatrix}
V_{i, j} \\ t
\end{pmatrix}}{\begin{pmatrix}
k \\ t
\end{pmatrix}} \right].
\end{equation}
The problem of \emph{extrapolation} concerns the case $t > k$, in
which the expression \eqref{eq:ustat} is undefined.  Still, the
estimator \eqref{eq:ustat} is worthy of study, since it has close to
optimal performance for the case $t \leq k$.

\subsection{Maximum pseudo-likelihood}

The psuedolikelihood is defined as
\begin{equation}\label{eq:psuedo}
\ell_t(\eta) = \sum_{i=1}^k \sum_{j=1}^{r_1} \log\left(\int u^{V_{i, j}} (1-u)^{k - V_{i, j}} \eta(u) du\right),
\end{equation}
and a maximum psuedolikelihood estimator (MPLE) is defined as any
density $\hat{\eta}$ such that
\[
\ell(\hat{\eta}_{MPLE}) = \sup_{\eta} \ell_t(\eta).
\]
The motivation for $\hat{\eta}_{MPLE}$ is that it consistently
estimates $\eta$ in the limit where $k \to \infty$.

\noindent\textbf{Theorem 4.2.}  \emph{ For given $\mathbb{F}$ and scoring
  rule $\mathcal{Q}$, assume that $U$ as defined in Theorem 3.1 has a
  density $\eta(u)$ and that $\mathcal{Q}$ satisfies the tie-breaking
  property \eqref{eq:tie}, and also that $r_2 \geq 1$.  For $t = 1, 2,
  \hdots, $, let $\hat{\eta}_t$ be any MPLE for $\ell_t$.  As $k_t \to
  \infty$, $\hat{\eta}_t$ weakly converges to $\eta$.}

However, in finite samples, $\hat{\eta}_{MPLE}$ is not uniquely defined,
and if we define the plug-in estimator
\[
\hat{p}_t^{MPLE} = \int u^{t-1} \hat{\eta}_{MPLE}(u) du,
\]
$\hat{p}_t^{MPLE}$ can vary over a large range, depending on which $\hat{\eta} \in \argmax_{\eta} \ell_t(\eta)$
is selected.
These shortcomings motivate the adoption of additional constraints on the estimator $\hat{\eta}.$

\subsection{Constrained pseudo-likelihood}

Theorem 3.2. motivates the \emph{monotonicity constraint} that $\frac{d\hat{\eta}}{du} > 0$,
hence we define $\hat{\eta}_{INC}$ as a solution to
\[
\text{maximize }\ell_t(\eta) \text{ subject to }\frac{d\hat{\eta}}{du} > 0.
\]
An alternative strategy is to directly attack the variability is $\hat{p}_t$ due to non-uniqueness of $\hat{\eta}$.
Therefore, we define $\hat{\eta}_{MC}$ (where MC stands for moment-constrained)
as
\[
\text{maximize }\ell_t(\eta) \text{ subject to }\int u^{k-1} \eta(u) du = \hat{p}_k^{UN}.
\]
Thirdly, we can combine both the moment constraint and the monotonicity constraint, yielding
$\hat{\eta}_{COM}$, which is obtained by solving
\[
\text{maximize }\ell_t(\eta) \text{ subject to }\int u^{k-1} \eta(u) du = \hat{p}_k^{UN}\text{ and }\frac{d\hat{\eta}}{du} > 0.
\]
Unfortunately, none of the three density estimators are uniquely defined.
An easy way to see this is to transform the parameterization of $\eta(u)$,
defining
\[
\eta(u) = \int_0^u \xi(u) du;
\]
the monotonicity constraint is equivalent to the condition that $\xi > 0$,
and the moment condition translates into a linear equality constraint on $\xi$.



\section{Results}

\begin{figure}
\centering
\includegraphics[scale = 0.6]{gabor_random_Y.pdf}
\caption{Extrapolation classification performance for CIFAR data.  (This simulation needs to be fixed later.)
PMLE: maximum psuedolikelihood. MCPMLE: Moment-constrained max psuedolikelihood.  Info: Zheng and Benjamini's info-theoretic method.
Unbiased: U-statistic (cannot be used to extrapolate.) }
\end{figure}

\section{Discussion}


\subsubsection*{Acknowledgments}

CZ is supported by an NSF graduate research fellowship.

\section*{References}

\small

[X] Ng, Andrew Y., and Michael I. Jordan. "On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes." (2002).

[X] Naselaris, T., Kay, K. N., Nishimoto, S., \& Gallant,
J. L. (2011). Encoding and decoding in fMRI. \emph{Neuroimage}, 56(2),
400-410.

[X] Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. \emph{The elements
of statistical learning.} Vol. 1. Springer, Berlin: Springer series in
statistics, 2008.

\end{document}








Unfortunately, the majority of classifiers used in practice are \emph{not} generative,
and for good reason.  Generative classifiers construct decision boundaries out of estimated marginal distributions,
so while a correctly specified marginal model can lead to an extremely efficient classifier,
a misspecified marginal model leads to extremely poor decision boundaries.
In contrast, discriminative classifiers directly search for good classification boundaries based on the data,
making them more robust to the data generating process, and as a result,
in most problems, discriminative classifiers outperform generative classifiers (Ng 2002).
Despite this fact, it can be shown that many non-generative classifiers are \emph{asymptotically generative},
meaning that i$\mathcal{M}_i^{(t)}$, the $i$th margin for the $t$th classification problem in the sequence,
converges to a function of $\hat{F}_i$ and $y$ with probability one:
\begin{equation}\label{eq:agen}
\lim_{t \to \infty} \mathcal{M}_i^{(t)}(\hat{F}_1,\hdots, \hat{F}_t, y) = \mathcal{Q}(\hat{F}_i, y) \text{ w.h.p. }
\end{equation}
While asymptotically generative classifiers share the properties of generative classifiers with respect to
prediction extrapolation, they need not share the same \emph{limitations} as generative classifiers.
A generative classifier requires the user to specify the scoring function $\mathcal{Q}$ in advance:
this practically requires some prior knowledge about the marginal distributions of the classes,
e.g. that the marginal distributions are approximately multivariate Gaussian.
But the scoring function $\mathcal{Q}$ appearing in the definition of an asymptotically generative classifier
\emph{need not be specified} by the user: 
it can (and usually does) depend on the unknown meta-distribution $\mathbb{F}$.
Therefore one can think of asymptotically generative classifiers as generative classifiers with `adaptive' $\mathcal{Q}$,
which suggests that the class of asymptotically generative classifiers could be quite diverse.

Ideally, we could extend Theorem 2.2 for asymptotic generative classifiers, but a stronger condition is needed.
Note that under the tie-breaking assumption, one can always take $\mathcal{Q}$ in \eqref{eq:agen} so that
\[
\Pr[\mathcal{Q}(\hat{F}, y) < u] = u\text{ for }u \in [0,1],
\]
for all $y \in \mathcal{Y}$: we refer to such $\mathcal{Q}$ as \emph{calibrated.}
We define a \emph{uniformly asymptotic generative} classifier by adding conditions on the worst-case rate that
$\mathcal{M}_i$ converges to calibrated $\mathcal{Q}$: these additional conditions are sufficient for the generalization of Theorem 2.3.  We now give the definition and the theorem.

\textbf{Definition 2.1.}  \emph{(Uniformly asymptotically generative) Suppose classifier $\mathcal{M}$ satisfies \eqref{eq:agen} for calibrated $\mathcal{Q}$, and furthermore that
\[
\lim_{t \to \infty} \sup_{i \leq t} \frac{1}{t}\sum_{j=1}^t \Pr_{F_j}[\mathcal{M}(\hat{F}_1,\hdots, \hat{F}_t, Y) - \mathcal{Q}(\hat{F}_i, Y)| > \epsilon] \to 0
\]
with probability one for all $\epsilon > 0$.  Then we say that $\mathcal{M}$ is uniformly asymptotically generative.}

\textbf{Theorem 2.3}\emph{
Let $\mathcal{Q}$ be the scoring function of a uniformly generative classifier, and 
assume that $F_1,\hdots, F_k \stackrel{iid}{\sim} \mathbb{F}$ and $\hat{F}_i \sim \hat{\mathbb{F}}(F_i)$ independently,
following the notation of section 2.
Further assume that $\mathcal{Q}$ and $\mathbb{F}$ satisfy the tie-breaking property \eqref{eq:tie}.
Then, recalling the definition of accuracy,
\[
\text{acc}^{(t)} = \frac{1}{t}\sum_{i=1}^t \Pr_{Y \sim F_i}[\mathcal{M}_i(\hat{F}_1,\hdots,\hat{F}_t, Y) > \argmax_{i > j} \mathcal{M}_j(\hat{F}_1,\hdots, \hat{F}_t, Y)],
\]
there exists a measure $\alpha$ on $[0, \infty)$ such that
\[
\E[\text{acc}^{(t)}] = \int_{\mathbb{R}^{+}} e^{\kappa t} d\alpha(\kappa).\]
}

