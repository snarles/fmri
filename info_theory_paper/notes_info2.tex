\title{Upper bounds for average Bayes accuracy in terms of mutual information}
\author{Charles Zheng and Yuval Benjamini}
\date{\today}

\documentclass[12pt]{article} 

% packages with special commands
\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}
\definecolor{grey}{rgb}{0.5,0.5,0.5}

\begin{document}
\maketitle

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}

These are preliminary notes.

\section{Introduction}

Suppose $X$ and $Y$ are continuous random variables (or vectors) which have a joint distribution with density $p(x, y)$.
Let $p(x) = \int p(x,y) dy$ and $p(y) = \int p(x,y) dx$ denote the respective marginal distributions,
and $p(y|x) = p(x,y)/p(x)$ denote the conditional distribution.

Mutual information is defined
\[
\text{I}[p(x, y)] = \int p(x, y) \log \frac{p(x, y)}{p(x)p(y)} dx dy.
\]

$\text{ABE}_k$, or $k$-class Average Bayes accuracy is defined as follows.  Let $X_1,...,X_K$ be iid from $p(x)$,
and draw $Z$ uniformly from $1,..,k$.  Draw $Y \sim p(y|X_Z)$.
Then, the average Bayes accuracy is defined as
\[
\text{ABA}_k[p(x, y)] = \sup_f \Pr[f(x_1,...,x_k, y) = Z] 
\]
where the supremum is taken over all functions $f$.  A function $f$ which achieves the supremum is
\[
f_{Bayes}(x_1,...,x_k, y) = \text{argmax}_{z \in \{1,...,k\}} p(y|x_z),
\]
where an arbitrary rule can be employed to break ties.
Such a function $f_{Bayes}$ is called a \emph{Bayes classification rule}.
It follows that $\text{ABA}_k$ is given explicitly by
\[
\text{ABA}_k = \frac{1}{k} \int \left[\prod_{i=1}^k p(x_i) dx_i \right] \int dy \max_i p(y|x_i).
\]

\section{Problem formulation}

Let $\mathcal{P}$ denote the collection of all joint densities $p(x, y)$ on finite-dimensional Euclidean space.
For $\iota \in [0,\infty)$ define $C_k(\iota)$ to be the largest $k$-class average Bayes error attained by any distribution $p(x,y)$ with mutual information not exceeding $\iota$:
\[
C_k(\iota) = \sup_{p \in \mathcal{P}: \text{I}[p(x,y)] \leq \iota} \text{ABA}_k[p(x,y)].
\]
A priori, $C_k(\iota)$ exists since $\text{ABA}_k$ is bounded between
0 and 1.  Furthermore, $C_k$ is nondecreasing since the domain of the
supremum is monotonically increasing with $\iota$.

It follows that for any density $p(x,
y)$, we have
\[
\text{ABA}_k[p(x,y)] \leq C_k(\text{I}[p(x,y)]).
\]
Hence $C_k$ provides an upper bound for average Bayes error in terms of mutual information.

Conversely we have
\[
\text{I}[p(x,y)] \geq C^{-1}_k(\text{ABA}_k[p(x,y)])
\]
so that $C^{-1}_k$ provides a lower bound for mutual information in terms of average Bayes error.

On the other hand, there is no nontrivial \emph{lower} bound for average Bayes error in terms of mutual information,
nor upper bound for mutual information in terms of average Bayes error, since
\[
\inf_{p \in \mathcal{P}: \text{I}[p(x,y)] \leq \iota} \text{ABA}_k[p(x,y)] = \frac{1}{k}.
\]
regardless of $\iota$.

The goal of this work is to attempt to compute or approximate the functions $C_k$ and $C_k^{-1}$.

\section{Special case}

We work out the special case where $p(x,y)$ lies on the unit square, and $p(x)$ and $p(y)$ are both the uniform distribution.
Let $\mathcal{P}^{unif}$ denote the set of such distributions, and 
\[
C_k^{unif}(\iota) = \sup_{p(x, y) \in \mathcal{P}^{unif}: \text{I}[p] \leq \iota} \text{ABA}_k[p]. 
\]


In this case, letting $X_1,...,X_k \sim \text{Unif}[0,1]$, and $Y \sim \text{Unif}[0,1]$ define $Z_i(y) = p(y|X_i)$.
We have $\E(Z(y)) = 1$ and,
\[
\text{I}[p(x,y)] = \E(Z(Y) \log Z(Y))
\]
while
\[
\text{ABA}_k[p(x,y)] = k^{-1}\E(\max_i Z_i(Y)).
\]

Letting $g_y$ be the density of $Z(y)$, we have
\[
\text{I}[p(x,y)] = \E(-H[g_Y])
\]
and
\[
\text{ABA}_k[p(x,y)] = \E(\psi_k[g_Y])
\]
where
\[
H[g] = -\int g(x) x \log x dx
\]
and
\[
\psi_k[g] = \int x g(x) G(x)^{k-1} dx
\]
for $G(x) = \int_0^x g(t) dt.$
Additionally $g_y$ satisfies the constraint $\int x g(x) dx= 1$ since $\E[Z(y)] = 1$.

Define the set $D = \{(\alpha, \beta)\}$ as the set of possible values of $(-H[g], \psi_k[g])$ taken over all distributions $g$ supported on $[0,
\infty)$ with $\int x g(x) dx = 1$.  Next, let $\mathcal{C}(D)$ denote the convex hull of $D$.
It follows that $(\text{I}[p], \text{ABA}_k[p]) \in \mathcal{C}(D)$ since the pair is obtained via a convex average of points $(-H[g_y], \psi_k[g])$.

Define the upper envelope of $D$ as the curve
\[
d_k(\alpha) = \sup\{\beta: (\alpha, \beta) \in D\}.
\]

We make the claim (to be shown in the following section) that $d_k(\alpha)$ is convex in $\alpha$.
As a result, the upper envelope of $D$ is also the upper envelope of $\mathcal{C}(D)$.
This in turn implies that $C_k^{unif}(\iota) = d_k(\iota)$.
We establish these results, along with a open-form expression for $C_k^{unif}$, in the following section.

\subsection{Variational methods}

Consider the quantile function $Q(t) = G^{-1}(t).$  $Q(t)$ must be a continuous function from $[0,1]$ to $[0,\infty).$
We can rewrite the moment constraint $\E[g]=1$ as
\[
\int_0^1 Q(t) dt = 1.
\]
Meanwhile, $\beta = \psi_k[g]$ takes the form
\[
\beta = \int_0^1 Q(t) x^{k-1} dt.
\]
and $\alpha = -H[g]$ takes the form
\[
\alpha = \int_0^1 Q(t) \log Q(t) dt.
\]
To find the upper envelope, it will be useful to write the Langrangian
\begin{align*}
\mathcal{L}[g] &= \lambda \int_0^1 Q(t) dt + \mu \int_0^1 Q(t) x^{k-1} dt + \lambda \int_0^1 Q(t) \log Q(t) dt
\\&= \int_0^1 Q(t) (\lambda + \mu x^{k-1} + \nu \log Q(t)) dt.
\end{align*}

In order for a quantile function $Q(t)$ to be on the upper envelope, it must be a local maximum of $-H$ with respect to small perturbations.  Therefore, consider the functional derivative
\[
D[\xi] = \lim_{\epsilon \to 0} \frac{\mathcal{L}[g + \epsilon \xi] - \mathcal{L}[g]}{\epsilon}.
\]
We have
\[
D[\xi] = \int_0^1 \xi(t) (\lambda + \nu  + \mu x^{k-1} + \nu \log Q(t)) dt.
\]

Now consider the following three cases:
\begin{itemize}
\item $Q(t)$ is strictly monotonic, i.e. $Q'(t) > 0.$
\item $Q(t)$ is differentiable but not strongly monotonic: 
\item $Q(t)$ is not strongly monotonic: there exist intervals $A_i = [a_i, b_i)$ such that $Q(t)$ is constant on $A_i$,
and isolated points $t_i$ where $Q'(t_i) = 0.$
\end{itemize}

\emph{Strictly monotonic case.}  Because $Q$ is defined on a closed interval,
strict monotonicity further implies the property of \emph{strong monotonicity} where 
$\inf_[0,1] Q'(t) > 0.$  Therefore, for any differentiable perturbation $\xi(t)$ with $\sup |\xi'(t)| <\infty$,
and further imposing that $\xi(0) \geq 0$ in the case that $Q(0) = 0$,
there exists some $\epsilon >0$ such that $(Q + \epsilon \xi)(t)$ is still a valid quantile function.
Therefore, in order for $Q(t)$ to be a local maximum, we must have
\[
0 = \lambda + \nu  + \mu x^{k-1} + \nu \log Q(t)
\]
for $t \in [0,1]$.  This implies that
\[
Q(t) = c_0 e^{-c_1 x^{k-1}}
\]
for some $c_0, c_1 \geq 0$.

\emph{Other cases.}   (TODO) We have to show that these cannot be local maxima.



\section{General case}

We claim that the constants $C_k^{unif}(\iota)$ obtained for the special case also apply for the general case, i.e.
\[
C_k(\iota) = C_k^{unif}(\iota).
\]

We make use of the following Lemma:

\textbf{Lemma.} \emph{
Suppose $X$, $Y$, $W$, $Z$ are continuous random variables, and that $W\perp Y|Z$, $Z \perp X|Y$, and $W \perp Z|(X,Y)$.
Then,
\[
\text{I}[p(x, y)] = \text{I}[p((x,w), (y,z))]
\]
and
\[
\text{ABA}_k[p(x, y)] = \text{ABA}_k[p((x,w), (y,z))].
\]
}

\textbf{Proof.}
Due to conditional independence relationships, we have
\[
p((x,w), (y,z)) = p(x,y)p(w|x)p(z|y).
\]

It follows that
\begin{align*}
\text{I}[p((x,w), (y,z))] &= \int dx dw dy dz  \ p(x,y)p(w|x)p(z|w) \log \frac{p((x,w), (y,z))}{p(x,w)p(y,z)}
\\&= \int dx dw dy dz \ p(x,y)p(w|x)p(z|w) \log \frac{p(x, y)p(w|x)p(z|y)}{p(x)p(y)p(w|x)p(z|y)}
\\&= \int dx dw dy dz \ p(x,y)p(w|x)p(z|w) \log \frac{p(x, y)}{p(x)p(y)}
\\&= \int dx dy \ p(x,y) \log \frac{p(x, y)}{p(x)p(y)} = \text{I}[p(x,y)].
\end{align*}

Also,
\begin{align*}
\text{ABA}_k[p((x,w),(y,z))] 
&= \int \left[\prod_{i=1}^k p(x_i, w_i) dx_i dw_i \right] \int dy dz \ \max_i p(y,z|x_i, w_i).
\\&= \int \left[\prod_{i=1}^k p(x_i, w_i) dx_i dw_i \right] \int dy \ \max_i p(y|x_i) \int dz \ p(z|y).
\\&= \int \left[\prod_{i=1}^k p(x_i) dx_i \right] \left[\prod_{i=1}^k \int dw_i p(w_i|x_i)\right] \int dy \ \max_i p(y|x_i)
\\&= \text{ABA}_k[p(x,y)].
\end{align*}

$\Box$

Next, we use the fact that for any $p(x,y)$ and $\epsilon > 0$, there exists a discrete distribution $p_\epsilon(\tilde{x}, \tilde{y})$ such that
\[
|\text{I}[p(x,y)] - \text{I}[p_\epsilon(\tilde{x}, \tilde{y})]| < \epsilon,
\]
where for discrete distributions, one defines
\[
\text{I}[p(x,y)] = \sum_x \sum_y p(x,y) \log \frac{p(x,y)}{p(x)p(y)}.
\]

We require the additional condition that the marginals of the discrete distribution are close to uniform: that is, for some $\delta > 0$, we have
\[
\sup_{x, x': p_\epsilon(x) > 0\text{ and }p_\epsilon(x') > 0} \frac{p_\epsilon(x)}{p_\epsilon(x')} \leq 1 + \delta.
\]
and likewise
\[
\sup_{y, y': p_\epsilon(y) > 0\text{ and }p_\epsilon(y') > 0} \frac{p_\epsilon(y)}{p_\epsilon(y')} \leq 1 + \delta.
\]

To construct the discretization with the required properties, choose a regular rectangular grid $\Lambda$ over the domain of $p(x,y)$
sufficiently fine so that partitioning $X,Y$ into grid cells, we have
\[
|\text{I}[p(x,y)] - \text{I}[\tilde{p}(\tilde{x}, \tilde{y})]| < \epsilon.
\]
[NOTE: to be written more clearly]
Next, define 


\end{document}



