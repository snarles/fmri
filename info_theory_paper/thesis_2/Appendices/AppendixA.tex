% Appendix A

\chapter{Appendix for Chapter 1} % Main appendix title

\label{AppendixA} % For referencing this appendix elsewhere, use \ref{AppendixA}


\section{Supervised learning}\label{sec:sup_learn}

\emph{Supervised learning} refers to the training of predictive
models; both \emph{classification} and \emph{regression} are special
cases of supervised learning.  But let us begin with the most general
recipe for a prediction task, which involves:

\begin{itemize}
\item A predictor space $\mathcal{X}$ defining the possible values the
  predictor $X$ can take; though typically, $\mathcal{X} =
  \mathbb{R}^p$.
\item A response space $\mathcal{Y}$ defining the possible values the response $Y$ can take;
\item An \emph{unknown} population joint distribution $G$ for the pair $(\vec{X}, Y)$;
\item A \emph{cost} function defining the penalty for incorrect
  predictions, $C: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$.  If
  $Y$ is the response, and $\hat{Y} = h(\vec{X})$ is the prediction, then
  the loss for making the prediction $\hat{Y}$ when the truth is $Y$
  is given by $C(Y; \hat{Y})$.
\end{itemize}

The prediction task can be viewed as a game between two players:
nature and the statistician.  Nature draws an observation $(\vec{X},
Y)$ from the joint distribution $G$.  The statistician only gets to
see $\vec{X}$, and has to make a prediction $\hat{Y}$ for the response
$Y$.  Later on, the true value $Y$ is revealed, and the statistician
pays the cost $C(Y; \hat{Y})$ depending on how well their prediction
matched the actual value of $Y$.

The various types of prediction tasks include classification,
regression, and multivariate variants: such as multi-label
classification and multiple-response regression.  These special cases
are just specializations of the general prediction task to a
particular type of response space.

\begin{itemize}
\item In \emph{classification}, the response space is finite and
  discrete.  In \emph{binary classification}, the response space
  $\mathcal{Y}$ consists of two elements, say, $\mathcal{Y} = \{0,
  1\}$.  Multi-class classification usually refers to the case
  $\mathcal{Y}$ has more than two elements.  The most common cost
  function for classification is zero-one loss,
\[
C(y; \hat{y}) = I(y \neq \hat{y}).
\]
\item In \emph{regression}, the response space is $\mathbb{R}$.  The most common cost function is squared loss:
\[
C(y; \hat{y}) = (y - \hat{y})^2.
\]
\item In \emph{multi-label classification}, the response space is a
  product of several finite sets, say $\mathcal{Y} = \mathcal{Y}_1
  \times \mathcal{Y}_2 \times \cdots \mathcal{Y}_\ell$.  That is to
  say, that the response $\vec{Y}$ consists of a categorical vector,
  $\vec{Y} = (Y_1,\hdots, Y_\ell)$. More complex types of cost
  functions can be considered, such as \emph{Jaccard distance},
\[
C(\vec{y}; \hat{\vec{y}}) = \frac{\sum_{i=1}^\ell y_i \wedge \hat{y}_i}{\sum_{i=1}^\ell y_i \vee \hat{y}_i}.
\]
\item In \emph{multiple-response regression}, the response space is $\mathbb{R}^p$.  A natural cost function is squared Euclidean distance,
\[
C(\vec{y}; \hat{\vec{y}}) = ||\vec{y} - \hat{\vec{y}}||^2.
\]
\end{itemize}

A \emph{prediction rule} is a function $h: \mathcal{X} \to
\mathcal{Y}$ for predicting $Y$ as a function of $\vec{X}$.
Prediction rules can be found through a variety of means.  In some
domains, experts manually construct the prediction rules using their
domain knowledge.  However, the field of \emph{supervised learning}
aims to algorithmically construct, or `learn' a good prediction rule
from data.  In supervised learning, we assume that we have access to a
\emph{training set} consisting of $n_1$ observations
$\{(\vec{X}_i,Y_i)\}_{i=1}^{n_1}$, plus a \emph{test set} consisting
of $n_2$ observations $\{(\vec{X}_i,Y_i\}_{i=n_1 + 1}^{n_1 + n_2}$;
usually, we assume that the pairs in both the training and test set
have been sampled i.i.d. from the distribution $G$.  As we will
elaborate further, the training set is used to construct $h$, while
the test set is used to evaluate the performance of $h$.
%We will also write $\bX$
%for the matrix of training observations, with each $\vec{X}_i$ stacked in
%rows, and $\bY$ for the vector of training responses. The training set
%is used to construct the prediction rule $h$.  The test set is then
%used to estimate the risk of the constructed rule (which is also
%called the \emph{generalization error}.)

A \emph{learning algorithm} $\Lambda$ is a procedure for constructing the
prediction rule $h$ given training data $\{(\vec{X}_i,Y_i)\}_{i=1}^{n_1}$ as
an input.  Formally, we write
\[
h = \Lambda(\{(\vec{X}_i,Y_i)\}_{i=1}^{n_1}),
\]
indicating that $h$ is the output of the function $\Lambda$ evaluated
on the input $\{(\vec{X}_i,Y_i)\}_{i=1}^{n_1}$.  But recall that $h:
\mathcal{X} \to \mathcal{Y}$, the classification rule, is also a
function!  How learning algorithms are implemented in practice can
very considerably; we illustrate just a few of the most common types
of learning algorithms:

\begin{itemize}
\item \emph{Parametric generative models.}  These types of learning
  algorithms $\Lambda$ first fit a statistical model to the observed
  data, then use that model to predict on new observations. Define a
  parametric family $F_\theta$ of joint distributions $(X, Y)$.  For
  instance, in linear regression, a commonly studied family is the
  multivariate normal linear model, where
\[
(\vec{X}, Y) \sim N((1,0,\hdots,0, \beta_0), \begin{pmatrix}\Sigma_X & \Sigma_X \beta \\
\beta^T \Sigma_X & \beta^T \Sigma_X \beta + \Sigma_\epsilon\end{pmatrix},
\]
or equivalently,
\[
\vec{X} \sim N((1,0,\hdots,0), \Sigma_X)
\]
\[
Y|\vec{X} \sim N(\vec{X}^T \beta, \Sigma_\epsilon).
\]
The learning algorithm $\Lambda$ proceeds by first fitting the
parametric model to estimate the parameter $\hat{\theta}$.  A variety
of methods may be chosen to estimate $\theta$, such as maximum
likelihood, penalized maximum likelihood, or Bayesian estimation.
Given the fitted statistical model, we can obtain the conditional
distribution of $Y$ given $\vec{X}$.  The prediction rule $h(\vec{x})$
is then constructed using this conditional distribution; for instance,
taking $h(\vec{x})$ to be the conditional mean of $Y$ given $\vec{X} =
\vec{x}$.
%Note also that
%in many cases, such as regression, not all parameters of the model
%need to be estimated for prediction purposes.  For instance, in the
%linear regression model given above, only $\beta$ needs to be
%estimated, and not $\Sigma_X$ or $\Sigma_Y$.  One then constructs the
%prediction rule $h$ depending on the estimated parameter
%$\hat{\theta}$, in a way so that the risk is controlled.  For
%instance, in linear regression, one takes $h(\vec{X}) = \hat{\beta}^T \vec{X}.$
\item \emph{Discriminative models.} These types of learning algorithms
  directly attempt to find a good prediction rule, using empirical
  performance on the training data as a criterion. One typically
  limits the search over possible prediction rules to a function class
  $\mathcal{H}$.  We wish to search for an element $h \in \mathcal{H}$
  which minimizes the empirical risk on the training set,
\[
h = \text{argmin}_{h \in \mathcal{H}} \frac{1}{n_1} \sum_{i=1}^{n_1} \tilde{C}(Y_i; h(\vec{X}_i))
\]
Here, $\tilde{C}$ could be taken to be equal to the original cost
function $C$, or could be taken to be a different function, such as a
smoothed approximation of $C$.  The advantage of using a smoothed
approximation $\tilde{C}$ is that the empirical risk can be made
differentiable (whereas the original cost $C$ might be
nondifferentiable) and hence the optimization made much more tractable
from a numerical standpoint.  This is often the case in binary
classification, where $C$ is zero-one loss, but $\tilde{C}$ is the logistic loss
\[
\tilde{C}(y; p) = y \log p + (1-y) \log (1-p).
\]
%% might have to explain why the prediction space is probabilities rather than responses, as well
\end{itemize}
Further complicating the picture is the fact that often the learning
algorithm requires specification of various \emph{hyperparameters}.
For instance, lasso regression (\cite{Hastie2009a}) is a penalized generative model which
finds $\beta$ minimizing the objective function
\[
\beta = \text{argmin}_\beta \frac{1}{2}\sum_{i=1}^{n_1}(y_i - \vec{x}_i^T \beta)^2 + \lambda ||\beta||_1.
\]
and then constructs the prediction rule
\[
h(\vec{x}) = \vec{x}^T \beta.
\]
%% No explanation of L1 norm notation
Here, the $L_1$-penalty constant $\lambda$ needs to be specified by
the user.  In practice, one can either use prior knowledge or
theoretically-justified rules to select $\lambda$; or, more commonly,
one uses various procedures to automatically tune $\lambda$ based on
the training data.  The most common procedure for automatically
selecting $\lambda$ is cross-validation, with either the ``min'' or
``one standard deviation'' rule.  We do not go into details here, and
refer the interested reader to \cite{Hastie2009a}, section 7.10.

\subsection{Performance evaluation}

In practice, we would often like to know how well the prediction rule
$h$ will perform on new data.  This can be done rigorously if we can
assume that the new data pairs $(X, Y)$ will be drawn i.i.d. from some
population distribution $G$, and that the observations in the test set
are also drawn i.i.d. from $G$.  The criterion we use to judge the
performance of the prediction rule $h$ is the \emph{prediction risk} (also called \emph{generalization error})
\[
\text{Risk}(h) = \mathbb{E}_G[C(Y; h(X))].
\]

Under the assumption that the test set is drawn i.i.d. from $G$, then
it follows that the test risk (aka \emph{test error}) is an unbiased
estimator of the risk.
\[
\text{TestRisk}(h) = \frac{1}{n_2} \sum_{i=n_1+1}^{n_1 + n_2} C(y_i; h(x_i)).
\]
\[
\E[\text{TestRisk}(h)] = \text{Risk}(h).
\]

Under mild assumptions, one can use the Student-t quantiles to
construct a confidence interval for the risk,
\[
\text{TestRisk}(h) \pm t_{1 - \alpha/2; df = n_2 - 1} \hat{\text{sd}}(\{C(y_i; h(x_i))\}_{i=n_1+1}^{n_1 + n_2})
\]
where $t_{1 - \alpha/2, df = n_2 - 1}$ is the $1 - \frac{\alpha}{2}$
quantile of the t-distribution with $n_2 - 1$ degrees of freedom, and
$\hat{\text{sd}}$ is the sample standard deviation.

A common pitfall is to attempt to use the \emph{training data}, rather
than independent test data, to estimate the risk.  The empirical risk
on the training data tends to be an underestimate of the true
population risk, due to the phenomenon of \emph{overfitting}.  That
is, the prediction rule $h$ may be capturing the effect of noise in
the training data as well as signal.

It is usually the job of the data analyst to make sure that the data
has been partitioned into independent training and test data sets
before carrying out any analysis.  It is an important decision as to
how much data to allocate to each of the training and test sets.  A
larger training set generally results in better prediction rules, but
a larger test set allows for more precise estimates of prediction
risk.

In any case, once it has been decided to allocate $n_1$ observations
to the training set, and $n_2$ observations to the test set, one
carries out \emph{data-splitting} in order to randomly assign the
observations to the training and test sets.  The randomization ensures
that the i.i.d. sampling assumption is met for both the training and
test set.  Concretely speaking, given observations $(\vec{x}_i,
y_i)_{i=1}^n$, one draws a random permutation $\sigma: n \to n$, then
takes $\{(\vec{x}_{\sigma_i}, y_{\sigma_i})_{i=1}^{n_1}\}$ as the
training set, and the remaining observations $\{(\vec{x}_{\sigma_i},
y_{\sigma_i})_{i=n_1 + 1}^{n}\}$ as the test set.

Often it is the case that the number of observations $n$ is so small
that one cannot afford to create a large test set.  To avoid the
tradeoff between having insufficient training data and insufficient
test data, one can use the $k$-fold \emph{cross-validation} procedure.
In cross-validation, one uses the entire data set to construct the
prediction rule $h$.  Now, in order to estimate the prediction risk,
one splits the data into $k$ (approximately) equally-sized partitions.
Then, for fold $i = 1,\hdots, k$, we take the $i$th partition as the
test set, and merge the the remaining $k-1$ partitions into the
training set.  The training set is used to construct a new prediction
rule, $h^{(i)}$.  Then, the test set is used to estimate the risk of
$h^{(i)}$, yielding the empirical risk $\text{TestRisk}^{(i)}$.  After
this has been done for all $k$ folds, we have the cross-validation
risk estimates $\text{TestRisk}^{(1)},\hdots, \text{TestRisk}^{(k)}$.
The risk of $h$ itself is estimated as
\[
\text{CVRisk} = \frac{1}{k}\sum_{i=1}^k \text{TestRisk}^{(i)}.
\]
The intuition behind cross-validation is that each cross-validated
risk estimate $\text{TestRisk}^{(i)}$ should be an overestimate of the
population risk of $h$, because $h^{(i)}$, being constructed from
fewer training data, tends to have a larger population risk than $h$.
Therefore, $\text{CVRisk}$ should be an overestimate of the risk of
$h$.

\subsection{Classification}

In classification, the response space $\mathcal{Y}$ is discrete.  The
prediction rule is called a \emph{classification rule}, and the
learning algorithm is called a \emph{classifier}.  The elements $y \in
\mathcal{Y}$ of the response space are called \emph{labels}.  Let $k =
|\mathcal{Y}|$ be the number of labels.  When a feature vector
$\vec{x}$ has the true label $i$, we can also say that $\vec{x}$
belongs to the $i$th class.

The most common cost function considered in classification problems is
zero-one loss,
\[
C(y;\hat{y}) = I(y \neq \hat{y}).
\]
We assume the zero-one loss for the rest of the discussion.  This
implies that the risk of a classification rule is the probability of
misclassification,
\[
\text{Risk}(h) = \E[C(Y; h(X))] = \Pr[Y \neq h(X)].
\]

A theoretically important (but non-implementable) classification rule
is the \emph{Bayes rule}, which achieves optimal prediction risk.
However, since the Bayes rule requires knowledge of the population
joint distribution, it cannot be constructed in practice.  Supposing
that $(\vec{X}, Y)$ are drawn from a joint distribution $G$, then
define $F_y$ as the conditional distribution of $\vec{X}$ given $Y =
y$.  Supposing that $F_y$ has a density $f_y$, and that the labels $Y$
have a uniform distribution, then the Bayes rule assigns feature
vectors $\vec{x}$ to the label with the highest density.
\[
h_{Bayes}(\vec{x}) = \text{argmax}_{y \in \mathcal{Y}} f_y(\vec{x}).
\]

Since the response space is discrete, the classification rule $h$
partitions the input space $\mathcal{X}$ into $k$ partitions.  The
boundaries between adjacent partitions are called \emph{decision
  boundaries}.  A large number of popular classifiers produce
\emph{linear decision boundaries}: that is, each decision boundary
lies on a hyperplane.

A large number of classifiers create classification rules that are
based on \emph{margin functions} (or \emph{discriminant functions.})
A margin function is produced for each label in $\mathcal{Y}$.  The
margin function for label $y$, $m_y: \mathcal{X} \to \mathbb{R}$
quantifies how likely a feature vector $\vec{x}$ has label $y$.  We
say that $m_y(\vec{x})$ is the margin (or \emph{discriminant score})
of $\vec{x}$ for the $i$th label.  The classification rule $h$,
therefore, assigns points to the label having the highest margin for
$\vec{x}$,
\[
h(\vec{x}) = \text{argmax}_{y \in \mathcal{Y}} m_y(x).
\]

Classifiers with \emph{linear discriminant functions}; that is, which produce margin functions of the form
\[
m_y(\vec{x}) = w^T \vec{x}
\]
result in \emph{linear decision boundaries.}  These include:
\begin{itemize}
\item \emph{Linear support vector machines} (\cite{Hastie2009a}).
\item \emph{Multinomial logistic regression}.
\item \emph{Fisher's linear discriminant analysis}.
\end{itemize}

Another large class of classifiers--\emph{generative} classifiers--are based on estimating the
conditional distribution of $\vec{x}$ within each class.
These classifiers use the discriminant function
\[
m_y(\vec{x}) = \log \hat{f}_y(\vec{x})
\]
where $\hat{f}_y$ is the estimated density of the distribution $F_y$.
The estimated densities $\hat{f}_y$ also comprise a \emph{generative
  model} in the sense that they allow the possibility of simulating
new data from the class--hence the nomenclature.  Different
distributional assumptions lead to different classifiers within the
generative category.  Some examples are:
\begin{itemize}
\item \emph{Naive Bayes.}  One assumes that $F_y$ is a product distribution on the components of $\vec{x}$.
\item \emph{Fisher's linear discriminant analysis.}  One assumes that $\{F_y\}_{y \in \mathcal{Y}}$ are multivariate normal with common covariance.
\item \emph{Quadratic discriminant analysis.}  One assumes that $\{F_y\}_{y \in \mathcal{Y}}$ are multivariate normal.
\end{itemize}

Some other commonly used classifiers include:
\begin{itemize}
\item \emph{k-Nearest neighbors.}  Uses margin functions
  $m_y(\vec{x})$ which count how many of the $k$ nearest neighbors of
  $\vec{x}$ in the training set have the label $y$.
\item \emph{Decision trees.}  Recursively partitions the input space
  $\mathcal{X}$ into smaller and smaller regions, then assigns points
  $\vec{x}$ to the majority class within the region.
\item \emph{Multilayer neural networks.}  Learns nonlinear
  representations of the input space, $g_j(\vec{x})$, then constructs
  margin functions which are linear combinations of the
  representations $g_j$.
\end{itemize}

%% discuss parallelization here??

Under zero-one loss, it is easy to conduct inference for the
prediction risk of $h$.  Under the i.i.d. sampling assumption, the
loss of a test observation $L(y_i; h(x_i))$ has a Bernoulli
distribution with probability equal to the population risk.  Therefore,
we have
\[
n_2 \text{TestRisk}(h) \sim \text{Bernoulli}(n_2, \text{Risk}(h)).
\]


\subsection{Regression}

\emph{Regression models} predict a continuous response $Y$ as a function of a vector-valued predictor $\vec{X}$.
The model assumed is written as
\[
Y = f(\vec{X}) + \epsilon
\]
where $\epsilon$ is additive noise with zero mean, $\E[\epsilon] = 0$.
The function $f$ is called the \emph{regression function} since
\[
f(\vec{x}) = \E[Y|\vec{X} = \vec{x}].
\]
The goal in regression is to recover the unknown function $f$.

In \emph{linear regression}, we assume $f$ is linear. However, if we
do not assume a particular form for $f$, we can use
\emph{nonparametric regression}.

When $\vec{X}$ is high dimensional, classical regression techniques
perform poorly.  Yet, if the true function $f$ only depends on a small
number of components in $\vec{X}$, we can still do well if we use
\emph{sparse} regression methods. Table \ref{tab:regression} shows
some examples of different regression procedures proposed in the
literature which are suited for different scenarios.

\begin{figure}
\centering
\begin{tabular}{c|c|c|}
 & \emph{Classical} & \emph{Sparse} \\ \hline
 \emph{Linear} & Ordinary Least-Squares  & Elastic net \\ 
  & (Legendre 1805) & (Zou 2008)  \\\hline
 \emph{Nonpar.} & LOWESS  & Random forests  \\ 
   & (Cleveland 1979) & (Breiman 2001)  \\\hline
\end{tabular}
\caption{Different variants of regression models}
\label{tab:regression}
\end{figure}

\section{Information Theory}\label{sec:intro_mi}

%% Sudden transition

Information theory is motivated by the question of how to design a
message-transmission system, which includes two users--a sender and a
reciever, a \emph{channel} that the sender can use in order to
communicate to the reciever, and a protocol that specifies:
\begin{itemize}
\item[a.] how the sender can \emph{encode} the message in order to
  transmit it over the channel.  Morse code is one example of an
  encoding scheme: a means of translating plaintext into signals than
  can be transmitted over a wire (dots and dashes); and
\item[b.] how the reciever can \emph{decode} the signals recieved from
  the channel output in order to (probabilistically) recover the
  original message.
\end{itemize}

Beginning with Shannon (1948), one constrains the properties of the
channel, and studies properties of encoding/decoding protocols to be
used with the channel.  Two types of channels are studied:
\emph{noiseless} channels, which transmit symbols from a fixed
alphabet (e.g. ``dots'' and ``dashes'') from the sender to reciever,
and \emph{noisy} channels, which transmit symbols from a discrete
symbol space $\mathcal{Y}$ to a possibly different symbol space
$\mathcal{X}$ in a stochastic fashion.  That is, for each input symbol
$y \in \mathcal{Y}$, the transmitted symbol\footnote{Note that here we
  have flipped the usual convention in information theory, in which
  the letter $X$ commonly denotes the input and $Y$ denotes the
  output.  However, we flip the notation in order to match the
  convention in multi-class classification.} output $X$ is drawn from a
distribution $F_y$ that depends on $y$.  It is the study of
noisy channels that is of primary interest to us.

We allow the sender to transmit a sequence of $L$ input symbols over
the channel, $\vec{Y} = (Y_1,Y_2,\hdots, Y_L)$. The reciever will observe the
output $\vec{X} = (X_1,X_2,\hdots, X_L)$, where each $X_i$ is drawn from
$F_{Y_i}$ independently of the previous $X_1,\hdots, X_{i-1}$.

An example of a noisy channel is the \emph{bit-flip} channel.
Let $\mathcal{Y} = \mathcal{X} = \{0,1\}$, so that both the input and output are binary strings.
The bit flip channel is given by
\[
F_0 = \text{Bernoulli}(\epsilon)
\]
\[
F_1 = \text{Bernoulli}(1-\epsilon)
\]
so that $X = Y$ with probability $1-\epsilon$, and $X = 1-Y$
otherwise.

Now, let us assume that the sender wants to transmit message $M$, out
of a finite set of possible messages $\mathcal{M} = \{1,\hdots, m\}$.
The message must be encoded into a signal $\vec{Y} \in \mathcal{Y}^L$,
which is sent through a stochastic channel $F$.  Thus, the encoding
scheme is given by a \emph{codebook} or \emph{encoding function} $g:
\{1,\hdots, m\} \to \mathcal{Y}^L$ which specifies how each message
$i$ is mapped to an input sequence, $g(i) \in \mathcal{Y}^L$.
Conversely, the decoding scheme is given by a decoding function
$d(\vec{X})$ which infers the message $\{1,\hdots, m\}$ from the
recieved signal $\vec{X}$.  Theoretically
speaking\footnote{Practically speaking, the maximum likelihood (ML)
  decoder may be intractable to implement, and computational
  considerations mean that development of practical decoders remains a
  challenging problem.}, a reasonable decoding scheme is the
\emph{maximum likelihood decoder},
\[
d(\vec{x}) = \max_{i \in \{1,\hdots, m\}} \Pr[\vec{X} = \vec{x}| \vec{Y} = g(i)] = \max_{i \in \{1,\hdots, m\}} \prod_{j=1}^L F_{(g(i))_j}(X_j).
\]

The design of encoding/decoding schemes with minimal error (or other
desirable properties) over a fixed channel is a highly nontrivial
problem, which remains a core problem in the information theory
literature.  However, Shannon's original proof of the noisy channel
capacity theorem demonstrates a surprising fact, which is that for
large message spaces $\mathcal{M}$, close-to-optimal information
transmission can be achieved by using a \emph{randomized} codebook.
In order to discuss the noisy channel capacity theorem and the
construction of the randomized codebook, we first need to define
the concept of \emph{mutual information}.

\subsection{Mutual information}

If $\bX$ and $\bY$ have joint density $p(\bx, \by)$ with respect to
the product measure $\mu_x \times \mu_y$, then the mutual information
is defined as
\[
\text{I}(\bX;\bY) = \int p(\bx, \by) \log \frac{p(\bx, \by)}{p(\bx)p(\by)}d\mu_x(\bx) d\mu_y(\by).
\]
where $p(\bx)$ and $p(\by)$ are the marginal densities with
respect\footnote{Note that the mutual information is invariant with
  respect to change-of-measure.} to $\mu_x$ and $\mu_y$.  When the
reference measure $\mu_x \times \mu_y$ is unambiguous, note that
$\text{I}(\bX;\bY)$ is simply a functional of the joint density
$p(\bx, \by)$.  Therefore, we can also use the \emph{functional}
notation
\[
\text{I}[p(\bx, \by)] = \int p(\bx, \by) \log \frac{p(\bx, \by)}{p(\bx)p(\by)}d\mu_x(\bx) d\mu_y(\by).
\]


The mutual information is a measure of dependence between random
vectors $\bX$ and $\bY$, and satisfies a number of important
properties.
\begin{enumerate}
\item The channel input $\bX$ and output $\bY$ can be random vectors of arbitrary dimension, and the mutual information remains a scalar functional of the joint distribution $P$ of $(\bX, \bY)$.
\item When $\bX$ and $\bY$ are independent, $\text{I}(\bX; \bY) = 0$; otherwise, $\text{I}(\bX; \bY) > 0$.
\item The data-processing inequality: for any vector-valued function $\vec{f}$ of the output space,
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
\item Symmetry: $\text{I}(\bX; \bY) = \text{I}(\bY; \bX)$.
\item Independent additivity: if $(\bX_1,\bY_1)$ is independent of $(\bX_2, \bY_2)$, then
\[
\text{I}((\bX_1,\bX_2); (\bY_1, \bY_2)) = \text{I}(\bX_1; \bY_1) + \text{I}(\bX_2; \bY_2).
\]
\end{enumerate}
Three additional consequences result from the data-processing inequality:
\begin{itemize}
\item \emph{Stochastic data-processing inequality.}  If $\vec{f}$ is a stochastic function independent of both $\bX$ and $\bY$, then
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY).
\]
This can be shown as follows: any stochastic function $\vec{f}(\bY)$
can be expressed as a deterministic function $\vec{g}(\bY, W)$, where
$W$ is a random variable independent of $\bX$ and $\bY$.
By independent additivity,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)).
\]
Then, by the data-processing inequality,
\[
\text{I}(\bX; \bY) = \text{I}(\bX; (\bY, W)) \geq \text{I}(\bX; \vec{g}(\bY, W)) = \text{I}(\bX; \vec{f}(\bY)).
\]
\item \emph{Invariance under bijections.} If $\vec{f}$ has an inverse $\vec{f}^{-1}$, then 
\[
\text{I}(\bX; \vec{f}(\bY)) \leq \text{I}(\bX; \bY) = \text{I}(\bX; \vec{f}^{-1}(\vec{f}(\bY))) \leq \text{I}(\bX; \vec{f}(\bY)),
\]
therefore, $\text{I}(\bX; \vec{f}(\bY)) = \text{I}(\bX; \bY)$.
\item \emph{Monotonicity with respect to inclusion of outputs.}  Suppose we have an output ensemble $(\bY_1,\bY_2)$.  Then the individual component $\bY_1$ can be obtained as a projection of the ensemble.  By the data-processing inequality, we therefore have
\[
\text{I}(\bX; \bY_1) \leq \text{I}(\bX; (\bY_1, \bY_2)).
\]
Intuitively, if we observe both $\bY_1$ and $\bY_2$, this can
only \emph{increase} the information we have about $\bX$ compared to
the case where we only observe $\bY_1$ by itself.
\end{itemize}
And it is the property of \emph{invariance under bijections},
inclusive of non-linear bijections, which qualifies mutual information
as a \emph{non-linear measure of dependence.}  Linear correlations are
invariant under scaling and translation, but not invariant
to \emph{nonlinear} bijections.

%As for
%the completeness of the five listed properties: as we know, Shannon's
%mutual information (up to arbitrary scaling factor) is the only
%functional proposed in the literature which satisfies all five
%properties.
Besides the formal definition, there are a number of well-known alternative
characterizations of mutual information in terms of other
information-theoretic quantities: the \emph{entropy} $\text{H}$:
\[
\text{H}_\mu(\bX) = -\int p(\bX) \log p(\bX) d\mu(\bX),
\]
and the \emph{conditional entropy}:
\[
\text{H}_\mu(\bX|\bY) = -\int p(\bY) d\mu_y(\bY) \int p(\bX|\bY) \log p(\bX|\bY) d\mu_x(\bX).
\]
Some care needs to be taken with entropy and conditional entropy since
they are not invariant with respect to change-of-measure: hence the
use of the subscript in the notation $\text{H}_\mu$.  In particular,
there is a difference between \emph{discrete entropy} (when $\mu$ is
the counting measure) and \emph{differential entropy} (when $\mu$ is
$p$-dimensional Lesbegue measure.)  Intutively, entropy measures an
observer's uncertainty of the random variable $\bX$, supposing the
observer has no prior information other than the distribution of
$\bX$. Conditional entropy measures the \emph{expected uncertainty} of
$\bX$ supposing the observer observes $\bY$.

The following identities characterize mutual information in terms of entropy:
\[
\text{I}(\bX; \bY) = \text{H}_{\mu_x \times \mu_y}((\bX, \bY)) - \text{H}_{\mu_x}(\bX) - \text{H}_{\mu_y}(\bY).
\]
\begin{equation}\label{eq:ce_ident}
\text{I}(\bX; \bY) = \text{H}_{\mu_y}(\bY) - \text{H}_{\mu_y}(\bY|\bX).
\end{equation}
Loosely speaking, $\text{H}_{\mu_y}(\bY)$ is the uncertainty of $\bY$
before having observed $\bX$, and $\text{H}_{\mu_y}(\bY|\bX)$ is the
uncertainty of $\bY$ after having observed $\bX$, hence
$\text{H}_{\mu_y}(\bY) - \text{H}_{\mu_y}(\bY|\bX)$ is how much the
observation of $\bX$ has \emph{reduced} the uncertainty of $\bY$.
Stated in words,
\[
\text{I}(\bX; \bY) = \text{average reduction of uncertainty about $\bY$ upon observing $\bX$}.
\]
The identity \eqref{eq:ce_ident} is also noteworthy
as being practically important for estimation of mutual information.
Since the entropies in question only depend on the marginal and
conditional distributions of $\bY$, the problem of estimating
$\text{I}(\bX; \bY)$ can be reduced from a $\dim(\bX)
+ \dim(\bY)$-dimensional nonparametric estimation problem to a
$\dim(\bY)$-dimensional problem: hence this identity is a basis of
several methods of estimation used in neuroscience, such as Gastpar
(2014).


\subsection{Channel capacity and randomized codebooks}

As a general measure of dependence, mutual information has enjoyed
numerous and diverse applications outside of information theory.
However, its original role in Shannon's paper was to define the
quantity known as \emph{channel capacity} of a noisy channel.

Let us first note that the channel capacity of a noiseless channel
with $S$ symbols is simply $\log S$.  The justification is that if we
allow $L$ symbols to be sent, then $S^L$ possible messages can be
encoded.  Therefore, the channel capacity of a noiseless channel can
be understood as the logarithm of the number of possible messages to
be transmitted divided by the length of the sequence, with is $\log
S$.

However, how can the idea of channel capacity be generalized to the
noisy case?  At first glance, it would seem like no comparison is
possible, because no matter how many symbols $L$ the sender is allowed
to transmit, the probability of decoding error may remain nonzero.
Consider the bit-flip channel, where $X = Y$ with probability
$1-\epsilon$ and $X = 1-Y$ otherwise.  Given two different messages,
$M \in \{1,2\}$, a reasonable encoding scheme is for the sender to
transmit a string of $L$ repeated zeros for $M = 1$, and an string of
$L$ repeated ones for $M = 2$.
\[
Y_1 = Y_2 = \cdots = Y_L = M-1.
\]
The reciever should guess $M = 1$ if she recieves more zeros than
ones, and guess $M = 2$ otherwise.  However, for any $L$, the decoding
error will always be nonzero.  Therefore there seems to be no analogy
to the noiseless channel, where zero decoding error can be achieved.

Shannon's idea was to invent an asymptotic definition of channel
capacity.  Consider a sequence of problems where the number of
messages $M$ is increasing to infinity.  In the $m$th coding problem,
where $M = m$, let $(g_m, d_m)$ be an encoder/decoder pair (or \emph{protocol}), where
$g_m$ produces strings of length $L_m$.  Let $e_m$ be the maximum
error probability over all messages $1,\hdots, m$ when using the
protocol $(g_m, d_m)$.  Now, let us require that we choose $(g_m,
d_m)$ so that the error probability vanishes in the limit:
\[
\lim_{m \to \infty} e_m \to 0.
\]
We can define the channel capacity to be the best possible limiting ratio
\[
C = \lim_{m \to \infty} \frac{\log m}{L_m}
\]
over all sequences of protocols that have vanishing error probability.
Note that this definition yields $C = \log S$ for the noiseless
channel, but can also be extended to the noisy channel case.
Remarkably, Shannon finds an explicit formula for the noisy channel
capacity, which is proved in his noisy channel capacity theorem.  We
will now discuss how to calculate the capacity of a noisy channel.

First, let us define the set of joint distributions which can be
realized in the noisy channel.  Let $p_y$ be a probability
distribution over input symbols $\mathcal{Y}$.  If we transmit input
$Y$ randomly according to $Y \sim p_y$, the induced joint distribution
$p(Y, X)$ is given by
\[
p(y, x) = p_y(y) F_y(\{x\}).
\]
The set $\mathcal{P}$ is simply the collection of all such distributions: that is,
\[
\mathcal{P} = \{p(y, x) \text{ such that } p(x|y) = F_y(\{x\})\text{ for all }(x, y) \in \mathcal{X} \times \mathcal{Y}\}.
\]

Suppose we have a noisy channel with transmission probabilities given by $\{F_y\}_{y \in \mathcal{Y}}$.
Shannon came with with the following result:
\[
C = \max_{p\in \mathcal{P}} I[p(y, x)].
\]
The noisy channel capacity is given by the maximal mutual information
$I(Y; X)$ over all joint distributions of $(Y, X)$ that can be
realized in the channel.

To show that $C = \max_p I[p(y, x)]$ is the noisy channel capacity,
then, (i) we need to show that there exists a sequence of codes with
length $L = \frac{\log M}{C}$ which achieves
vanishing\footnote{Shannon's noisy channel capacity theorem shows a
  much stronger property--that the \emph{maximum} decoding error over
  all messages has to vanish.  However, for our purposes, we will
  limit our discussion to a weaker form of the noisy channel capacity
  theorem which is only concerned with average decoding error over all
  messages.} decoding error as $M \to \infty$, and (ii) we need to
show that any code with a shorter length has non-vanishing decoding
error.  We omit the proof of (i) and (ii), which can be found in any
textbook on information theory, such as \cite{Cover2006}.  However,
for our purposes, it is very much worth discussing the construction
that shows direction (i) of the proof--the achievability of channel
capacity.

For a given channel $\{F_y\}$, let $p^* \in \mathcal{P}$ be the
distribution which maximizes $I[p(y, x)]$.  Let $p^*_y$ be the
marginal distribution of $Y$, and let $L = \lceil \frac{\log M}{C}
\rceil$.  Now we can define the random code.  Let $g(i) =
(Y_1^{(i)},\hdots, Y_L^{(i)})$ where $Y_j^{(i)}$ are iid draws from
$p^*_y$ for $i = 1,\hdots, M$ and $j = 1,\hdots, L$.  Shannon proved
that average decoding error, taken over the distribution of random
codebooks, goes to zero as $M \to \infty$.  This implies the existence
of a deterministic sequence of codebooks with the same property, hence
establishing (i).
