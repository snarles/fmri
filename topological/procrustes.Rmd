---
title: "Bootstrap testing of procrustes hypothesis"
author: "Charles Zheng"
date: "12/16/2015"
output: html_document
---

Let $X$ and $Y$ be indepedent $p \times q$ random matrices, with distributions $F$ and $G$ respectively.
Let $A = E[X]$ and $B = E[Y]$.
The goal is to test the *Procrustes hypothesis*

$$
H_0: A^T A = B^T B
$$

or equivalently, that there exists $\Omega$ orthogonal such that $\Omega A = B$.
The data consists of i.i.d. indpendent realizations $X^{(1)}, ..., X^{(n_x)}$ and $Y^{(1)},..., Y^{(n_y)}$.

*Example:* 
We are testing the congruence of two physical objects.  The $q$ columns of $A$ consist of the true positions of $q$ labelled landmarks on the first object, measured according to some orthogonal coordinate system.  The $q$ columns of $B$ give the positions of the corresponding landmarks of the second object, measured using a different orthogonal system compared to the first.  Both sets of positions are centered and standardized using an $(q + 1)$ th landmark which is used to define the origin.  $A$ and $B$ are not observed directly; instead, they must be inferred from unbiased, noisy measurements $X$ and $Y$ respectively.

The situation is depicted in Figure 1.

```{r, fig.height=4.4, fig.width=6.6}
library(pracma)
shape.inner <- matrix(c(0,0, 1,-2, -1,-2, 0,0), 2, 4)
shape.outer <- matrix(c(0,4, 3,-2, 3,-4, -3,-4, -3,-2, 0,4), 2, 6)
A <- cbind(shape.inner[, -4], shape.outer[, -1])
theta <- pi/3; Omega0 <- matrix(c(cos(theta), sin(theta), -sin(theta), cos(theta)), 2, 2)
B <- Omega0 %*% A
par(mfrow=c(1,2),oma = c(0, 0, 2, 0))
plot(NA, NA, xlim = c(-5, 5), ylim = c(-5, 5), ann = FALSE); title("A")
for (i in 1:8) text(t(A[, i]), paste(i-1))
polygon(t(shape.inner), lty = 2); polygon(t(shape.outer), lty = 2)
plot(NA, NA, xlim = c(-5, 5), ylim = c(-5, 5), ann = FALSE); title("B")
polygon(t(Omega0 %*% shape.inner), lty = 2); polygon(t(Omega0 %*% shape.outer), lty = 2)
for (i in 1:8) text(t(B[, i]), paste(i-1))
mtext("Figure 1", outer = TRUE, cex = 1.5)
```

# Procrustes analysis

A natural analysis of this problem is as follows.
Obtain $\hat{A} = \bar{X}$, and $\hat{B} = \bar{Y}$.
Then look for a rotation matrix $R$ which minimizes

$$
||R\hat{A} - \hat{B}||_F^2.
$$

The solution to the above optimization problem was derived by Schonemann (1964).
Having obtained the SVD decomposition
$$
\hat{B}\hat{A}^T = V D U^T,
$$
the optimal rotation is given by
$$
R = VU^T.
$$

# Data model and test statistics

The data is given in the form of two samples, $X^{(1)}, ..., X^{(n_x)}$ and $Y^{(1)},..., Y^{(n_y)}$.
In order to conveniently apply resampling methods, let us adopt the following one-sample sampling model:
the data is given in the form of ordered pairs $(0, X^{(1)}), ..., (0, X^{(n_x)}), (1, Y^{(1)}),..., (1, Y^{(n_y)})$.  Then any one-sample resampling method can be applied to the data.

The data model is given in code as follows. 


`procrustes_data_model_` is a function which takes as inputs:

 * The true parameters $A$, $B$
 * A covariance matrix $\Sigma_X$ for the noise in each column of $X$.
 * A covariance matrix $\Sigma_Y$

The output is a function `sampler(nX, nY)`, which produces $n_X$ observations of $X$ and $n_Y$ observations of $Y$ (vectorized) plus metadata;  `sampler(0,0)` produces noiseless observations.

```{r}
library(MASS)

f2 <- function(x, y = 0) sum((x-y)^2)

procrustes_data_model_ <- function(A, B, SigmaX, SigmaY) {
  p <- dim(A)[1]; q <- dim(A)[2]
  vecA <- as.numeric(A)
  vecB <- as.numeric(B)
  sampler <- function(nX, nY) {
    noise_mult <- 1
    if (nX == 0 && nY == 0) {
      nX <- 1; nY <- 1
      noise_mult <- 0
    }
    rawX <- repmat(vecA, nX, 1)
    temp <- noise_mult * t(mvrnorm(nX * q, rep(0, p), SigmaX))
    rawX <- t(t(rawX) + as.numeric(temp))
    rawY <- repmat(vecB, nY, 1)
    temp <- noise_mult * t(mvrnorm(nY * q, rep(0, p), SigmaY))
    rawY <- t(t(rawY) + as.numeric(temp))
    dat <- rbind(cbind(0, rawX), cbind(1, rawY))
    list(p = p, q  = q, dat = dat)
  }
  sampler
}

## computes Ahat and Bhat
sample_moments <- function(res) {
  p <- res$p; q <- res$q; dat <- res$dat
  rawX <- dat[dat[,1] == 0, -1, drop = FALSE]
  rawY <- dat[dat[,1] == 1, -1, drop = FALSE]  
  Ahat <- matrix(colMeans(rawX), p, q)
  Bhat <- matrix(colMeans(rawY), p, q)
  list(Ahat = Ahat, Bhat = Bhat)
}
```

To test the code, we define null (h0) models and a non-null (h1) model, for a variety of dimensionalities.

``` {r}
p <- 5
q <- 2
SigmaX <- cov(randn(2*p, p)); SigmaY <- cov(randn(2 * p, p))

A_0 <- randn(p, q); B_0 <- svd(randn(p, p))$u %*% A_0
h0_small <- procrustes_data_model_(A_0, B_0, SigmaX, SigmaY)

A_1 <- randn(p, q); B_1 <- randn(p, q)
h1_small <- procrustes_data_model_(A_1, B_1, SigmaX, SigmaY)

dat <- h0_small(100, 200)
mus <- sample_moments(dat)
c(f2(mus$Ahat, A_0), f2(mus$Bhat, B_0))

dat <- h0_small(0, 0)
mus <- sample_moments(dat)
c(f2(mus$Ahat, A_0), f2(mus$Bhat, B_0))

p <- 20; q <- 10
SigmaX <- cov(randn(2*p, p)); SigmaY <- cov(randn(2 * p, p))
A <- randn(p, q); B <- svd(randn(p, p))$u %*% A
h0_med1 <- procrustes_data_model_(A, B, SigmaX, SigmaY)
B <- randn(p, q)
h1_med1 <- procrustes_data_model_(A, B, SigmaX, SigmaY)

p <- 30; q <- 2
SigmaX <- cov(randn(2*p, p)); SigmaY <- cov(randn(2 * p, p))
A <- randn(p, q); B <- svd(randn(p, p))$u %*% A
h0_med2 <- procrustes_data_model_(A, B, SigmaX, SigmaY)
B <- randn(p, q)
h1_med2 <- procrustes_data_model_(A, B, SigmaX, SigmaY)

p <- 15; q <- 15
SigmaX <- cov(randn(2*p, p)); SigmaY <- cov(randn(2 * p, p))
A <- randn(p, q); B <- svd(randn(p, p))$u %*% A
h0_med3 <- procrustes_data_model_(A, B, SigmaX, SigmaY)
B <- randn(p, q)
h1_med3 <- procrustes_data_model_(A, B, SigmaX, SigmaY)
```

Define the following multivariate test statistics $T$ and $D$:

$$
T = VU^T \hat{A} - \hat{B}.
$$

$$
S = \hat{A}^T \hat{A} - \hat{B}^T\hat{B}.
$$

The test statistics are given in code as follows
```{r}

stat.T <- function(res) {
  mus <- sample_moments(res)
  res <- svd(mus$Ahat %*% t(mus$Bhat))
  R <- res$v %*% t(res$u)
  stat.T.raw <- R %*% mus$Ahat - mus$Bhat
  as.numeric(stat.T.raw)
}

stat.S <- function(res) {
  mus <- sample_moments(res)
  stat.S.raw <- t(mus$Ahat) %*% mus$Ahat - t(mus$Bhat) %*% mus$Bhat
  as.numeric(stat.S.raw)
}

stat.T.norm <- function(dat) f2(stat.T(dat))
stat.S.norm <- function(dat) f2(stat.S(dat))

```


# Estimating bias and variance with jackknife

True bias and variance of test statistics.

```{r}

sampling_dist <- function(sampler, theta, nX, nY, mc.reps = 1e3) {
  # true params
  dat0 <- sampler(0, 0)
  theta0 <- theta(dat0)
  # get population
  thetas <- lapply(1:mc.reps, function(i) theta(sampler(nX, nY)))
  thetas <- do.call(cbind, thetas)
  diffs <- thetas - theta0
  bias <- rowMeans(diffs)
  sdv <- apply(diffs, 1, sd)
  list(bias = bias, sdv = sdv)
}

```

Jackknife estimates of bias and variance.

```{r}

jackknife <- function(res, theta) {
  theta0 <- theta(res)
  n <- dim(res$dat)[1]
  thetas <- list()
  for (i in 1:n) {
    dat2 <- res$dat[-i, , drop = FALSE]
    thetas[[i]] <- theta(list(p = res$p, q = res$q, dat = dat2))
  }
  thetas <- do.call(cbind, thetas)
  theta_bar <- rowMeans(thetas)
  bias <- (n-1) * (theta_bar - theta0)
  sdv <- sqrt((n-1) * rowMeans((thetas - theta_bar)^2))
  list(bias = bias, sdv = sdv)
}
  
```


```{r}

sampler <- h0_small
theta <- stat.S
nX <- 20; nY <- 20
res.true <- sampling_dist(sampler, theta, nX, nY)
res.true
res <- sampler(nX, nY)
res.j <- jackknife(res, theta)
res.j

```

# Bootstrap tests


 * A single hypothesis test, $||T||_F^2 = 0$.
 * Multiple hypothesis tests, $T_{ij} = 0$.








