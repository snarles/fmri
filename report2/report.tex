\title{Outline of Stimulus Identification Project}
\author{Charles Zheng and Yuval Benjamini}
\date{\today}

\documentclass[12pt]{article} 

% packages with special commands
\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}
\definecolor{grey}{rgb}{0.5,0.5,0.5}

\begin{document}
\maketitle

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}

\section{Introduction}

Neuroscientists have developed an extensive theory of how simple
phenomena (e.g. wind direction, arm reaching angle) are encoded by
single neurons, and presumably decoded by higher cognitive functions.
However, much remains to be understood about how complex phenomena
(e.g. visual stimuli) are encoded or decoded.

In particular, how much information about the stimulus is captured by
the various subsystems of the brain, or discarded?  Taking vision as
an example, we know that information from the retina is passed up
successive layers of visual subsystems (V1, V2, ...), and presumably
information is lost as each layer successively filters the signal
recieved from the previous layer.

One might also ask about the redundancy of the information encoded by
different neurons.  Presumably each and every neuron is not unique,
but can be grouped with similar, nearby neurons, so that the entire
group can be understood as a homogenous population of neurons.  In
that case, one would expect that the information contained by a
subsample of neurons tends to plateau as one includes more and more
'redundant' neurons in the subsample.

These two questions can both be posed in the framework of information
theory.  In principle, we have the tools to answer these kinds of
questions.  We can infer the connective structure of neural circuits
in living brains, and infer their patterns of neuron activation using
a variety of imaging modalities, including EEG, MEG, fMRI, and calcium
imaging.

However, even after the data has been collected, there exists an
inferential barrier between the data and the information theoretic
conclusions we might wish to make.  First of all, in order to quantify
the information content of a neural system, we would ideally know the
encoding scheme.  However, usually the encoding scheme has to be
hypothesized, and then fit to the data.  And even if we had the
correct encoding scheme, we still can only estimate the information
content due randomness inherent in our experimental design,
measurement devices, and the neuronal dynamics themselves.

This inferential barrier has classically been ignored in classical
studies of encoding and decoding, for good reason.  When the stimulus
is relatively simple, such as wind direction, we can often discover
the correct model based on intuition, and estimate its parameters in
data to a high reliability.  Therefore our error in estimating the
information in a population of neurons might be fairly accurate.

However, the limitations of this naive approach have been recognized
for more complex, multivariate phenomena.  As noted by Quiroga and
Panzeri, 
\begin{displayquote}
... decoding algorithms may fail to decode stimuli owing to a
high-dimensional response space... In such circumstances it may
therefore be dangerous to rule out a candidate neuronal code only
because it gives a near-chance performance with a given decoding
algorithm.
\end{displayquote}

To give a concrete example of why it might be difficult to estimate an
encoding scheme from data, we introduce our motivating example of
natural image identification (Kay 2008).  Here the stimulus $s$ is a
grayscale image with $128 \times 128$ pixels. Suppose for the sake of
illustration that one was able to measure neural activations directly.
Then, one might suppose that the firing rate of the $i$th neuron as a
function of the stimulus is some linear function of the pixels,
\[
r_i(s) = \langle a_i,  s \rangle
\]
One might further assume that $a_i$ belongs to a family of Gabor
filters, parameterized by a location $x_i$, orientation $\theta_i$,
frequency $f_i$, and scale $\tau_i$.  Given that each neuron only has
five parameters, this might be realtively easy.  However, in fact we
do not have the means to measure individual neurons, but rather voxels
$v_i$ which are some time-and-space average version of the neural
activity.  Hence we might model each voxel as
\[
v_i(s) = \langle b_i,  s \rangle
\]
where now $b_i$ is a mixture of many Gabor filters.  The complexity of
each voxel limits the accuracy to which we can recover its encoding
parameters, even supposing we have an accurate model of encoding.

As statisticians, we can develop methodology which account for the
uncertainty in estimation.  Such methodology can be used for the
purpose of evaluating candidate encoding schemes, and inferring the
information content of neuronal populations.  Depending on the quality
of the measurements and the sample size, the uncertainty may be too
great to draw any conclusions.  In that case, statistics is at the
very least informative of the difficulty of the problem, or the need
to acquire more data or use different modalities.

In the next sections, we narrow our scope to a specific hypothetical
model of stimulus encoding, which motivates our statistical approach
for inferring the information content of a population of neurons.
Section 2 introduces the measure of information we employ, Section 3
discusses methods to estimate the encoding and decoding schemes, and
Section 4 proposes methodology for inferring information content given
a candidate encoding scheme, which accounts for statistical uncertainty.

\section{Quantifying Information}

\subsection{Parameterization Issues}

The information content of a neural system has traditionally been
understood as the mutual information between the population of neural
responses, $X$, and the stimulus, $S$: that is, the information encoded by $X$ is
\[
I(S; X) = H(S) - H(S|X)
\]
where $H$ is entropy.

A signficant source of difficulty in developing a theory for encoding
complex phenomena (such as visual stimuli) is that unlike in the case
of simple stimuli, an obvious parameterization of the stimuli in
Euclidean space may not exist.  It is clear how to encode arm
direction in three-dimensional space; on the other hand, it is not
clear if it even makes sense to ask what is the ``dimensionality'' of
a natural scene.  One might restrict the scope to pixelized images,
but this is a rather artificial and unsatisfactory solution.  Without
such an \emph{a priori} parameterization, the most suitable
mathemtical framework for a given stimulus space $\mathcal{S}$ is to
posit distinct stimuli $s$ as elements of a $\sigma$-algebra.

Since the dimensionality of $\mathcal{S}$ is ill-defined, the entropy
$H(S)$ is undefined so the classical concepts of information theory
cannot be directly applied.

An obvious idea might be to replace the external stimulus $s$ with the
most low-level neuronal representation available: e.g., in the case of
vision, to replace $s$ with the vector of firing rates of the retinal
neurons, $\vec{r}^{(0)}$.  While $\vec{r}^{(0)}$ can indeed be finitely
parameterized, and may even have a nice mapping from the source space,
there are two serious drawbacks to this approach.  Firstly, the
low-level mapping differs between subjects; whereas we think of the
stimulus $s$ as external to the subjects and common between
experiments. Secondly, the entropy of $\vec{r}^{(0)}$ would include both
signal specific to $s$ and noise terms.  In order to distinguish
between signal and noise, we have to incorporate the stimulus $s$
after all.

A next approach might be to think of $\mathcal{S}$ as a function space
with some smoothness conditions, and to represent $\mathcal{S}$ in
some suitable basis.  An infinite basis would be needed, but the
coefficients for any particular stimulus would decay in the higher
dimensions, so there is some hope of being able to measure the
information of the stimulus distribution.  This is not an unreasonable
approach, but requires many technicalities which are more or less
tangential to the scientific problem.

An intuitively simpler (if mathematically less elegant) approach is to
base the measure of information on \emph{the ability to discriminate
between distinct stimuli}.  That is, in order to measure the
information content of some neuronal system $\vec{r}$, we need to
specify some \emph{discrimination test}, and supply
a \emph{discriminator} which uses information in $\vec{r}$ to answer
the discrimination test.  Examples of discrimination tests are:
\begin{itemize}
\item I obtain $\vec{r}_1$ from $s_1$ and $\vec{r}_2$ from $s_2$.  Are $s_1$ and $s_2$ the same stimulus? \emph{(Hypothesis testing)}
\item I obtain $\vec{r}$ from one of $\{s_1,\hdots, s_k\}$.  Which stimulus did I chose? \emph{(Classification)}
\end{itemize}
An individual discrimination test provides little information about
the information capacity of a system $\vec{r}$. However, a large
collection of such tests might be more informative; and, indeed, in
the discrete setting an exact correspondence can be made between the
mutual information of $\vec{r}$ and some measure of its average
performance over a collection of tests.

In general, though, there are infinitely many possible measures of
information based on different kinds of discrimination tests.  As a
start, we might look for a measure of information which does not
involve a procedure that is too complicated or arbitrary, in terms of
choosing discrimination tests.  To give some good and bad examples:
\begin{itemize}
\item Fixing some \emph{particular} stimuli $s_1,\hdots ,s_K$ and defining the information content in terms of classification performance on that set.  Too arbitrary: why did you choose those stimuli in particular?
\item (Packing) Choosing a probability $\epsilon$, and declaring a set $s_1,\hdots, s_k$ as \emph{separable} if given any $s_i$, $s_j$ in the set, one can distinguish between $s_i$ and $s_j$ with probability $1-\epsilon$ or better.  Then defining the information content as the log of the largest $k$ for which there exists a separable set of that size.  Too complicated: given that the actual value of $K$ might be millions or billions, there is no way to actually validate this measure experiementally.
\item (Random classification) Defining some probability distribution $\pi$ over stimuli, and then defining the information content in terms of \emph{average} classification performance on a set $s_1,\hdots, s_k$ drawn randomly from $\pi$.  Better: the main source of subjectivity is how to define $\pi$, but it might be acceptable to simply take some image database as representative of a ``natural'' distribution $\pi$.  Then there is also a choice of $k$: more on this later!
\end{itemize}

Another reason to motivate a notion of information based on
classification is that researchers in multivoxel pattern analysis
already intuitively interpret classification performance as a measure
of information; formalizing this intuition would therefore open a new
perspective on existing results.

To narrow down what we might consider an ideal measure of information,
we can turn to the properties of Shannon information as a guideline.
Shannon information has nice properties for describing the information
content of a combined system in terms of the information of its
subsystems.  Given that many of our motivating scientific questions
involve understanding the redundancy of neuronal subsystems, these are
highly desirable properties for our application.  These properties
are:
\begin{enumerate}
\item Given a random vector $\vec{r}^{(1)}$ and an independent random vector $\vec{r}^{(2)}$, the joint entropy is the sum of the individual entropies,
\[
H(r^{(1)}, r^{(2)}) = H(r^{(1)}) + H(r^{(2)})
\]
\item Given non-independent random vectors, the joint entropy is the sum of individual entropies minus the mutual information,
\[
H(r^{(1)}, r^{(2)}) = H(r^{(1)}) + H(r^{(2)}) - I(r^{(1)}, r^{(2)})
\]
\end{enumerate}
The measure of information, $H$, has a desirable additive property,
while the mutual information $I$ provides a measure of redundancy.

In fact, the \emph{packing} definition of information comes close to satisfying property 1, other than the fact that the probability threshold $\epsilon$ has to be adjusted.  That is,
\[
P_{\epsilon'}(r^{(1)}, r^{(2)}) = P_\epsilon(r^{(1)}) + P_\epsilon(r^{(2)})
\]
where $\epsilon'$ might be different from $\epsilon$.  To intuitively
see why this is, the packing number $P$ is the log number of balls
which can be `packed' into the response space $R$.  If $e^{P_1}$
response balls can be packed in $R_1$, and $e^{P_2}$ balls can be
packed into $R_2$, then by taking products of balls in $R_1$ and
$R_2$, $e^{P_1 + P_2}$ balls can be packed into $R_1 \times R_2$.
This is exact if we happened to be talking about lattices of
hypercubes, but it is approximately correct for general metrics.

Since the packing definition of information is impractical to measure,
requiring some estimate of all $k$ choose 2 discrimination tests, we
might hope that this nice property of packing could be obtained by
using random classification.  In fact, we can recover this property if
we consider a range of test sizes $k$, which is quite feasible to do
in practice.  But if we are willing to make some distributional
assumptions on $\vec{r}$, we can go even further and eliminate the
arbitrary choice of $\epsilon$.  This is the subject of the next
subsection.

\subsection{Summarizing performance in random classification}

Consider a large set of neurons, $r_1,\hdots, r_m$.  Each neuron
$r_i(s)$ is the sum of a signal term $h_i(s)$ and a noise term,
$\epsilon_i$.  Let us assume that for random stimuli $S$ drawn from
distribution $S \sim \pi$, we have
\[
(h_1(S),\hdots, h_m(S)) \sim N(0, \Sigma_H)
\]
and
\[
(\epsilon_1,\hdots, \epsilon_m) \sim N(0, \Sigma_\epsilon).
\]
hence $\vec{r} \sim N(0, \Sigma_H + \Sigma_\epsilon).$

Now consider a random classification problem, $s_1,\hdots, s_K$ drawn
from $\pi$.  Let $\mu_i = h(s_i)$.

Let $j^*$ be the random label drawn uniformly from $\{1,\hdots,
N\}$. Then the classification rule is to estimate
\[
\hat{j} = \argmin_{j \in \{1, \hdots, K\}} ||\vec{r} - \mu_j||^2_{\Sigma_\epsilon}
\]

The classification is correct in the event that $||\vec{r}
- \mu_j||_{\Sigma_\epsilon}^2 < ||\vec{r}
- \mu_j||_{\Sigma_\epsilon}^2$ for all $j \neq j^*$.  The average
misclassification is a function $\text{MC}(\Sigma_H, \Sigma_\epsilon,
K)$.  By transformation of variables, we see that
\[
\text{MC}(\Sigma_H, \Sigma_\epsilon, K) = \text{MC}(\Sigma_\epsilon^{-1/2}\Sigma_H\Sigma_\epsilon^{-1/2}, I, K)
\]
so we might as well only consider $\Sigma_\epsilon = I$.

Now our goal is to find a summary statistic
$\mathcal{I}(\Sigma_\epsilon, \Sigma_H)$ for the misclassification
curve $\text{MC}(\Sigma_\epsilon, \Sigma_H, \cdot)$ as $K$ varies.
Ideally the curve $\text{MC}(\Sigma_H, I, K)$ versus $K$ only depends
on a one-dimensional function of $\Sigma_H$, hence yielding a perfect
summary.

TODO: See if a good summary $\mathcal{I}$ can be found, or if more
than one summary statistic is needed.

\emph{Ideas.}
Approximations for $\text{MC}$ can be found depending on $K$:
\begin{itemize}
\item $K$ small, ``stochastic'' regime.  Difficult to approximate because of variability in spacing.
\item $K$ medium, ``boundary'' regime.  Concentration of measure: most centroids lie close to the level surface with Mahalanobis distance $m$.  Therefore it's basically a uniform distribution on an ellipsoid, and shouldn't depend too much on $\Sigma_h$ beyond the surface area of the ellipsoid.
\item $K$ large, ``interior'' regime.  Most centroids are inside the convex hull, and hence have very high misclassification rates.  Most observations $\vec{r}$ are classified into the small minority of the centroids lying on the boundary.  The boundary itself itself is very well behaved: it is a level surface of mahalanobis distance $\log(2K)$.  
\end{itemize}
In the $K$ large regime misclassification rate is necessarily quite high, so arguably the case of most interest ($K$ medium) is also the most nicely behaved.  See figure 1:

\begin{center}
\includegraphics[scale = 0.5]{kgauss1.png}
\end{center}


\comm{
Consider independent $\vec{r}^{(1)}$ and $\vec{r}^{(2)}$ with
corresponding signal covariances $\Sigma_H^{(1)}$ and
$\Sigma_H^{(2)}$, and error covariances $\Sigma_\epsilon^{(1)}$ and
$\Sigma_\epsilon^{(2)}$.  By transforming both $\vec{r}^{(i)}$ it is
clear that we can take $\Sigma_\epsilon = I$.

TODO: See how the summary $\mathcal{I}$  behaves when combining independent vectors.

The average correct classification rate is
\begin{align*}
\text{CC} &= \Pr[||\vec{r} - \mu_1||_{\Sigma_\epsilon}^2 < \min_{j > 1} ||\vec{r} - \mu_j||_{\Sigma_\epsilon}^2 | j^* = 1]
\\&= \Pr[\forall j  > 1: \mu_j \notin B_{||\epsilon||_{\Sigma_\epsilon}}(\vec{r})]
\\&= \Pr[\mu_2 \notin B_{||\epsilon||_{\Sigma_\epsilon}}(\vec{r})]^{K - 1}
\\&= \int_{\mathbb{R}^d \times \mathbb{R}^d} \left[1 - \int_{B_{||\epsilon||_{\Sigma_\epsilon}}(r)} p(\mu) d\mu\right]^{K-1} dP(\epsilon, \vec{r})
\end{align*}
where $B_r(x)$ is the euclidean $\Sigma_\epsilon$-normed ball of radius $r$ centered at $x$ and
\[
p(\mu) = \frac{1}{(2\pi|\Sigma_\mu|)^{d/2}} \exp(-\frac{1}{2}\mu^T \Sigma_\mu^{-1} \mu)
\]
}

\subsection{Relation to information theory}

Somehow the above can be related to mutual information 
\[I(\vec{h}, \vec{r}) \propto \ln(\Sigma_H) - \ln(\Sigma_H(\Sigma_H + \Sigma_\epsilon)^{-1} \Sigma_H)\]

\subsection{Robustness}

We wish to apply the methods derived using the preceding Guassian
theory to non-Gaussian case: how well does it generalize?

\section{Estimating Encoding Parameters}

\subsection{Previous Work}

Consider a parametric model
\[
Y \sim F_\theta(X)
\]
Such a \emph{forward model} gives the distribution of the
response conditional on the stimuli features.

The \emph{maximum likelihood} (ML) principle can be invoked to
identify the stimuli $i \in S$ ``most likely'' to have produced $y^*$.
Let $x_i : i \in S$ denote features of the test stimuli, and
identify $y^*$
\[
i^* = \text{argmax}_i \ell_\theta(y^*| x_i)
\]

\emph{Example.} We take the following as a representative approach, combining features of [1] and [2]:
\begin{itemize}
\item Assume the normal mutivariate linear model
\[Y \sim N( XB , \Sigma_E), \text{ where }B \in \mathbb{R}^{q \times p}\]
\item Estimate $B$ using elastic net [4]
\item Estimate $\Sigma_E$ using off-diagonal shrinkage of sample covariance matrix of residuals
\item The ML rule takes the form
\begin{equation}\label{mlrule}
i^* = \text{argmin}_{i} (x_i^T \hat{B} - y^*)^T \hat{\Sigma}_E^{-1} (x_i^T \hat{B} - y^*)
\end{equation}
\end{itemize}

\subsection{Emprirical Bayes Approach}

\begin{itemize}
\item \emph{Idea}: Unlike ML, the Bayes rule surely optimizes the ``correct'' objective function.  Can we approximate the Bayes rule?
\item \emph{Empirical Bayes}: use the data to estimate the covariances
$\Sigma_B$ and $\Sigma_E$, then compute posterior distribution of $B$
\item Assume coefficients of $B$ independent; diagonals of $\Sigma_B$ can be estimated using any estimate of signal strength, e.g. \emph{Eigenprism} [3].
\item Decision rule similiar to \eqref{mlrule} but with ``added noise'' due to uncertainty of $B$.
\[
\text{min} (x_i^T B - y^*)^T (\text{Cov}(x_i^T B) + \hat{\Sigma}_E)^{-1} (x_i^T B - y^*)
\]
\item Analogous to LDA vs QDA
\end{itemize}

\subsection{Covariance Estimation}

Both ML approach and EB approach depend on estimates of the covariance of $Y$.
We might assume a ``Kronecker'' model where $\Cov(Y) = \Sigma_v \times \Sigma_t$
where $\Sigma_v$ is the covariance of voxels (for a fixed time point)
and $\Sigma_t$ is the autocovariance of a given voxel.

New approaches to estimating covariance $\Sigma_v$ might incorporate
spatial information about voxel location, or functional information.

\subsection{Computation}

Suppose we have a $n \times p_Y$ response matrix $Y$ and a $n \times p_X$ covariate matrix $X$.
We have the model
$$
Y = XB + E
$$
where $B$ is a $p_X \times p_Y$ coefficient matrix and $E$ is a matrix of noise.
Write $$
E = \begin{pmatrix} e_1^T\\e_2^T\\ ... \\ e_n^T \end{pmatrix}
$$
Assume the rows of $E$ are independent, and each row is distributed
$$
e_i \sim N(0, \Sigma_e)
$$

Let us assume a prior distribution on the coefficients,
$$
B_{ij} \sim N(0, \sigma^2_{B, j})
$$
and where $B_{ij}$ are independent.
Note that each column $j = 1,...,p_Y$ has a different prior coviarance $\sigma^2_{B, j}$.
This reflects our prior knowledge that some column of $Y$ may have more signal than other columns.

Bayes' rule gives us the posterior mean and covariance of the coefficients $B_{ij}$.
First, we introduce the vectorized notation $\vec{B}$ to denote the $p_X p_Y \times 1$ vector obtained by 
stacking the columns of $B$, and similarly $\vec{Y}$ to denote the $p_Y n \times 1$ vector obtained by stacking the columns of $Y$.  Also recall the definition of Kronecker product,

$$
A \otimes B = \begin{pmatrix}a_{11} B & a_{12} B & ...\\
a_{21} B & a_{22} B & ...\\
... & ... & ...\end{pmatrix}
$$

Also define
$$
\Omega_e = \Sigma_e^{-1}
$$
$$
\Omega_b = \text{diag}\left(\frac{1}{\sigma^2_{b, 1}},..., \frac{1}{\sigma^2_{b, p_Y}}\right)
$$

The posterior mean and covariance are given as follows.

$$
\text{E}[\vec{B}|Y] = (\Omega_e \otimes X^T X + \Omega_b \otimes I_{p_X})^{-1} ((I\otimes X)^T (\Omega_e \otimes I) \vec{y})
$$
$$
\text{Cov}(\vec{B}|Y) = = (\Omega_e \otimes X^T X + \Omega_b \otimes I_{p_X})^{-1}
$$

Given a new observation $y_* = B^T x_* + e_*$, where $y_*$ and $x_*$ are column vectors,
the posterior predictive distribution is
$$
\text{E}[y_* | Y] = (I_{p_Y} \otimes x_*^T) \text{E}[\vec{B}|Y]
$$
$$
\text{Cov}[y_* | Y] = (I_{p_Y} \otimes x_*^T) \text{Cov}(\vec{B}|Y) (I_{p_Y} \otimes x_*) + \Sigma_e
$$


In the following, we assume that $p_X > n$.

Naively, computing the posterior covariance takes $O(p_X^3 p_Y^3)$ operations
and computing the posterior mean takes $O(p_X^2 p_Y^2)$ operations.

However, by diagonalizing the covariance matrix and taking advantage of the properties of Kronecker products, we can make the computation much more efficient.

Using simultaneous diagonalization, find $V_e, D_e$ such that
$$
\Omega_e = V_e D_e V_e^T
$$
and
$$
\Omega_b = V_e V_e^T
$$
Note that $V_e$ is not orthogonal.
The procedure for finding $V_e, D_e$ is well-known and also given in the code.

Furthermore, define 
$$\tilde{X} = \begin{pmatrix} X \\ 0\end{pmatrix}$$ so that $\tilde{X}$ is an $p_X \times p_X$ matrix.
Then take the SVD
$$
\tilde{X} = U_X D_X V_X^T
$$
so that $D_X$ and $V_X$ are both  $p_X \times p_X$.
We have
$$
V_X V_X^T = V_X V_X^T = I_{p_X}
$$
and
$$
V_X D_X^2 V_X^T = X^T X.
$$

Now we can rewrite the expression
$$
\text{Cov}(\vec{B}|Y) = = (\Omega_e \otimes X^T X + \Omega_b \otimes I_{p_X})^{-1}
$$
$$
= ((V_e D_e V_e^T) \otimes (V_X D_X^2 V_X^T) + (V_e V_e^T) \otimes (V_X V_X^T))^{-1}
$$
$$
= [(V_e \otimes V_X) (D_e \otimes D_X^2+ I_{p_X p_Y})(V_e^T \otimes V_X^T)]^{-1}
$$
$$
= (V_e^{-1} \otimes V_X^{T})^T (D_e \otimes D_X^2+ I_{p_X p_Y})^{-1} (V_e^{-1} \otimes V_X^{T})
$$

In this form the expression is much each to compute since
\begin{itemize}
\item Only a diagonal matrix needs be inverted, and yields a diagonal matrix
\item Multiplying the tranpose of a Kronecker product with a diagonal with itself is easy to compute.
\end{itemize}
To see the second point, consider computing the product

$$
C = (A \otimes B)^T D (A \otimes B)
$$
where $A$ is $a_1 \times a_2$, $B$ is $b_1 \times b_2$, and $D$ is diagonal with
$$
D = \begin{pmatrix} D_1 & 0 & 0 & 0 &... \\
0 & D_2 & 0 & 0 & ...\\
0 & 0 & D_3 & 0 & ...\\
... & ...& ... & ... & ...
\end{pmatrix}
$$
such that each $D_1,..., D_{a_1}$ is $b_1 \times b_1$.

For $i, j = 1..., a_2$, let $C_{[ij]}$ denote $b_2 \times b_2$ blocks such that
$$
C = \begin{pmatrix} C_{[11]} & C_{[12]} & ...\\
C_{[21]} & C_{[22]} & ... \\
... & ... & ...\end{pmatrix}
$$

Now we can write
$$
C_{[ij]} = \sum_{k=1}^{a_1} a_{ki}a_{kj} B^T D_k B 
= \left(\sum_{k=1}^{a_1} a_{ki} a_{kj} B^T D_k B \right)
= B^T \left(\sum_{k=1}^{a_1} a_{ki} a_{kj} D_k \right) B.
$$

The second-to-last and last equality present two alternative methods to compute $C_{[ij]}$.

\begin{itemize}
\item Using the second-to-last equality, one precomputes each of the matrices $B^T D_i B$, which takes $O(b_2^2 b_1 a_1)$ operations.  To compute each block, it takes $O(b_2^2 a_1)$ operations.  Hence, to compute $C$ takes $O(a_1 a_2^2 b_2^2)$.
\item Using the last equality, for each block one takes $O(a_1 b_1)$ operations to compute the weighted sum of $D_1,..., D_{a_1}$, and then $O(b_2^1 b_1)$ to multiply on the left with $B^T$ and on the right with $B$.
\end{itemize}
Therefore, we see that it takes $O(a_2^2 b_2^2 b_1)$ operations to compute $C$.

Choosing the best of the two methods, the cost to compute $C$ is $O(a_2^2 b_2^2 \min(a_1, b_1))$.
Applying to our problem, we see that the cost to evaluate the posterior covariance using this method is $O(p_Y^2 p_X^2 \min(p_X, p_Y))$, which is a saves a factor of $\max(p_Y, p_X)$ compared to the naive approach.


\section{Inferring Information Content}

\subsection{Finite Sample Effect}

Changing slightly the notation, let $\text{MC}(\vec{r}, K, n)$ denote
the misclassification when classifying $K$ random classes, given a
training set of size $n$.  Our definition of information is computed
from $\text{MC}(\vec{r}, K, \infty)$: the misclassification curves
assuming a \emph{known} encoding rule.  However, in practice, we only
know $\text{MC}(\vec{r}, K, n)$ for finite $n$.

\subsection{Learning Curves}

The question is whether one can estimate $\text{MC}(\vec{r},
K, \infty)$ based on knowing $\text{MC}(\vec{r}, K, n)$ for various
$n \leq N$, where $N$ is the total number of observations available.

Cortes et al propose the following method to model the training and
test error as a function of $n$, the training set size:

\[
\text{Err}_{\text{train}} = a - \frac{b}{n^\beta}
\]
\[
\text{Err}_{\text{test}} = a + \frac{c}{n^\alpha}
\]
where $a$, $b$, $c$, $\alpha$ and $\beta$ are to be determined
empirically.

GOAL: Extend this type of empirical model to also incoporate $K$, the
number of test classes, and justify somehow.

\end{document}



