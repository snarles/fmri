\title{Metric learning for multivariate linear models}
\author{Charles Zheng and Yuval Benjamini}
\date{\today}

\documentclass[12pt]{article} 

% packages with special commands
\usepackage{amssymb, amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage{ifthen}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{csquotes}
\definecolor{grey}{rgb}{0.5,0.5,0.5}

\begin{document}
\maketitle

\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\newcommand{\Var}{\text{Var}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\comm}[1]{}


\section{Introduction}

Let $X \in \mathcal{X} \subset \mathbb{R}^p$ and $Y \in \mathcal{Y} \subset \mathbb{R}^q$ be random vectors with
a joint distribution, and let $d_F(\cdot, \cdot)$ be a distance on
probability measures.

Let $F_x$ denote the conditional distribution of $Y$ given $X=x$ (and
assume that such conditional distributions can be constructed.)
Define the \emph{induced metric} on $\mathcal{X}$ by
\[
d_\mathcal{X}(x_1, x_2) = d_F(F_{x_1}, F_{x_2})
\]

We are interested in the problem of estimating the induced metric
$d_{\mathcal{X}}$ based on iid observations $(x_1,y_1),\hdots, (x_n,
y_n)$ drawn from the joint distribution of $(X, Y)$.  We define the
loss function for estimation as follows. Let $\hat{d}$ (suppressing
the subscript) denote the estimate of $d_{\mathcal{X}}$, and let
$G$ denote the marginal distribution of $X$. Then the loss
is defined as
\[
\mathcal{L}(d_\mathcal{X}, \hat{d}) 
= 1 - \Cor_{X, X' \sim G}[d_\mathcal{X}(X, X'), \hat{d}(X, X')]
\]
where the correlation is taken over independent random pairs $(X, X')$
drawn from $G \times G$.

Now we make the following additional assumptions.  Let us assume that
$X \sim N(0, \Sigma_X)$ and that the conditional distribution of
$Y|X=x$ is given by
\[
F_x =  N(B^T x + \eta, \Sigma_\epsilon)
\]
for some $p \times q$ coefficient matrix $B$, $p \times p$ covariance
matrix $\Sigma_X$, $q \times q$ covariance matrix $\Sigma_\epsilon$,
and $q \times 1$ vector $\eta$.

In the special case that both arguments of the KL divergence are are
multivariate gaussian distributions with the same covariance matrix,
the KL divergence reduces to a multiple of the Mahalanobis distance.
Hence given our assumptions it is natural to adopt a multiple of the
KL divergence as the error metric $d_F$:
\[
d_F(\mu_1, \mu_2) = (\mu_1-\mu_2)\Sigma_\epsilon^{-1}(\mu_1 - \mu_2)
\]

Therefore, we obtain the following induced metric:
\[
d_\mathcal{X}(x_1,x_2) = (x_1-x_2)^T B \Sigma_\epsilon^{-1} B^T (x_1 - x_2)
\]
This is a function only of $\delta = x_1 - x_2$.
So defining the
positive-semidefinite matrix norm
\[
||x||_A = \sqrt{x^T A x}
\]
we have
\[
d_\mathcal{X}(x_1,x_2) = ||x_1 - x_2||^2_{B\Sigma_\epsilon^{-1}B^T}
\]

\section{Estimation}

Since $d_\mathcal{X}$ is completely specified by $B$ and
$\Sigma_\epsilon$, the problem of \emph{metric
learning for a multivariate linear models} (MLMLM) reduces to the
problem of jointly estimating $B$ and $\Sigma_\epsilon$, under a loss
function $\tilde{L}$ defined by
\[
\tilde{\mathcal{L}}(B, \Sigma_\epsilon; \hat{B}, \hat{\Sigma}_\epsilon) = 
1 - \Cor_{\delta \sim N(0, \Sigma_X)}(||\delta||^2_{B\Sigma_\epsilon^{-1}B^T},
||\delta||^2_{\hat{B}\hat{\Sigma}_\epsilon^{-1}\hat{B}^T})
\]
One can verify that
\[
\tilde{\mathcal{L}}(B, \Sigma_\epsilon; \hat{B}, \hat{\Sigma}_\epsilon) 
= \mathcal{L}(d_\mathcal{X}, \hat{d})
\]
where
\[
\hat{d}(x_1, x_2) =  (x_1-x_2)^T \hat{B} \hat{\Sigma}_\epsilon^{-1} \hat{B}^T (x_1 - x_2).
\]
The loss function $\tilde{L}$ looks complicated at first,
but perhaps we can find a simplified approximation.

\subsection{Approximating $\tilde{L}$}

Let $\delta$ be multivariate normal $N(0, \Sigma_X)$.
Then $X = \Sigma^{-1/2}\delta$ has distribution $N(0, I_p)$
and
\[
||\delta||^2_{B\Sigma_\epsilon^{-1}B^T}
= \delta^T B\Sigma_\epsilon^{-1}B^T \delta
= X^T \Sigma_X^{1/2}B\Sigma_\epsilon^{-1}B^T \Sigma_X^{1/2} X
= ||X||^2_{\Sigma_X^{1/2}B\Sigma_\epsilon^{-1}B^T \Sigma_X^{1/2}}
\]
Defining the $p \times p$ matrices $\Gamma$ and $\hat{\Gamma}$ by
\[\Gamma = \Sigma_X^{1/2}B\Sigma_\epsilon^{-1}B^T \Sigma_X^{1/2}\]
\[\hat{\Gamma}
 = \Sigma_X^{1/2}\hat{B}\hat{\Sigma}_\epsilon^{-1}\hat{B}^T \Sigma_X^{1/2},\]
we have
\[
\tilde{L} = 
1 - \Cor_{z \sim N(0, I)}(||Z||^2_\Gamma, ||Z||^2_{\hat{\Gamma}})
\]
In the appendix we show that
\[
\Cor(z^T A z, z^T B z) = \frac{\tr[AB]}{\sqrt{\tr[A]\tr[B]}}
\]
for any positive semidefinite symmetric $A, B$.
Hence
\[
\tilde{L} = 1 - \frac{\tr[\Gamma\hat{\Gamma}]}{\sqrt{\tr[\Gamma]\tr[\hat{\Gamma}]}}.
\]

\section{Appendix}

\subsection{Covariance formula} 

\noindent\textbf{Lemma}. \emph{
Let $Z \sim N(0, I)$.  Then for any PSD $A, B$ we have
\[
\Cov(Z^T A Z, Z^T B Z) = 2\tr[AB]
\]
and
\[
\Cor(Z^TA Z, Z^T B Z) = \frac{\tr[AB]}{\sqrt{\tr[A]\tr[B]}}
\]
}

\textbf{Proof}.
From Fujikoshi (2010) theorems 2.2.5. and 2.2.6. we have that if $X \sim W_p(n, \Sigma)$
\[
\E\tr[AW] = n \tr[A\Sigma]
\]
and
\[
\E\tr[AWBW] = n\tr[A\Sigma B^T \Sigma] + n\tr[A\Sigma]\tr[B\Sigma] + n^2\tr[A\Sigma B\Sigma]
\]
for any $p \times p$ matrices $A, B$.

Now, since $ZZ^T \sim W_p(1, I_p)$, since $A$ and $B$ are symmetric, we have
\begin{align*}
\Cov(Z^T A Z, Z^T B Z) &= \E\tr[AZZ^T BZZ^T] - (\E\tr[AZZ^T])(\E\tr[BZZ^T])
\\&= 2\tr[AB] + \tr[A]\tr[B] - \tr[A]\tr[B]= 2\tr[AB].
\end{align*}

Hence
\[
\Cor(Z^T A Z, Z^T B Z) = \frac{\Cov(Z^T A Z, Z^T B Z)}{\sqrt{\Cov(Z^T A Z) \Cov(Z^T B Z)}} = \frac{2\tr[AB]}{\sqrt{2\tr[A^2]2\tr[B^2]}},
\]
completing the proof.





\end{document}



