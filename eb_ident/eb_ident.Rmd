---
title: "An Empirical Bayes Approach to Identification"
author: "Charles Zheng"
date: "05/21/2015"
output: html_document
---

# Setup

A scientist and statistician are collaborating on understanding the visual system.
The scientist presents a series of $2 K$ stimuli (pictures) to the subject
and records the subject's response to each stimuli as a $p$-dimensional vector.
The $2K$ stimuli consist of $K$ different pictures, each picture repeated twice.
Let $y_{i}^1$ and $y_i^2$ denote the subject's responses to the $i$ th picture.

The statistican seeks to model the subject's response to the stimuli
in terms of a set of $q$ stimulus features.  For example, the statistician might define
a library of $q = 10000$ Gabor filters to featurize each picture.
Let $x_i$ denote the $q$-dimensional feature vector for each picture.

Let us assume the following multivariate regression model for the data.  We have
$$
y_i^j = x_i^T B + \epsilon_i^j
$$
so the subject's response is a linear function of the image features plus noise.
Here $B$ is a $q \times p$ matrix.  Let us write $\vec{B}$ as the $qp \times 1$ vectorized
representation of $B$, and further suppose that $\vec{B}$ is a random variate 
$$
\vec{B} \sim N(0, \Sigma_B)
$$
and that the $\epsilon_i^j$ are indenpendent of the $B$, with $\epsilon_i^j$ 
i.i.d.
$$
\epsilon_i^j \sim N(0,\Sigma_\epsilon)
$$

Let $Y$ denote the matrix of responses $Y = [(y_1^1)^T, (y_1^2)^T, ..., (y_K^1)^T, (y_K^2)^T]$
and $X$ denote the corresponding design matrix $X = [x_1^T, x_1^T, ..., x_K, x_K]$, so that the model can be written
$$
Y = XB + E
$$
Further, let $\vec{Y}$ denote the vectorized matrix of responses and $Z = I_p \otimes X$
the corresponding expanded design matrix, so that the model can be written
$$
\vec{Y} = Z\vec{B} + \vec{E}
$$
The assumed dsitribution of the noise can be restated as
$$
\vec{E} \sim N(0, \Sigma_E)
$$
where $\Sigma_E = \Sigma_\epsilon \otimes I_{2K}$.

```{r}
library(pracma)
library(magrittr)
library(MASS)
K <- 30 # number of stimuli
p <- 20 # dimension of response
q <- 8 # dimension of features
# true noise covariance
Sigma_E <- 2/p * randn(2 * p, p) %>% { t(.) %*% . }
# true signal covariance = sB * I
sB0 <- 2
B0 <- sqrt(sB0) * randn(q, p)
# random features
X <- randn(K, q) %>% repmat(n = 2, m = 1)
# noise
E <- mvrnorm(2 * K, rep(0, p), Sigma_E)
# observations
Y <- X %*% B0 + E
```

Now supposing the statistican knows the exact value of $\Sigma_\beta$ and $\Sigma_\epsilon$,
it is a standard result in Bayesian inference that the posterior distribution for $\vec{B}$ can be obtained as
$$
\vec{B} \sim N( (Z^T \Sigma_E^{-1} Z + \Sigma_B^{-1})^{-1} Z^T \Sigma_E^{-1} \vec{Y},
(Z^T \Sigma_E^{-1} Z +\Sigma_B^{-1})^{-1})
$$

```{r}
# posterior mean
B_mu <- solve(t(X) %*% X + 1/sB0 * diag(rep(1, q))) %*% t(X) %*% Y
# posterior variance per column
post_cov_B_col <- solve(t(X) %*% X + 1/sB0 * diag(rep(1, q)) )
# marginal variances of B
post_var_B <- t(t(diag(post_cov_B_col))) %*% diag(Sigma_E)
# upper and lower intervals
B_up <- B_mu + 2 * sqrt(post_var_B)
B_low <- B_mu - 2 * sqrt(post_var_B)
# plot
plot(B0, B_mu, main = "Posterior mean B vs true B"); abline(0, 1, col = "green")
for (i in 1:q) { for (j in 1:p) {
  if (B_low[i,j] < B0[i, j] && B_up[i, j] > B0[i, j]) {
    cc <- "blue"
  } else {
    cc <- "red"
  }
  lines(rep(B0[i, j], 2), c(B_low[i, j], B_up[i, j]), col = cc)
  points(rep(B0[i, j], 2), c(B_low[i, j], B_up[i, j]), pch = "-", col = cc)
}}
points(B0, B_mu)
```

The posterior distribution of $\mu_i$ are given as follows:

$$
\text{E}(\mu_i) = (x_i)^T \text{E}(B)
$$

$$
\text{Cov}(\mu_i, \mu_j) = (I_p \otimes (x_i)^T) \text{Cov}(B) (I_p \otimes x_j)
$$

```{r}
post_cov_B_vec <- solve(solve(Sigma_E) %x% (t(X) %*% X) + 1/sB0 * diag(rep(1, p * q)))
f_post_cov_mu <- function(i, j) {
  (diag(rep(1, p)) %x% X[i, , drop = FALSE]) %*% post_cov_B_vec %*% (diag(rep(1, p)) %x% t(t(X[j, ])))
}
```

# Identification

